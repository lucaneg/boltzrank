{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# BoltzRank #\n",
    "## Luca Negrini - Mat. 956516 ##\n",
    "### From \"BoltzRank: Learning to Maximize Expected Ranking Gain\" ###\n",
    "#### Maxims M. Volkovs, Richard S. Zemel ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Initialization ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "# install lightgbm (required only on first run)\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "from sklearn.datasets import load_svmlight_file \n",
    "\n",
    "# datasets available at: \n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "DATASET_FOLDER = \"C:/opt/kiis-training/MSLR-WEB10K/Fold1/\"\n",
    "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
    "METRIC_NAME = 'Custom-MSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Data loading ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensureFile(path):\n",
    "    if not os.path.exists(path) or not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
    "    return path\n",
    "\n",
    "def retrieveFileNames():\n",
    "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
    "    train_file = ensureFile(folder + \"train.txt\")\n",
    "    valid_file = ensureFile(folder + \"vali.txt\")\n",
    "    test_file = ensureFile(folder + \"test.txt\")\n",
    "    return train_file, valid_file, test_file\n",
    "\n",
    "def loadDataset(path):\n",
    "    return load_svmlight_file(path, query_id=True)\n",
    "\n",
    "def loadLightGBM(svmlight_dataset):\n",
    "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
    "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid):\n",
    "        self.qid = qid\n",
    "        self.labels_to_docs = {}\n",
    "        self.perms = np.empty((1,1))\n",
    "        self.probs = np.empty((1,1))\n",
    "    def addlabel(self, label):\n",
    "        if not label in self.labels_to_docs:\n",
    "            self.labels_to_docs[label] = list()\n",
    "    def adddoc(self, label, doc):\n",
    "        self.labels_to_docs[label].append(doc)\n",
    "    def finalize(self):\n",
    "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
    "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
    "        i = 0\n",
    "        totaldocs = 0\n",
    "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
    "        for label, docs in sorteddict:\n",
    "            self.labels[i] = label\n",
    "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
    "            for j in range(len(docs)):\n",
    "                self.docs[i][j] = docs[j]\n",
    "            i += 1\n",
    "            totaldocs += len(docs)\n",
    "        self.alldocs = np.concatenate(self.docs)\n",
    "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
    "        i = 0\n",
    "        for label, docs in sorteddict:\n",
    "            for j in range(len(docs)):\n",
    "                self.flatlabels[i] = label\n",
    "                i += 1\n",
    "        del self.labels_to_docs\n",
    "    def setperms(self, perms):\n",
    "        self.perms = perms\n",
    "    def setprobs(self, probs):\n",
    "        self.probs = probs\n",
    "    def __repr__(self):  \n",
    "        return str(self)\n",
    "    def __str__(self):\n",
    "        res = \"Query \" + str(self.qid) + \"[\"\n",
    "        for i in range(len(self.labels)):\n",
    "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
    "        res += \"]\"\n",
    "        for i in range(len(self.perms)):\n",
    "            res += \"\\n[\" + str(self.perms[i]) + \"] -> p: \" + str(self.probs[i]) + \", g: \" + str(self.gains[i])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "#  dataset: svmlight_dataset \n",
    "#      the datset to process\n",
    "# returned values:\n",
    "#  query_to_labels_to_documents: dict(int -> dict(float -> list(int)))\n",
    "#      a map containing, for each query in the dataset, the documents (row index in the dataset) provided \n",
    "#      in the input dataset grouped by label\n",
    "#  doc_to_query: dict(document -> query)\n",
    "#      a mapping between document (row index in the dataset) and the relative query\n",
    "def mapQueryToDocuments(dataset):\n",
    "    queries = {}\n",
    "    doc_to_query = {} \n",
    "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
    "    for i in range(0, len(dataset[2])):\n",
    "        if not dataset[2][i] in queries:\n",
    "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
    "        query = queries[dataset[2][i]]\n",
    "        query.addlabel(dataset[1][i])\n",
    "        query.adddoc(dataset[1][i], i)\n",
    "        doc_to_query[i] = query.qid\n",
    "        alllabels[i] = dataset[1][i]\n",
    "        \n",
    "    for q in queries.values():\n",
    "        q.finalize()\n",
    "    \n",
    "    return queries, alllabels, doc_to_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### BoltzRank logic in Cython ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: C:\\Users\\NegriniL\\.ipython\\cython\\_cython_magic_1717f12ed364dbe44453d032e561f121.pyx:130:33: Use boundscheck(False) for faster access\n",
      "warning: C:\\Users\\NegriniL\\.ipython\\cython\\_cython_magic_1717f12ed364dbe44453d032e561f121.pyx:130:22: Use boundscheck(False) for faster access\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound\n",
    "\n",
    "# The sign-preserving function introduced in formula (4) and defined in formula (5). The implementation follows \n",
    "# the one proposed near the end of Section 4.2.\n",
    "#\n",
    "# parameters:\n",
    "#  x: double \n",
    "#      the difference in rankings between the two current documents\n",
    "#  m: int\n",
    "#      the number of documents available for the current query\n",
    "# returned values:\n",
    "#  val: double\n",
    "#      the result of the function\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double g_q(double x, int m) nogil:\n",
    "    return (2 * x) / (m - 1)\n",
    "\n",
    "# The energy function E(R|S) defined in formula (4). This represents the incompatibility between the \n",
    "# given ranking and the given score. More energy means less compatibility\n",
    "#\n",
    "# parameters:\n",
    "#  rank: int[:]\n",
    "#      the array of document IDs (row index in the dataset) representing the ranking\n",
    "#  scores: double[:]\n",
    "#      the array of document scores, score of document i will be retrieved from scores[i]\n",
    "# returned values:\n",
    "#  val: double\n",
    "#      the energy of the ranking w.r.t. the target scores. The lower, the better.\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double energy(int[:] rank, double[:] scores) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return 0\n",
    "    cdef double factor = 2 / (m * (m - 1))\n",
    "    cdef double res = 0\n",
    "    cdef int j\n",
    "    cdef int k\n",
    "    for k in prange(m, schedule='static', num_threads=8):\n",
    "        for j in range(k + 1, m):\n",
    "            res += g_q(j - k, m) * (scores[rank[j]] - scores[rank[k]])\n",
    "    return factor * res\n",
    "\n",
    "from libc.math cimport exp\n",
    "\n",
    "# The approximate rank probability P[Rq](R|S) defined in formula (12). This approximates the probability of a ranking\n",
    "# given a scoring. Since the effective computation is intractable due to the exponentail number of possible rankings, \n",
    "# this is computed through a montecarlo estimate over sample ranking sets\n",
    "#\n",
    "# parameters:\n",
    "#  sampleSet: int[:,:]\n",
    "#      the array containing all sample rankings, expresed as arrays of document IDs (row index in the dataset)\n",
    "#  rank: int[:]\n",
    "#      the ranking whose probability is to be computed, expressed as array of document IDs (row index in the dataset)\n",
    "#  scores: double[:]\n",
    "#      the scores to be used for the energy computation, score of document i will be retrieved from scores[i]\n",
    "# returned values:\n",
    "#  val: double\n",
    "#      the probability of the given ranking\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] approx_rank_probabilities(int[:,:] sampleSet, double[:] norm_probs, double[:] scores) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef int r\n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = exp(-energy(sampleSet[r], scores))\n",
    "        norm += norm_probs[r]\n",
    "    for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = norm_probs[r] / norm\n",
    "    return norm_probs\n",
    "\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "\n",
    "#int[:,:] source, int i, int j, int count, int[:,:] perms\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    # find maximum amount of permutations that can be done\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = min(count, _min)\n",
    "    # randomly selects the indexes\n",
    "    cdef int[:] first = np.random.choice(len(query.docs[i]), amount)\n",
    "    cdef int[:] second = np.random.choice(len(query.docs[j]), amount)\n",
    "    # perform the single-element permutations\n",
    "    cdef int k\n",
    "    cdef perm\n",
    "    cdef int[:] p\n",
    "    for k in range(amount):\n",
    "        perm = query.docs.copy()\n",
    "        perm[i][first[k]], perm[j][second[k]] = query.docs[j][second[k]], query.docs[i][first[k]]\n",
    "        p = np.concatenate(perm)\n",
    "        perms[start + k] = p\n",
    "    return count - amount, start + amount\n",
    "\n",
    "import random\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
    "    cdef bint match\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    for i in prange(len(container), schedule='static', num_threads=8):\n",
    "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
    "            continue\n",
    "        else:\n",
    "            match = True\n",
    "            for j in range(len(container[i])):\n",
    "                if container[i][j] != array[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
    "    cdef int i\n",
    "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
    "        container[pos][i] = array[i]\n",
    "                \n",
    "\n",
    "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation2(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    cdef int c = 0\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = max(1, int(_min * .5))\n",
    "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
    "    cdef int k\n",
    "    cdef int d\n",
    "    for k in range(count):\n",
    "        perm = query.docs.copy()\n",
    "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
    "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
    "        for d in range(len(first)):\n",
    "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
    "        p = np.concatenate(perm)\n",
    "        if not contained(perms, p):\n",
    "            setrow(perms, start + c, p)\n",
    "            c += 1\n",
    "            if c == limit:\n",
    "                return count - c, start + c\n",
    "        else:\n",
    "            k -= 1\n",
    "    return 0, start + c\n",
    "    \n",
    "import itertools\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
    "    cdef int i = 0\n",
    "    cdef int k\n",
    "    perm = itertools.permutations(source)\n",
    "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
    "    for p in perm:\n",
    "        for k in range(len(p)):\n",
    "            result[i][k] = p[k]\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
    "                                int(.20 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
    "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
    "                                int(.14 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
    "                                int(.08 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
    "                                int(.14 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
    "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
    "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
    "                                int(.04 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
    "                                ]\n",
    "\n",
    "#cdef int[:,:] process_query(int query, double[:] labels, int[:,:] docs, double[:] probs) nogil:\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def process_query(query, alllables):\n",
    "    cdef int carry = 0\n",
    "    fact = factorial(len(query.alldocs))\n",
    "    #s = \" (\" + str(RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS) + \" permutations computed)\"\n",
    "    cdef perms\n",
    "    cdef int last = 0\n",
    "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
    "        # evaluate all possible permutations, each one representing a different ranking\n",
    "        perms = allPerms(query.alldocs, fact)\n",
    "        #s = \" (all \" + str(fact) + \" permutations computed)\"\n",
    "    else:\n",
    "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
    "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
    "        carry, last = perform_permutation2(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
    "        carry, last = perform_permutation2(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
    "        carry, last = perform_permutation2(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
    "        if carry != 0:\n",
    "            if not query.alldocs in perms:\n",
    "                perms[last] = query.alldocs\n",
    "                #s = \" (missing \" + str(carry - 1) + \" permutations - considering also dataset ranking)\"\n",
    "            #else:\n",
    "                #s = \" (missing \" + str(carry) + \" permutations) \"\n",
    "        perms = perms[perms.max(axis=1)>=0]\n",
    "    cdef double[:] probs = np.zeros(len(perms))\n",
    "    query.setperms(perms)\n",
    "    query.setprobs(approx_rank_probabilities(perms, probs, alllables))\n",
    "    #print(\"query \" + str(query.qid) + \" done \" + s)\n",
    "    return query\n",
    "\n",
    "from libc.math cimport log, log2\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double cross_entropy(double[:] probs, double[:] scores_probs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(len(probs), schedule='static', num_threads=8):\n",
    "        result += probs[i] * log(scores_probs[i])\n",
    "    return -result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(0, k, schedule='static', num_threads=8):\n",
    "        result += (2**scores[rank[i]]) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double ndcg_k(int[:] true_rank, double[:] labels, int[:] rank, double[:] scores, int k) nogil:\n",
    "    return dcg_k(rank, scores, k) / dcg_k(true_rank, labels, k)\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double monte_carlo_gain(int[:] true_rank, double[:] labels, int[:,:] perms, double[:] scores_probs, double[:] scores) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    cdef int k = min(10, len(true_rank))\n",
    "    for i in prange(len(perms), schedule='static', num_threads=8):\n",
    "        result += scores_probs[i] * ndcg_k(true_rank, labels, perms[i], scores, k)\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_grads(queries, doc_to_query, preds, train_data): \n",
    "    cdef double lam = .9\n",
    "    labels = train_data.get_label().astype(np.double)\n",
    "    query_to_gain = {}\n",
    "    cdef double[:] score_probs\n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros(len(q.probs), dtype=np.double)\n",
    "        score_probs = approx_rank_probabilities(q.perms, score_probs, preds)\n",
    "        query_to_gain[q.qid] = lam * monte_carlo_gain(q.alldocs, labels, q.perms, score_probs, preds) \n",
    "        query_to_gain[q.qid] -= (1 - lam) * cross_entropy(q.probs, score_probs)\n",
    "    \n",
    "    cdef double[:] gain = np.zeros_like(preds)\n",
    "    cdef int i\n",
    "    for i in range(len(gain)):\n",
    "        gain[i] = query_to_gain[doc_to_query[i]]\n",
    "\n",
    "    cdef double[:] hess = np.ones_like(gain) \n",
    "    return gain, hess   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/train.txt\n",
      "validation file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/vali.txt\n",
      "test file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/test.txt\n",
      "loading datasets... \n",
      "train dataset loading took 55.625 s\n",
      "validation dataset loading took 27.90625 s\n",
      "test dataset loading took 16.734375 s\n",
      "converting datasets to LightGBM format... \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = retrieveFileNames()\n",
    "\n",
    "print(\"training file: \" + train_file)\n",
    "print(\"validation file: \" + valid_file)\n",
    "print(\"test file: \" + test_file)\n",
    "    \n",
    "print(\"loading datasets... \")\n",
    "import time\n",
    "start = time.process_time()\n",
    "train_dataset = loadDataset(train_file)\n",
    "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "valid_dataset = loadDataset(valid_file)\n",
    "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "test_dataset = loadDataset(test_file)\n",
    "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "\n",
    "import itertools\n",
    "print(\"converting datasets to LightGBM format... \")\n",
    "train_lgb = loadLightGBM(train_dataset)\n",
    "valid_lgb = loadLightGBM(valid_dataset)\n",
    "test_lgb = loadLightGBM(test_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating query-documents mappings...\n",
      "done\n",
      "creating sample sets...\n",
      "sample set creation took 31.1875 s\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"creating query-documents mappings...\")\n",
    "queries, alllables, doc_to_query = mapQueryToDocuments(train_dataset)\n",
    "print(\"done\")\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "print(\"creating sample sets...\")\n",
    "start = time.process_time()\n",
    "\n",
    "#num_cores = multiprocessing.cpu_count()\n",
    "#with parallel_backend('threading', n_jobs=num_cores):\n",
    "#    result_list = Parallel()(delayed(process_query)(query) for query in queries.values())\n",
    "#result_list = Parallel(n_jobs=num_cores)(delayed(process_query)(query) for query in queries.values())\n",
    "#for q in result_list:\n",
    "    #queries[q.qid] = q\n",
    "for q in queries.values():\n",
    "    queries[q.qid] = process_query(q, alllables)\n",
    "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(q, k):\n",
    "    indexes = set(range(0, len(q.perms)))\n",
    "    indexes.remove(k)\n",
    "    result = []\n",
    "    for i in range(len(q.perms[k])):\n",
    "        tmp = set(indexes)\n",
    "        for j in tmp:\n",
    "            if q.perms[k][i] != q.perms[j][i]:\n",
    "                indexes.remove(j)\n",
    "    for w in indexes:\n",
    "        if w < k:\n",
    "            result.append((w, k))\n",
    "        else: \n",
    "            result.append((k,w))\n",
    "    return result\n",
    "\n",
    "def checkRepetitions():\n",
    "    global queries\n",
    "    same = dict()\n",
    "    for q in queries.values():\n",
    "        for i in range(len(q.perms)):\n",
    "            r = check(q, i)\n",
    "            if len(r) != 0:\n",
    "                if not q.qid in same.keys():\n",
    "                    same[q.qid] = set()\n",
    "                for t in r:\n",
    "                    same[q.qid].add(t)\n",
    "\n",
    "    print(str(len(same.keys())) + \"/\" + str(len(queries.keys())) + \" queries have duplicate permutations\")\n",
    "    for q, s in same.items():\n",
    "        print(\"query \" + str(q) + \" has repeated permutations: \" + str(s))\n",
    "        \n",
    "# checkRepetitions()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define evaluation metric and objective function\n",
    "#\n",
    "\n",
    "# current predictions, dataset => name, score, true iff higher means better\n",
    "def mse_eval(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    return METRIC_NAME, avg_mse, False\n",
    "\n",
    "# current predictions, dataset => first order derivative, second order derivative\n",
    "def mse_grads(preds, train_data): \n",
    "    global queries\n",
    "    global doc_to_query\n",
    "    gain, hess = eval_grads(queries, doc_to_query, preds, train_data)\n",
    "    return np.asarray(gain), np.asarray(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lightgbm...\n",
      "[1]\ttrain's Custom-MSE: 0.540575\tvalid's Custom-MSE: 0.555297\ttest's Custom-MSE: 0.557876\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train the model\n",
    "#\n",
    "\n",
    "params = {\n",
    "#    'objective':'lambdarank', # what to optimize during training\n",
    "#    'max_position': 10,      # threshold used in optimizing lamdarank (NDCG)\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'metric': ['None'], #['ndcg'],       # what to use/print for evaluation\n",
    "#    'ndcg_eval_at': 10\n",
    "# try printing ndcg and testing\n",
    "}    \n",
    "\n",
    "print(\"training lightgbm...\")\n",
    "start = time.process_time()\n",
    "lgbm_info = {}\n",
    "lgbm_model = lightgbm.train(params, train_lgb, num_boost_round=100,\n",
    "                            feval = mse_eval,\n",
    "                            fobj  = mse_grads,\n",
    "                            valid_sets   = [train_lgb, valid_lgb, test_lgb], \n",
    "                            valid_names  = [\"train\", \"valid\", \"test\"],\n",
    "                            evals_result = lgbm_info,\n",
    "                            verbose_eval = 1)\n",
    "print(\"training took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(lgbm_info['train'][METRIC_NAME], label='training')\n",
    "plt.plot(lgbm_info['valid'][METRIC_NAME], label='validation')\n",
    "plt.plot(lgbm_info['test'][METRIC_NAME], label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"# Trees\")\n",
    "plt.ylabel(METRIC_NAME)\n",
    "plt.title(\"Model Error\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
