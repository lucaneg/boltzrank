{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# BoltzRank #\n",
    "## Luca Negrini - Mat. 956516 ##\n",
    "### From \"BoltzRank: Learning to Maximize Expected Ranking Gain\" ###\n",
    "#### Maxims M. Volkovs, Richard S. Zemel ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Initialization ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "# install lightgbm (required only on first run)\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "from sklearn.datasets import load_svmlight_file \n",
    "\n",
    "# datasets available at: \n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "DATASET_FOLDER = \"C:/opt/kiis-training/MSLR-WEB10K/Fold1/\"\n",
    "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
    "METRIC_NAME = 'ndcg@10'#'BoltzRank-NDCG@10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Data loading ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensureFile(path):\n",
    "    if not os.path.exists(path) or not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
    "    return path\n",
    "\n",
    "def retrieveFileNames():\n",
    "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
    "    train_file = ensureFile(folder + \"train.txt\")\n",
    "    valid_file = ensureFile(folder + \"vali.txt\")\n",
    "    test_file = ensureFile(folder + \"test.txt\")\n",
    "    return train_file, valid_file, test_file\n",
    "\n",
    "def loadDataset(path):\n",
    "    return load_svmlight_file(path, query_id=True)\n",
    "\n",
    "def loadLightGBM(svmlight_dataset):\n",
    "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
    "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid):\n",
    "        self.qid = qid\n",
    "        self.labels_to_docs = {}\n",
    "    def addlabel(self, label):\n",
    "        if not label in self.labels_to_docs:\n",
    "            self.labels_to_docs[label] = list()\n",
    "    def adddoc(self, label, doc):\n",
    "        self.labels_to_docs[label].append(doc)\n",
    "    def finalize(self, alllabels):\n",
    "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
    "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
    "        i = 0\n",
    "        totaldocs = 0\n",
    "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
    "        for label, docs in sorteddict:\n",
    "            self.labels[i] = label\n",
    "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
    "            for j in range(len(docs)):\n",
    "                self.docs[i][j] = docs[j]\n",
    "            i += 1\n",
    "            totaldocs += len(docs)\n",
    "        self.alldocs = np.concatenate(self.docs)\n",
    "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
    "        i = 0\n",
    "        for label, docs in sorteddict:\n",
    "            for j in range(len(docs)):\n",
    "                self.flatlabels[i] = label\n",
    "                i += 1       \n",
    "        k = min(10, len(self.alldocs))\n",
    "        self.idealdcg = dcg_k(self.alldocs, alllabels, k) \n",
    "        del self.labels_to_docs\n",
    "    def setperms(self, perms):\n",
    "        self.perms = perms\n",
    "    def setprobs(self, probs):\n",
    "        self.probs = probs\n",
    "    def setndcgs(self, ndcgs):\n",
    "        self.ndcgs = ndcgs\n",
    "    def __repr__(self):  \n",
    "        return str(self)\n",
    "    def __str__(self):\n",
    "        res = \"Query \" + str(self.qid) + \"[\"\n",
    "        res += \"\\nideal dcg: \" + str(self.idealdcg)\n",
    "        for i in range(len(self.labels)):\n",
    "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
    "        res += \"]\"\n",
    "        if hasattr(self, 'perms'):\n",
    "            for i in range(len(self.perms)):\n",
    "                res += \"\\n[\" + str(self.perms[i]) + \"] -> p: \" + str(self.probs[i]) + \", dcg: \" + str(self.ndcgs[i])\n",
    "        else:\n",
    "            res += \"\\nNo permutations computed yet\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "#  dataset: svmlight_dataset \n",
    "#      the datset to process\n",
    "# returned values:\n",
    "#  query_to_labels_to_documents: dict(int -> dict(float -> list(int)))\n",
    "#      a map containing, for each query in the dataset, the documents (row index in the dataset) provided \n",
    "#      in the input dataset grouped by label\n",
    "#  doc_to_query: dict(document -> query)\n",
    "#      a mapping between document (row index in the dataset) and the relative query\n",
    "def mapQueryToDocuments(dataset):\n",
    "    queries = {}\n",
    "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
    "    for i in range(0, len(dataset[2])):\n",
    "        if not dataset[2][i] in queries:\n",
    "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
    "        query = queries[dataset[2][i]]\n",
    "        query.addlabel(dataset[1][i])\n",
    "        query.adddoc(dataset[1][i], i)\n",
    "        alllabels[i] = dataset[1][i]\n",
    "        \n",
    "    for q in queries.values():\n",
    "        q.finalize(alllabels)\n",
    "    \n",
    "    return queries, alllabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### BoltzRank logic in Cython ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "from libc.math cimport exp\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound\n",
    "from libc.math cimport log, log2\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from libc.stdio cimport printf\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                              NDCG EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(k, schedule='static', num_threads=8):\n",
    "        result += (2**scores[rank[i]] - 1) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
    "    return result\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double ndcg_k(int[:] rank, double[:] scores, int k, double ideal) nogil:\n",
    "    if ideal == 0:\n",
    "        return 1.0\n",
    "    return dcg_k(rank, scores, k) / ideal\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                          PERMUTATIONS GENERATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
    "                                int(.30 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
    "                                int(.22 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
    "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
    "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
    "                                int(.10 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
    "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
    "                                ]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
    "    cdef bint match\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    for i in prange(len(container), schedule='static', num_threads=8):\n",
    "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
    "            continue\n",
    "        else:\n",
    "            match = True\n",
    "            for j in range(len(container[i])):\n",
    "                if container[i][j] != array[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
    "    cdef int i\n",
    "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
    "        container[pos][i] = array[i]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
    "    cdef int i = 0\n",
    "    cdef int k\n",
    "    perm = itertools.permutations(source)\n",
    "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
    "    for p in perm:\n",
    "        for k in range(len(p)):\n",
    "            result[i][k] = p[k]\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    cdef int c = 0\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = max(1, int(_min * .5))\n",
    "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
    "    cdef int k\n",
    "    cdef int d\n",
    "    for k in range(count):\n",
    "        perm = query.docs.copy()\n",
    "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
    "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
    "        for d in range(len(first)):\n",
    "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
    "        p = np.concatenate(perm)\n",
    "        if not contained(perms, p):\n",
    "            setrow(perms, start + c, p)\n",
    "            c += 1\n",
    "            if c == limit:\n",
    "                return count - c, start + c\n",
    "        else:\n",
    "            k -= 1\n",
    "    return 0, start + c\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def process_query(query, alllabels):\n",
    "    cdef int carry = 0\n",
    "    fact = factorial(len(query.alldocs))\n",
    "    cdef perms\n",
    "    cdef int last = 0\n",
    "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
    "        # evaluate all possible permutations, each one representing a different ranking\n",
    "        perms = allPerms(query.alldocs, fact)\n",
    "    else:\n",
    "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
    "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
    "        carry, last = perform_permutation(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
    "        if carry != 0:\n",
    "            if not query.alldocs in perms:\n",
    "                perms[last] = query.alldocs\n",
    "        perms = perms[perms.max(axis=1)>=0]\n",
    "    cdef double[:] probs = np.zeros(len(perms))\n",
    "    cdef double[:] ndcgs = np.zeros(len(perms))\n",
    "    query.setperms(perms)\n",
    "    query.setprobs(rank_probabilities(perms, probs, alllabels))\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in range(len(perms)):\n",
    "        ndcgs[i] = ndcg_k(perms[i], alllabels, k, query.idealdcg)\n",
    "    query.setndcgs(ndcgs)\n",
    "    return query\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                           GRADIENTS EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] doc_energy(int[:] rank, double[:] scores, double[:,:] result) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return result\n",
    "    cdef double factor = 4.0 / (m * ((m - 1)**2))\n",
    "    cdef double res\n",
    "    cdef double res_w_scores\n",
    "    cdef int k, pos\n",
    "    for pos in prange(len(rank), schedule='static', num_threads=8):\n",
    "        res = 0.0\n",
    "        res_w_scores = 0.0\n",
    "        for k in range(len(rank)):\n",
    "            if k < pos: \n",
    "                res = res + (pos - k)\n",
    "                res_w_scores = res_w_scores + (pos - k) * (scores[rank[pos]] - scores[rank[k]])\n",
    "            elif k > pos: \n",
    "                res = res + (k - pos)\n",
    "                res_w_scores = res_w_scores + (k - pos) * (scores[rank[k]] - scores[rank[pos]])\n",
    "        result[rank[pos]][0] = factor * res_w_scores\n",
    "        result[rank[pos]][1] = factor * res\n",
    "        #printf(\"result of %i: m %i, factor %f, res %f, res_scores %f, result[0] %f, result[1] %f\\n\", pos, m, factor, res_w_scores, res, result[rank[pos]][0], result[rank[pos]][0])\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:,:] doc_rank_probabilities(int[:,:] sampleSet, double[:,:,:] norm_probs, double[:] scores, double[:,:] accumulator, double[:,:] tmp) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef double grad_norm = 0\n",
    "    cdef double _energy = 0\n",
    "    cdef double grad_energy = 0\n",
    "    cdef int r\n",
    "    cdef int d\n",
    "    cdef int pos\n",
    "    cdef int doc\n",
    "    \n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        doc_energy(sampleSet[r], scores, tmp)\n",
    "        for d in range(len(sampleSet[r])):\n",
    "            doc = sampleSet[r][d]\n",
    "            norm_probs[doc][r][0] = exp(-tmp[doc][0]) # e^{-E}\n",
    "            norm_probs[doc][r][1] = -tmp[doc][1] * norm_probs[doc][r][0] # -E' * e^{-E}\n",
    "            accumulator[doc][0] = accumulator[doc][0] + norm_probs[doc][r][0] # sum(e^{-E})\n",
    "            accumulator[doc][1] = accumulator[doc][1] + norm_probs[doc][r][1] # sum(-E' * e^{-E})\n",
    "        \n",
    "        #_energy, grad_energy = doc_energy(sampleSet[r], scores, pos) # E, E'\n",
    "        #norm_probs[r][0] = exp(-_energy) # e^{-E}\n",
    "        #norm_probs[r][1] = -grad_energy * norm_probs[r][0] # -E' * e^{-E}\n",
    "        #norm += norm_probs[r][0] # sum(e^{-E})\n",
    "        #grad_norm += norm_probs[r][1] # sum(-E' * e^{-E})\n",
    "\n",
    "    for pos in prange(len(sampleSet[0]), schedule='static', num_threads=8):\n",
    "        doc = sampleSet[0][pos]\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "        norm_probs[doc][r][1] = ((norm_probs[doc][r][1] * accumulator[doc][0]) - (norm_probs[doc][r][0] * accumulator[doc][1])) / (accumulator[doc][0]**2)\n",
    "        norm_probs[doc][r][0] = norm_probs[doc][r][0] / accumulator[doc][0] # e^{-E} / sum(e^{-E})\n",
    "        \n",
    "    #for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "    #    norm_probs[r][0] = norm_probs[r][0] / norm # e^{-E} / sum(e^{-E})\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "    #    norm_probs[r][1] = ((norm_probs[r][1] * norm) - (norm_probs[r][0] * grad_norm)) / (norm**2) \n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_cross_entropy(int[:,:] perms, double[:] probs, double[:,:,:] scores_probs, double[:,:] entropies) nogil:\n",
    "    cdef int i, doc, j\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            entropies[doc][0] = entropies[doc][0] + -probs[j] * log(scores_probs[doc][j][0])\n",
    "            entropies[doc][1] = entropies[doc][1] + -1 * (probs[j] / scores_probs[doc][j][0]) * scores_probs[doc][j][1]\n",
    "    return entropies\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_monte_carlo_gain(int[:,:] perms, double[:,:,:] scores_probs, double[:] scores, double[:] ndcgs, double ideal, double[:,:] gains) nogil:\n",
    "    cdef int i, doc, j\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            gains[doc][0] = gains[doc][0] + scores_probs[doc][j][0] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "            gains[doc][1] = gains[doc][1] + scores_probs[doc][j][1] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "    return gains \n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank_grads(queries, preds): \n",
    "    cdef double lam = .9\n",
    "    cdef double[:] gain = np.ones_like(preds)\n",
    "    cdef double[:] hess = np.ones_like(preds) \n",
    "    \n",
    "    cdef int d, i\n",
    "    cdef double[:,:,:] score_probs\n",
    "    cdef double[:,:] accumulator = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] tmp = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] gains = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] entropies = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros((len(preds), len(q.probs), 2), dtype=np.double)\n",
    "        score_probs = doc_rank_probabilities(q.perms, score_probs, preds, accumulator, tmp)\n",
    "        gains = grad_monte_carlo_gain(q.perms, score_probs, preds, q.ndcgs, q.idealdcg, gains) \n",
    "        entropies = grad_cross_entropy(q.perms, q.probs, score_probs, entropies)\n",
    "    for i in range(len(gain)):\n",
    "        gain[i] = (lam * gains[i][1]) - ((1-lam) * entropies[i][1])\n",
    "    return gain, hess\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                            METRIC EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double energy(int[:] rank, double[:] scores) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return 0\n",
    "    cdef double factor = 4 / (m * ((m - 1)**2))\n",
    "    cdef double res = 0\n",
    "    cdef int j, k\n",
    "    for k in prange(m, schedule='static', num_threads=8):\n",
    "        for j in range(k + 1, m):\n",
    "            res += (j - k) * (scores[rank[j]] - scores[rank[k]])\n",
    "    return factor * res\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] rank_probabilities(int[:,:] sampleSet, double[:] norm_probs, double[:] scores) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef int r\n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = exp(-energy(sampleSet[r], scores))\n",
    "        norm += norm_probs[r]\n",
    "    for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = norm_probs[r] / norm\n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double cross_entropy(double[:] probs, double[:] scores_probs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(len(probs), schedule='static', num_threads=8):\n",
    "        result += probs[i] * log(scores_probs[i])\n",
    "    return -result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double monte_carlo_gain(int[:,:] perms, double[:] scores_probs, double[:] ndcgs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms), schedule='static', num_threads=8):\n",
    "        result += scores_probs[i] * ndcgs[i]\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank(queries, preds):\n",
    "    cdef double lam = .9\n",
    "    cdef double[:] score_probs\n",
    "    cdef double mc = 0\n",
    "    cdef double ce = 0\n",
    "    cdef double _mc, _ce\n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros(len(q.probs), dtype=np.double)\n",
    "        score_probs = rank_probabilities(q.perms, score_probs, preds)\n",
    "        mc += monte_carlo_gain(q.perms, score_probs, q.ndcgs) \n",
    "        ce += cross_entropy(q.probs, score_probs)\n",
    "    return (lam * mc) - ((1-lam) * ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/train.txt\n",
      "validation file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/vali.txt\n",
      "test file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/test.txt\n",
      "loading datasets... \n",
      "train dataset loading took 71.453125 s\n",
      "validation dataset loading took 22.765625 s\n",
      "test dataset loading took 22.4375 s\n",
      "converting datasets to LightGBM format... \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = retrieveFileNames()\n",
    "\n",
    "print(\"training file: \" + train_file)\n",
    "print(\"validation file: \" + valid_file)\n",
    "print(\"test file: \" + test_file)\n",
    "    \n",
    "print(\"loading datasets... \")\n",
    "import time\n",
    "start = time.process_time()\n",
    "train_dataset = loadDataset(train_file)\n",
    "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "valid_dataset = loadDataset(valid_file)\n",
    "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "test_dataset = loadDataset(test_file)\n",
    "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "\n",
    "import itertools\n",
    "print(\"converting datasets to LightGBM format... \")\n",
    "train_lgb = loadLightGBM(train_dataset)\n",
    "valid_lgb = loadLightGBM(valid_dataset)\n",
    "test_lgb = loadLightGBM(test_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating query-documents mappings...\n",
      "done\n",
      "creating sample sets...\n",
      "sample set creation took 70.5625 s\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "print(\"creating query-documents mappings...\")\n",
    "train_id = file_len(train_file)\n",
    "vali_id = file_len(valid_file)\n",
    "test_id = file_len(test_file)\n",
    "ds_to_queries = {}\n",
    "ds_to_queries[train_id] = mapQueryToDocuments(train_dataset)\n",
    "ds_to_queries[vali_id] = mapQueryToDocuments(valid_dataset)\n",
    "ds_to_queries[test_id] = mapQueryToDocuments(test_dataset)\n",
    "#queries, alllabels = mapQueryToDocuments(train_dataset)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"creating sample sets...\")\n",
    "start = time.process_time()\n",
    "for ds_id, queries in ds_to_queries.items():\n",
    "    for q in queries[0].values():\n",
    "        process_query(q, queries[1])\n",
    "#for q in queries.values():\n",
    "#    queries[q.qid] = process_query(q, alllabels)\n",
    "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lightgbm...\n",
      "MSE grads: 0.5592629096559084\n",
      "MSE eval: 0.5406102691116346\n",
      "MSE eval: 0.5549749986796221\n",
      "MSE eval: 0.5580458431656531\n",
      "[1]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.54061\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.554975\ttest's ndcg@10: 0.252445\ttest's MSE: 0.558046\n",
      "MSE grads: 0.5406102691116346\n",
      "MSE eval: 0.5228330623578503\n",
      "MSE eval: 0.5366148919002286\n",
      "MSE eval: 0.5400070299462415\n",
      "[2]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.522833\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.536615\ttest's ndcg@10: 0.252445\ttest's MSE: 0.540007\n",
      "MSE grads: 0.5228330623578503\n",
      "MSE eval: 0.5059312875210156\n",
      "MSE eval: 0.5191463791304102\n",
      "MSE eval: 0.5228395981236421\n",
      "[3]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.505931\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.519146\ttest's ndcg@10: 0.252445\ttest's MSE: 0.52284\n",
      "MSE grads: 0.5059312875210156\n",
      "MSE eval: 0.489904943561193\n",
      "MSE eval: 0.502569457238632\n",
      "MSE eval: 0.5065435450724581\n",
      "[4]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.489905\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.502569\ttest's ndcg@10: 0.252445\ttest's MSE: 0.506544\n",
      "MSE grads: 0.489904943561193\n",
      "MSE eval: 0.47475403025749\n",
      "MSE eval: 0.48688412380968416\n",
      "MSE eval: 0.49111886883133743\n",
      "[5]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.474754\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.486884\ttest's ndcg@10: 0.252445\ttest's MSE: 0.491119\n",
      "MSE grads: 0.47475403025749\n",
      "MSE eval: 0.46047854819995854\n",
      "MSE eval: 0.4720903771316603\n",
      "MSE eval: 0.4765655680911569\n",
      "[6]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.460479\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.47209\ttest's ndcg@10: 0.252445\ttest's MSE: 0.476566\n",
      "MSE grads: 0.46047854819995854\n",
      "MSE eval: 0.4470784987803169\n",
      "MSE eval: 0.4581882161814424\n",
      "MSE eval: 0.4628836421817179\n",
      "[7]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.447078\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.458188\ttest's ndcg@10: 0.252445\ttest's MSE: 0.462884\n",
      "MSE grads: 0.4470784987803169\n",
      "MSE eval: 0.43455388418390695\n",
      "MSE eval: 0.4451776406111429\n",
      "MSE eval: 0.4500730910590166\n",
      "[8]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.434554\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.445178\ttest's ndcg@10: 0.252445\ttest's MSE: 0.450073\n",
      "MSE grads: 0.43455388418390695\n",
      "MSE eval: 0.4229047073805879\n",
      "MSE eval: 0.43305865073242933\n",
      "MSE eval: 0.4381339152907163\n",
      "[9]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.422905\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.433059\ttest's ndcg@10: 0.252445\ttest's MSE: 0.438134\n",
      "MSE grads: 0.4229047073805879\n",
      "MSE eval: 0.41213097212450084\n",
      "MSE eval: 0.42183124750997053\n",
      "MSE eval: 0.4270661160499585\n",
      "[10]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.412131\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.421831\ttest's ndcg@10: 0.252445\ttest's MSE: 0.427066\n",
      "MSE grads: 0.41213097212450084\n",
      "MSE eval: 0.4022326829433412\n",
      "MSE eval: 0.41149543254309184\n",
      "MSE eval: 0.4168696950988399\n",
      "[11]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.402233\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.411495\ttest's ndcg@10: 0.252445\ttest's MSE: 0.41687\n",
      "MSE grads: 0.4022326829433412\n",
      "MSE eval: 0.39320984514228147\n",
      "MSE eval: 0.4020512080619102\n",
      "MSE eval: 0.40754465478450047\n",
      "[12]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.39321\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.402051\ttest's ndcg@10: 0.252445\ttest's MSE: 0.407545\n",
      "MSE grads: 0.39320984514228147\n",
      "MSE eval: 0.38506246479749884\n",
      "MSE eval: 0.3934985769125919\n",
      "MSE eval: 0.39909099802522147\n",
      "[13]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.385062\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.393499\ttest's ndcg@10: 0.252445\ttest's MSE: 0.399091\n",
      "MSE grads: 0.38506246479749884\n",
      "MSE eval: 0.37779054875894785\n",
      "MSE eval: 0.3858375425508377\n",
      "MSE eval: 0.39150872830490147\n",
      "[14]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.377791\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.385838\ttest's ndcg@10: 0.252445\ttest's MSE: 0.391509\n",
      "MSE grads: 0.37779054875894785\n",
      "MSE eval: 0.3713941046497817\n",
      "MSE eval: 0.37906810903185756\n",
      "MSE eval: 0.38479784966356995\n",
      "[15]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.371394\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.379068\ttest's ndcg@10: 0.252445\ttest's MSE: 0.384798\n",
      "MSE grads: 0.3713941046497817\n",
      "MSE eval: 0.36587314086879585\n",
      "MSE eval: 0.37319028100191665\n",
      "MSE eval: 0.37895836668923977\n",
      "[16]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.365873\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.37319\ttest's ndcg@10: 0.252445\ttest's MSE: 0.378958\n",
      "MSE grads: 0.36587314086879585\n",
      "MSE eval: 0.36122766659633937\n",
      "MSE eval: 0.3682040636932813\n",
      "MSE eval: 0.37399028451371474\n",
      "[17]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.361228\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.368204\ttest's ndcg@10: 0.252445\ttest's MSE: 0.37399\n",
      "MSE grads: 0.36122766659633937\n",
      "MSE eval: 0.3574576917967736\n",
      "MSE eval: 0.3641094629146941\n",
      "MSE eval: 0.3698936088035028\n",
      "[18]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.357458\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.364109\ttest's ndcg@10: 0.252445\ttest's MSE: 0.369894\n",
      "MSE grads: 0.3574576917967736\n",
      "MSE eval: 0.3545632272275923\n",
      "MSE eval: 0.36090648504743456\n",
      "MSE eval: 0.3666683457559902\n",
      "[19]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.354563\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.360906\ttest's ndcg@10: 0.252445\ttest's MSE: 0.366668\n",
      "MSE grads: 0.3545632272275923\n",
      "MSE eval: 0.352544284445886\n",
      "MSE eval: 0.35859513703813806\n",
      "MSE eval: 0.3643145020932889\n",
      "[20]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.352544\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.358595\ttest's ndcg@10: 0.252445\ttest's MSE: 0.364315\n",
      "MSE grads: 0.352544284445886\n",
      "MSE eval: 0.3514008758198516\n",
      "MSE eval: 0.3571754263954769\n",
      "MSE eval: 0.36283208505863507\n",
      "[21]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.351401\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.357175\ttest's ndcg@10: 0.252445\ttest's MSE: 0.362832\n",
      "MSE grads: 0.3514008758198516\n",
      "MSE eval: 0.3511330145395965\n",
      "MSE eval: 0.356647361184935\n",
      "MSE eval: 0.3622211024117854\n",
      "[22]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.351133\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.356647\ttest's ndcg@10: 0.252445\ttest's MSE: 0.362221\n",
      "MSE grads: 0.3511330145395965\n",
      "MSE eval: 0.3517407146291886\n",
      "MSE eval: 0.3570109500244754\n",
      "MSE eval: 0.36248156242489205\n",
      "[23]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.351741\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.357011\ttest's ndcg@10: 0.252445\ttest's MSE: 0.362482\n",
      "MSE grads: 0.3517407146291886\n",
      "MSE eval: 0.35322399096347706\n",
      "MSE eval: 0.35826620208263005\n",
      "MSE eval: 0.36361347388090004\n",
      "[24]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.353224\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.358266\ttest's ndcg@10: 0.252445\ttest's MSE: 0.363613\n",
      "MSE grads: 0.35322399096347706\n",
      "MSE eval: 0.35558285928408867\n",
      "MSE eval: 0.36041312707560763\n",
      "MSE eval: 0.36561684607039735\n",
      "[25]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.355583\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.360413\ttest's ndcg@10: 0.252445\ttest's MSE: 0.365617\n",
      "MSE grads: 0.35558285928408867\n",
      "MSE eval: 0.35881733621970624\n",
      "MSE eval: 0.36345173526658625\n",
      "MSE eval: 0.3684916887916965\n",
      "[26]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.358817\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.363452\ttest's ndcg@10: 0.252445\ttest's MSE: 0.368492\n",
      "MSE grads: 0.35881733621970624\n",
      "MSE eval: 0.3629274393020415\n",
      "MSE eval: 0.36738203745952397\n",
      "MSE eval: 0.37223801234422993\n",
      "[27]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.362927\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.367382\ttest's ndcg@10: 0.252445\ttest's MSE: 0.372238\n",
      "MSE grads: 0.3629274393020415\n",
      "MSE eval: 0.3679131869949039\n",
      "MSE eval: 0.3722040450044858\n",
      "MSE eval: 0.37685582753426156\n",
      "[28]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.367913\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.372204\ttest's ndcg@10: 0.252445\ttest's MSE: 0.376856\n",
      "MSE grads: 0.3679131869949039\n",
      "MSE eval: 0.37377459871470803\n",
      "MSE eval: 0.37791776979257496\n",
      "MSE eval: 0.3823451456694714\n",
      "[29]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.373775\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.377918\ttest's ndcg@10: 0.252445\ttest's MSE: 0.382345\n",
      "MSE grads: 0.37377459871470803\n",
      "MSE eval: 0.38051169486186087\n",
      "MSE eval: 0.3845232242602514\n",
      "MSE eval: 0.3887059785634042\n",
      "[30]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.380512\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.384523\ttest's ndcg@10: 0.252445\ttest's MSE: 0.388706\n",
      "MSE grads: 0.38051169486186087\n",
      "MSE eval: 0.3881244968481829\n",
      "MSE eval: 0.39202042138775856\n",
      "MSE eval: 0.3959383385338274\n",
      "[31]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.388124\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.39202\ttest's ndcg@10: 0.252445\ttest's MSE: 0.395938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 0.3881244968481829\n",
      "MSE eval: 0.3966130271277308\n",
      "MSE eval: 0.400409374698198\n",
      "MSE eval: 0.40404223840178466\n",
      "[32]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.396613\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.400409\ttest's ndcg@10: 0.252445\ttest's MSE: 0.404042\n",
      "MSE grads: 0.3966130271277308\n",
      "MSE eval: 0.40597730923763364\n",
      "MSE eval: 0.4096900982649792\n",
      "MSE eval: 0.4130176914983275\n",
      "[33]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.405977\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.40969\ttest's ndcg@10: 0.252445\ttest's MSE: 0.413018\n",
      "MSE grads: 0.40597730923763364\n",
      "MSE eval: 0.4162173678337872\n",
      "MSE eval: 0.41986260671191267\n",
      "MSE eval: 0.4228647116652129\n",
      "[34]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.416217\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.419863\ttest's ndcg@10: 0.252445\ttest's MSE: 0.422865\n",
      "MSE grads: 0.4162173678337872\n",
      "MSE eval: 0.42733322872900914\n",
      "MSE eval: 0.4309269152124871\n",
      "MSE eval: 0.4335833132532618\n",
      "[35]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.427333\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.430927\ttest's ndcg@10: 0.252445\ttest's MSE: 0.433583\n",
      "MSE grads: 0.42733322872900914\n",
      "MSE eval: 0.4393249189458192\n",
      "MSE eval: 0.4428830395011431\n",
      "MSE eval: 0.4451735111336814\n",
      "[36]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.439325\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.442883\ttest's ndcg@10: 0.252445\ttest's MSE: 0.445174\n",
      "MSE grads: 0.4393249189458192\n",
      "MSE eval: 0.4521924667593004\n",
      "MSE eval: 0.45573099587241195\n",
      "MSE eval: 0.45763532069671253\n",
      "[37]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.452192\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.455731\ttest's ndcg@10: 0.252445\ttest's MSE: 0.457635\n",
      "MSE grads: 0.4521924667593004\n",
      "MSE eval: 0.46593590174920074\n",
      "MSE eval: 0.46947080118439105\n",
      "MSE eval: 0.4709687578546442\n",
      "[38]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.465936\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.469471\ttest's ndcg@10: 0.252445\ttest's MSE: 0.470969\n",
      "MSE grads: 0.46593590174920074\n",
      "MSE eval: 0.48055525486287687\n",
      "MSE eval: 0.4841024728712652\n",
      "MSE eval: 0.48517383905385125\n",
      "[39]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.480555\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.484102\ttest's ndcg@10: 0.252445\ttest's MSE: 0.485174\n",
      "MSE grads: 0.48055525486287687\n",
      "MSE eval: 0.4960505584686851\n",
      "MSE eval: 0.49962602894103325\n",
      "MSE eval: 0.5002505812725735\n",
      "[40]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.496051\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.499626\ttest's ndcg@10: 0.252445\ttest's MSE: 0.500251\n",
      "MSE grads: 0.4960505584686851\n",
      "MSE eval: 0.51242184642772\n",
      "MSE eval: 0.5160414879879762\n",
      "MSE eval: 0.5161990020320066\n",
      "[41]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.512422\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.516041\ttest's ndcg@10: 0.252445\ttest's MSE: 0.516199\n",
      "MSE grads: 0.51242184642772\n",
      "MSE eval: 0.5296691541589738\n",
      "MSE eval: 0.5333488691946218\n",
      "MSE eval: 0.5330191193983467\n",
      "[42]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.529669\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.533349\ttest's ndcg@10: 0.252445\ttest's MSE: 0.533019\n",
      "MSE grads: 0.5296691541589738\n",
      "MSE eval: 0.5477925187243539\n",
      "MSE eval: 0.5515481923473314\n",
      "MSE eval: 0.5507109519976712\n",
      "[43]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.547793\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.551548\ttest's ndcg@10: 0.252445\ttest's MSE: 0.550711\n",
      "MSE grads: 0.5477925187243539\n",
      "MSE eval: 0.566791978903962\n",
      "MSE eval: 0.5706394778383441\n",
      "MSE eval: 0.569274519016685\n",
      "[44]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.566792\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.570639\ttest's ndcg@10: 0.252445\ttest's MSE: 0.569275\n",
      "MSE grads: 0.566791978903962\n",
      "MSE eval: 0.5866675752873036\n",
      "MSE eval: 0.5906227466760807\n",
      "MSE eval: 0.5887098402129866\n",
      "[45]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.586668\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.590623\ttest's ndcg@10: 0.252445\ttest's MSE: 0.58871\n",
      "MSE grads: 0.5866675752873036\n",
      "MSE eval: 0.6074193503732856\n",
      "MSE eval: 0.6114980205010659\n",
      "MSE eval: 0.609016935929918\n",
      "[46]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.607419\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.611498\ttest's ndcg@10: 0.252445\ttest's MSE: 0.609017\n",
      "MSE grads: 0.6074193503732856\n",
      "MSE eval: 0.629047348665585\n",
      "MSE eval: 0.6332653215848288\n",
      "MSE eval: 0.6301958270949283\n",
      "[47]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.629047\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.633265\ttest's ndcg@10: 0.252445\ttest's MSE: 0.630196\n",
      "MSE grads: 0.629047348665585\n",
      "MSE eval: 0.65155161679004\n",
      "MSE eval: 0.6559246728510372\n",
      "MSE eval: 0.6522465352393397\n",
      "[48]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.651552\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.655925\ttest's ndcg@10: 0.252445\ttest's MSE: 0.652247\n",
      "MSE grads: 0.65155161679004\n",
      "MSE eval: 0.6749322036172386\n",
      "MSE eval: 0.6794760978859603\n",
      "MSE eval: 0.6751690825082194\n",
      "[49]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.674932\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.679476\ttest's ndcg@10: 0.252445\ttest's MSE: 0.675169\n",
      "MSE grads: 0.6749322036172386\n",
      "MSE eval: 0.6991891603873783\n",
      "MSE eval: 0.7039196209473624\n",
      "MSE eval: 0.6989634916681153\n",
      "[50]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.699189\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.70392\ttest's ndcg@10: 0.252445\ttest's MSE: 0.698963\n",
      "MSE grads: 0.6991891603873783\n",
      "MSE eval: 0.724322540851362\n",
      "MSE eval: 0.7292552669784468\n",
      "MSE eval: 0.7236297861199116\n",
      "[51]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.724323\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.729255\ttest's ndcg@10: 0.252445\ttest's MSE: 0.72363\n",
      "MSE grads: 0.724322540851362\n",
      "MSE eval: 0.7503324014278745\n",
      "MSE eval: 0.7554830616257283\n",
      "MSE eval: 0.7491679899159093\n",
      "[52]\ttrain's ndcg@10: 0.249027\ttrain's MSE: 0.750332\tvalid's ndcg@10: 0.253556\tvalid's MSE: 0.755483\ttest's ndcg@10: 0.252445\ttest's MSE: 0.749168\n",
      "MSE grads: 0.7503324014278745\n",
      "MSE eval: 0.777113896553893\n",
      "MSE eval: 0.7824889384516655\n",
      "MSE eval: 0.7754861156135638\n",
      "[53]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.777114\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.782489\ttest's ndcg@10: 0.261186\ttest's MSE: 0.775486\n",
      "MSE grads: 0.777113896553893\n",
      "MSE eval: 0.8047719272993427\n",
      "MSE eval: 0.8103864895382453\n",
      "MSE eval: 0.802677356353126\n",
      "[54]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.804772\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.810386\ttest's ndcg@10: 0.261186\ttest's MSE: 0.802677\n",
      "MSE grads: 0.8047719272993427\n",
      "MSE eval: 0.8333065638696849\n",
      "MSE eval: 0.839175748426183\n",
      "MSE eval: 0.8307417434903888\n",
      "[55]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.833307\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.839176\ttest's ndcg@10: 0.261186\ttest's MSE: 0.830742\n",
      "MSE grads: 0.8333065638696849\n",
      "MSE eval: 0.8627178802337675\n",
      "MSE eval: 0.8688567498255495\n",
      "MSE eval: 0.8596793094598908\n",
      "[56]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.862718\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.868857\ttest's ndcg@10: 0.261186\ttest's MSE: 0.859679\n",
      "MSE grads: 0.8627178802337675\n",
      "MSE eval: 0.8930059543652655\n",
      "MSE eval: 0.8994295296503412\n",
      "MSE eval: 0.8894900878063203\n",
      "[57]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.893006\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.89943\ttest's ndcg@10: 0.261186\ttest's MSE: 0.88949\n",
      "MSE grads: 0.8930059543652655\n",
      "MSE eval: 0.924170868503399\n",
      "MSE eval: 0.930894125052353\n",
      "MSE eval: 0.9201741132147793\n",
      "[58]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.924171\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.930894\ttest's ndcg@10: 0.261186\ttest's MSE: 0.920174\n",
      "MSE grads: 0.924170868503399\n",
      "MSE eval: 0.9562127094548106\n",
      "MSE eval: 0.9632505744748866\n",
      "MSE eval: 0.9517314215609761\n",
      "[59]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.956213\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.963251\ttest's ndcg@10: 0.261186\ttest's MSE: 0.951731\n",
      "MSE grads: 0.9562127094548106\n",
      "MSE eval: 0.9891315689074794\n",
      "MSE eval: 0.9964989176948078\n",
      "MSE eval: 0.9841620499501373\n",
      "[60]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 0.989132\tvalid's ndcg@10: 0.261396\tvalid's MSE: 0.996499\ttest's ndcg@10: 0.261186\ttest's MSE: 0.984162\n",
      "MSE grads: 0.9891315689074794\n",
      "MSE eval: 1.0227848126976067\n",
      "MSE eval: 1.0307215068877067\n",
      "MSE eval: 1.017505671834251\n",
      "[61]\ttrain's ndcg@10: 0.2572\ttrain's MSE: 1.02278\tvalid's ndcg@10: 0.261396\tvalid's MSE: 1.03072\ttest's ndcg@10: 0.261186\ttest's MSE: 1.01751\n",
      "MSE grads: 1.0227848126976067\n",
      "MSE eval: 1.0573127389781405\n",
      "MSE eval: 1.0658401810467686\n",
      "MSE eval: 1.0517245800101134\n",
      "[62]\ttrain's ndcg@10: 0.257198\ttrain's MSE: 1.05731\tvalid's ndcg@10: 0.261396\tvalid's MSE: 1.06584\ttest's ndcg@10: 0.261186\ttest's MSE: 1.05172\n",
      "MSE grads: 1.0573127389781405\n",
      "MSE eval: 1.092715403664906\n",
      "MSE eval: 1.1018550054223246\n",
      "MSE eval: 1.0868188311398044\n",
      "[63]\ttrain's ndcg@10: 0.25733\ttrain's MSE: 1.09272\tvalid's ndcg@10: 0.261785\tvalid's MSE: 1.10186\ttest's ndcg@10: 0.261128\ttest's MSE: 1.08682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 1.092715403664906\n",
      "MSE eval: 1.1289928650892553\n",
      "MSE eval: 1.1387660481415542\n",
      "MSE eval: 1.1227884843396307\n",
      "[64]\ttrain's ndcg@10: 0.25733\ttrain's MSE: 1.12899\tvalid's ndcg@10: 0.261785\tvalid's MSE: 1.13877\ttest's ndcg@10: 0.261128\ttest's MSE: 1.12279\n",
      "MSE grads: 1.1289928650892553\n",
      "MSE eval: 1.1661451841288966\n",
      "MSE eval: 1.1765733803643355\n",
      "MSE eval: 1.1596336013130606\n",
      "[65]\ttrain's ndcg@10: 0.25733\ttrain's MSE: 1.16615\tvalid's ndcg@10: 0.261785\tvalid's MSE: 1.17657\ttest's ndcg@10: 0.261128\ttest's MSE: 1.15963\n",
      "MSE grads: 1.1661451841288966\n",
      "MSE eval: 1.2041724243174836\n",
      "MSE eval: 1.2152770764164391\n",
      "MSE eval: 1.1973542464613158\n",
      "[66]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.20417\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.21528\ttest's ndcg@10: 0.261353\ttest's MSE: 1.19735\n",
      "MSE grads: 1.2041724243174836\n",
      "MSE eval: 1.2430746519871232\n",
      "MSE eval: 1.2548772139583773\n",
      "MSE eval: 1.2359504870284814\n",
      "[67]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.24307\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.25488\ttest's ndcg@10: 0.261353\ttest's MSE: 1.23595\n",
      "MSE grads: 1.2430746519871232\n",
      "MSE eval: 1.2828519364088444\n",
      "MSE eval: 1.2953738741539562\n",
      "MSE eval: 1.2754223932432907\n",
      "[68]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.28285\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.29537\ttest's ndcg@10: 0.261353\ttest's MSE: 1.27542\n",
      "MSE grads: 1.2828519364088444\n",
      "MSE eval: 1.323504349929191\n",
      "MSE eval: 1.3367671418361906\n",
      "MSE eval: 1.315770038460231\n",
      "[69]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.3235\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.33677\ttest's ndcg@10: 0.261353\ttest's MSE: 1.31577\n",
      "MSE grads: 1.323504349929191\n",
      "MSE eval: 1.3650319681477558\n",
      "MSE eval: 1.3790571057164698\n",
      "MSE eval: 1.3569934993382995\n",
      "[70]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.36503\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.37906\ttest's ndcg@10: 0.261353\ttest's MSE: 1.35699\n",
      "MSE grads: 1.3650319681477558\n",
      "MSE eval: 1.4074322299055824\n",
      "MSE eval: 1.4222021269460972\n",
      "MSE eval: 1.3990813393759676\n",
      "[71]\ttrain's ndcg@10: 0.257621\ttrain's MSE: 1.40743\tvalid's ndcg@10: 0.262291\tvalid's MSE: 1.4222\ttest's ndcg@10: 0.261353\ttest's MSE: 1.39908\n",
      "MSE grads: 1.4074322299055824\n",
      "MSE eval: 1.4507078753489338\n",
      "MSE eval: 1.4662424596521164\n",
      "MSE eval: 1.4420448085142492\n",
      "[72]\ttrain's ndcg@10: 0.257602\ttrain's MSE: 1.45071\tvalid's ndcg@10: 0.262608\tvalid's MSE: 1.46624\ttest's ndcg@10: 0.261301\ttest's MSE: 1.44204\n",
      "MSE grads: 1.4507078753489338\n",
      "MSE eval: 1.4948321599873935\n",
      "MSE eval: 1.5111175576928608\n",
      "MSE eval: 1.4858495191247065\n",
      "[73]\ttrain's ndcg@10: 0.257563\ttrain's MSE: 1.49483\tvalid's ndcg@10: 0.262572\tvalid's MSE: 1.51112\ttest's ndcg@10: 0.261292\ttest's MSE: 1.48585\n",
      "MSE grads: 1.4948321599873935\n",
      "MSE eval: 1.5397113629278913\n",
      "MSE eval: 1.5569837240729651\n",
      "MSE eval: 1.5305449778932134\n",
      "[74]\ttrain's ndcg@10: 0.260477\ttrain's MSE: 1.53971\tvalid's ndcg@10: 0.264696\tvalid's MSE: 1.55698\ttest's ndcg@10: 0.263802\ttest's MSE: 1.53054\n",
      "MSE grads: 1.5397113629278913\n",
      "MSE eval: 1.5858256504120505\n",
      "MSE eval: 1.6040683595941223\n",
      "MSE eval: 1.5767277756612716\n",
      "[75]\ttrain's ndcg@10: 0.26115\ttrain's MSE: 1.58583\tvalid's ndcg@10: 0.265463\tvalid's MSE: 1.60407\ttest's ndcg@10: 0.264921\ttest's MSE: 1.57673\n",
      "MSE grads: 1.5858256504120505\n",
      "MSE eval: 1.6335150627184292\n",
      "MSE eval: 1.652700664792408\n",
      "MSE eval: 1.6248402048914201\n",
      "[76]\ttrain's ndcg@10: 0.26115\ttrain's MSE: 1.63352\tvalid's ndcg@10: 0.265463\tvalid's MSE: 1.6527\ttest's ndcg@10: 0.264921\ttest's MSE: 1.62484\n",
      "MSE grads: 1.6335150627184292\n",
      "MSE eval: 1.6832266019664315\n",
      "MSE eval: 1.7032959844358557\n",
      "MSE eval: 1.675550739904996\n",
      "[77]\ttrain's ndcg@10: 0.26115\ttrain's MSE: 1.68323\tvalid's ndcg@10: 0.265463\tvalid's MSE: 1.7033\ttest's ndcg@10: 0.264921\ttest's MSE: 1.67555\n",
      "MSE grads: 1.6832266019664315\n",
      "MSE eval: 1.7361618378454144\n",
      "MSE eval: 1.756971882697126\n",
      "MSE eval: 1.7306547521783946\n",
      "[78]\ttrain's ndcg@10: 0.26115\ttrain's MSE: 1.73616\tvalid's ndcg@10: 0.265463\tvalid's MSE: 1.75697\ttest's ndcg@10: 0.264921\ttest's MSE: 1.73065\n",
      "MSE grads: 1.7361618378454144\n",
      "MSE eval: 1.7935532898559658\n",
      "MSE eval: 1.8160881977978038\n",
      "MSE eval: 1.7871200677291856\n",
      "[79]\ttrain's ndcg@10: 0.261134\ttrain's MSE: 1.79355\tvalid's ndcg@10: 0.265357\tvalid's MSE: 1.81609\ttest's ndcg@10: 0.264759\ttest's MSE: 1.78712\n",
      "MSE grads: 1.7935532898559658\n",
      "MSE eval: 1.8970245147347053\n",
      "MSE eval: 1.929545380074958\n",
      "MSE eval: 1.8772403771200141\n",
      "[80]\ttrain's ndcg@10: 0.261964\ttrain's MSE: 1.89702\tvalid's ndcg@10: 0.265479\tvalid's MSE: 1.92955\ttest's ndcg@10: 0.265178\ttest's MSE: 1.87724\n",
      "MSE grads: 1.8970245147347053\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[81]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[82]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[83]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[84]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[85]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[86]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[87]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[88]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[89]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[90]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[91]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[92]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[93]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[94]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[95]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[96]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[97]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[98]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[99]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "MSE grads: 81.48011406103772\n",
      "MSE eval: 81.48011406103772\n",
      "MSE eval: 146.9491215800448\n",
      "MSE eval: 119.46479481942379\n",
      "[100]\ttrain's ndcg@10: 0.263109\ttrain's MSE: 81.4801\tvalid's ndcg@10: 0.265967\tvalid's MSE: 146.949\ttest's ndcg@10: 0.266768\ttest's MSE: 119.465\n",
      "training took 10077.125 s\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Error')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluation(preds, train_data):\n",
    "    global ds_to_queries\n",
    "    #bz = eval_boltzrank(ds_to_queries[len(preds)][0], preds)\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE eval: \" + str(avg_mse))\n",
    "    return 'MSE', avg_mse, False\n",
    "\n",
    "def compute_grads(preds, train_data): \n",
    "    global ds_to_queries\n",
    "    global train_id\n",
    "    gain, hess = eval_boltzrank_grads(ds_to_queries[train_id][0], preds)\n",
    "    gain = np.asarray(gain)\n",
    "    hess = np.asarray(hess)\n",
    "    #print(\"min \" + str(np.min(gain)) + \" max \" + str(np.max(gain)) + \" mean \" + str(np.mean(gain)) + \" std \" + str(np.std(gain)))\n",
    "    #print(\"preds \" + str(preds))\n",
    "    #print(\"gain \" + str(gain))\n",
    "    #print(\"hess \" + str(hess))\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE grads: \" + str(avg_mse))\n",
    "    return gain, hess\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'metric': ['ndcg'],# ['None']\n",
    "    'ndcg_eval_at': 10\n",
    "}    \n",
    "\n",
    "print(\"training lightgbm...\")\n",
    "start = time.process_time()\n",
    "lgbm_info = {}\n",
    "lgbm_model = lightgbm.train(params, train_lgb, num_boost_round=100,\n",
    "                            feval = evaluation,\n",
    "                            fobj  = compute_grads,\n",
    "                            valid_sets   = [train_lgb, valid_lgb, test_lgb], \n",
    "                            valid_names  = [\"train\", \"valid\", \"test\"],\n",
    "                            evals_result = lgbm_info,\n",
    "                            verbose_eval = 1)\n",
    "print(\"training took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(lgbm_info['train'][METRIC_NAME], label='training')\n",
    "plt.plot(lgbm_info['valid'][METRIC_NAME], label='validation')\n",
    "plt.plot(lgbm_info['test'][METRIC_NAME], label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"# Trees\")\n",
    "plt.ylabel(METRIC_NAME)\n",
    "plt.title(\"Model Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(q, k):\n",
    "    indexes = set(range(0, len(q.perms)))\n",
    "    indexes.remove(k)\n",
    "    result = []\n",
    "    for i in range(len(q.perms[k])):\n",
    "        tmp = set(indexes)\n",
    "        for j in tmp:\n",
    "            if q.perms[k][i] != q.perms[j][i]:\n",
    "                indexes.remove(j)\n",
    "    for w in indexes:\n",
    "        if w < k:\n",
    "            result.append((w, k))\n",
    "        else: \n",
    "            result.append((k,w))\n",
    "    return result\n",
    "\n",
    "def checkRepetitions():\n",
    "    global queries\n",
    "    same = dict()\n",
    "    for q in queries.values():\n",
    "        for i in range(len(q.perms)):\n",
    "            r = check(q, i)\n",
    "            if len(r) != 0:\n",
    "                if not q.qid in same.keys():\n",
    "                    same[q.qid] = set()\n",
    "                for t in r:\n",
    "                    same[q.qid].add(t)\n",
    "\n",
    "    print(str(len(same.keys())) + \"/\" + str(len(queries.keys())) + \" queries have duplicate permutations\")\n",
    "    for q, s in same.items():\n",
    "        print(\"query \" + str(q) + \" has repeated permutations: \" + str(s))\n",
    "        \n",
    "#checkRepetitions()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4dcdedcd6052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for query in queries.values():\n",
    "    for prob in query.probs:\n",
    "        if not prob in freq.keys():\n",
    "            freq[prob] = 0\n",
    "        freq[prob] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for prob, f in sorted(freq.items()):\n",
    "    x.append(prob)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel(\"# rank\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"probabilities of the \" + str(totperms) + \" permutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "totperms = 0\n",
    "for query in queries.values():\n",
    "    for ndcg in query.ndcgs:\n",
    "        totperms += 1\n",
    "        if not ndcg in freq.keys():\n",
    "            freq[ndcg] = 0\n",
    "        freq[ndcg] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for ndcg, f in sorted(freq.items()):\n",
    "    x.append(ndcg)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"ndcg@10\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"ndcg@10 frequencies over \" + str(totperms) + \" permutations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
