{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# BoltzRank #\n",
    "## Luca Negrini - Mat. 956516 ##\n",
    "### From \"BoltzRank: Learning to Maximize Expected Ranking Gain\" ###\n",
    "#### Maxims M. Volkovs, Richard S. Zemel ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Initialization ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "# install lightgbm (required only on first run)\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "from sklearn.datasets import load_svmlight_file \n",
    "\n",
    "# datasets available at: \n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "DATASET_FOLDER = \"C:/opt/kiis-training/MSLR-WEB10K/Fold1/\"\n",
    "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
    "METRIC_NAME = 'ndcg@10'#'BoltzRank-NDCG@10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Data loading ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensureFile(path):\n",
    "    if not os.path.exists(path) or not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
    "    return path\n",
    "\n",
    "def retrieveFileNames():\n",
    "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
    "    train_file = ensureFile(folder + \"train.txt\")\n",
    "    valid_file = ensureFile(folder + \"vali.txt\")\n",
    "    test_file = ensureFile(folder + \"test.txt\")\n",
    "    return train_file, valid_file, test_file\n",
    "\n",
    "def loadDataset(path):\n",
    "    return load_svmlight_file(path, query_id=True)\n",
    "\n",
    "def loadLightGBM(svmlight_dataset):\n",
    "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
    "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid):\n",
    "        self.qid = qid\n",
    "        self.labels_to_docs = {}\n",
    "    def addlabel(self, label):\n",
    "        if not label in self.labels_to_docs:\n",
    "            self.labels_to_docs[label] = list()\n",
    "    def adddoc(self, label, doc):\n",
    "        self.labels_to_docs[label].append(doc)\n",
    "    def finalize(self, alllabels):\n",
    "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
    "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
    "        i = 0\n",
    "        totaldocs = 0\n",
    "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
    "        for label, docs in sorteddict:\n",
    "            self.labels[i] = label\n",
    "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
    "            for j in range(len(docs)):\n",
    "                self.docs[i][j] = docs[j]\n",
    "            i += 1\n",
    "            totaldocs += len(docs)\n",
    "        self.alldocs = np.concatenate(self.docs)\n",
    "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
    "        i = 0\n",
    "        for label, docs in sorteddict:\n",
    "            for j in range(len(docs)):\n",
    "                self.flatlabels[i] = label\n",
    "                i += 1       \n",
    "        k = min(10, len(self.alldocs))\n",
    "        self.idealdcg = dcg_k(self.alldocs, alllabels, k) \n",
    "        del self.labels_to_docs\n",
    "    def setperms(self, perms):\n",
    "        self.perms = perms\n",
    "    def setprobs(self, probs):\n",
    "        self.probs = probs\n",
    "    def setndcgs(self, ndcgs):\n",
    "        self.ndcgs = ndcgs\n",
    "    def __repr__(self):  \n",
    "        return str(self)\n",
    "    def __str__(self):\n",
    "        res = \"Query \" + str(self.qid) + \"[\"\n",
    "        res += \"\\nideal dcg: \" + str(self.idealdcg)\n",
    "        for i in range(len(self.labels)):\n",
    "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
    "        res += \"]\"\n",
    "        if hasattr(self, 'perms'):\n",
    "            for i in range(len(self.perms)):\n",
    "                res += \"\\n[\" + str(self.perms[i]) + \"] -> p: \" + str(self.probs[i]) + \", dcg: \" + str(self.ndcgs[i])\n",
    "        else:\n",
    "            res += \"\\nNo permutations computed yet\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "#  dataset: svmlight_dataset \n",
    "#      the datset to process\n",
    "# returned values:\n",
    "#  query_to_labels_to_documents: dict(int -> dict(float -> list(int)))\n",
    "#      a map containing, for each query in the dataset, the documents (row index in the dataset) provided \n",
    "#      in the input dataset grouped by label\n",
    "#  doc_to_query: dict(document -> query)\n",
    "#      a mapping between document (row index in the dataset) and the relative query\n",
    "def mapQueryToDocuments(dataset):\n",
    "    queries = {}\n",
    "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
    "    for i in range(0, len(dataset[2])):\n",
    "        if not dataset[2][i] in queries:\n",
    "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
    "        query = queries[dataset[2][i]]\n",
    "        query.addlabel(dataset[1][i])\n",
    "        query.adddoc(dataset[1][i], i)\n",
    "        alllabels[i] = dataset[1][i]\n",
    "        \n",
    "    for q in queries.values():\n",
    "        q.finalize(alllabels)\n",
    "    \n",
    "    return queries, alllabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### BoltzRank logic in Cython ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "from libc.math cimport exp\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound\n",
    "from libc.math cimport log, log2\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from libc.stdio cimport printf\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                              NDCG EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(k, schedule='static', num_threads=8):\n",
    "        result += (2**scores[rank[i]] - 1) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
    "    return result\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double ndcg_k(int[:] rank, double[:] scores, int k, double ideal) nogil:\n",
    "    if ideal == 0:\n",
    "        return 1.0\n",
    "    return dcg_k(rank, scores, k) / ideal\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                          PERMUTATIONS GENERATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
    "                                int(.30 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
    "                                int(.22 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
    "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
    "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
    "                                int(.10 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
    "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
    "                                ]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
    "    cdef bint match\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    for i in prange(len(container), schedule='static', num_threads=8):\n",
    "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
    "            continue\n",
    "        else:\n",
    "            match = True\n",
    "            for j in range(len(container[i])):\n",
    "                if container[i][j] != array[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
    "    cdef int i\n",
    "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
    "        container[pos][i] = array[i]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
    "    cdef int i = 0\n",
    "    cdef int k\n",
    "    perm = itertools.permutations(source)\n",
    "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
    "    for p in perm:\n",
    "        for k in range(len(p)):\n",
    "            result[i][k] = p[k]\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    cdef int c = 0\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = max(1, int(_min * .5))\n",
    "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
    "    cdef int k\n",
    "    cdef int d\n",
    "    for k in range(count):\n",
    "        perm = query.docs.copy()\n",
    "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
    "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
    "        for d in range(len(first)):\n",
    "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
    "        p = np.concatenate(perm)\n",
    "        if not contained(perms, p):\n",
    "            setrow(perms, start + c, p)\n",
    "            c += 1\n",
    "            if c == limit:\n",
    "                return count - c, start + c\n",
    "        else:\n",
    "            k -= 1\n",
    "    return 0, start + c\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def process_query(query, alllabels):\n",
    "    cdef int carry = 0\n",
    "    fact = factorial(len(query.alldocs))\n",
    "    cdef perms\n",
    "    cdef int last = 0\n",
    "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
    "        # evaluate all possible permutations, each one representing a different ranking\n",
    "        perms = allPerms(query.alldocs, fact)\n",
    "    else:\n",
    "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
    "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
    "        carry, last = perform_permutation(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
    "        if carry != 0:\n",
    "            if not query.alldocs in perms:\n",
    "                perms[last] = query.alldocs\n",
    "        perms = perms[perms.max(axis=1)>=0]\n",
    "    cdef double[:] probs = np.zeros(len(perms))\n",
    "    cdef double[:] ndcgs = np.zeros(len(perms))\n",
    "    query.setperms(perms)\n",
    "    query.setprobs(rank_probabilities(perms, probs, alllabels))\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in range(len(perms)):\n",
    "        ndcgs[i] = ndcg_k(perms[i], alllabels, k, query.idealdcg)\n",
    "    query.setndcgs(ndcgs)\n",
    "    return query\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                           GRADIENTS EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] doc_energy(int[:] rank, double[:] scores, double[:,:] result) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return result\n",
    "    cdef double factor = 4.0 / (m * ((m - 1)**2))\n",
    "    cdef double res\n",
    "    cdef double res_w_scores\n",
    "    cdef int k, pos\n",
    "    for pos in prange(len(rank), schedule='static', num_threads=8):\n",
    "        res = 0.0\n",
    "        res_w_scores = 0.0\n",
    "        for k in range(len(rank)):\n",
    "            if k < pos: \n",
    "                res = res + (pos - k)\n",
    "                res_w_scores = res_w_scores + (pos - k) * (scores[rank[pos]] - scores[rank[k]])\n",
    "            elif k > pos: \n",
    "                res = res + (k - pos)\n",
    "                res_w_scores = res_w_scores + (k - pos) * (scores[rank[k]] - scores[rank[pos]])\n",
    "        result[rank[pos]][0] = factor * res_w_scores\n",
    "        result[rank[pos]][1] = factor * res\n",
    "        #printf(\"result of %i: m %i, factor %f, res %f, res_scores %f, result[0] %f, result[1] %f\\n\", pos, m, factor, res_w_scores, res, result[rank[pos]][0], result[rank[pos]][0])\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:,:] doc_rank_probabilities(int[:,:] sampleSet, double[:,:,:] norm_probs, double[:] scores, double[:,:] accumulator, double[:,:] tmp) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef double grad_norm = 0\n",
    "    cdef double _energy = 0\n",
    "    cdef double grad_energy = 0\n",
    "    cdef int r\n",
    "    cdef int d\n",
    "    cdef int pos\n",
    "    cdef int doc\n",
    "    \n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        doc_energy(sampleSet[r], scores, tmp)\n",
    "        for d in range(len(sampleSet[r])):\n",
    "            doc = sampleSet[r][d]\n",
    "            norm_probs[doc][r][0] = exp(-tmp[doc][0]) # e^{-E}\n",
    "            norm_probs[doc][r][1] = -tmp[doc][1] * norm_probs[doc][r][0] # -E' * e^{-E}\n",
    "            accumulator[doc][0] = accumulator[doc][0] + norm_probs[doc][r][0] # sum(e^{-E})\n",
    "            accumulator[doc][1] = accumulator[doc][1] + norm_probs[doc][r][1] # sum(-E' * e^{-E})\n",
    "        \n",
    "        #_energy, grad_energy = doc_energy(sampleSet[r], scores, pos) # E, E'\n",
    "        #norm_probs[r][0] = exp(-_energy) # e^{-E}\n",
    "        #norm_probs[r][1] = -grad_energy * norm_probs[r][0] # -E' * e^{-E}\n",
    "        #norm += norm_probs[r][0] # sum(e^{-E})\n",
    "        #grad_norm += norm_probs[r][1] # sum(-E' * e^{-E})\n",
    "\n",
    "    for pos in prange(len(sampleSet[0]), schedule='static', num_threads=8):\n",
    "        doc = sampleSet[0][pos]\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "        norm_probs[doc][r][1] = ((norm_probs[doc][r][1] * accumulator[doc][0]) - (norm_probs[doc][r][0] * accumulator[doc][1])) / (accumulator[doc][0]**2)\n",
    "        norm_probs[doc][r][0] = norm_probs[doc][r][0] / accumulator[doc][0] # e^{-E} / sum(e^{-E})\n",
    "        \n",
    "    #for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "    #    norm_probs[r][0] = norm_probs[r][0] / norm # e^{-E} / sum(e^{-E})\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "    #    norm_probs[r][1] = ((norm_probs[r][1] * norm) - (norm_probs[r][0] * grad_norm)) / (norm**2) \n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_cross_entropy(int[:,:] perms, double[:] probs, double[:,:,:] scores_probs, double[:,:] entropies) nogil:\n",
    "    cdef int i, doc, j\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            entropies[doc][0] = entropies[doc][0] + -probs[j] * log(scores_probs[doc][j][0])\n",
    "            entropies[doc][1] = entropies[doc][1] + -1 * (probs[j] / scores_probs[doc][j][0]) * scores_probs[doc][j][1]\n",
    "    return entropies\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_monte_carlo_gain(int[:,:] perms, double[:,:,:] scores_probs, double[:] scores, double[:] ndcgs, double ideal, double[:,:] gains) nogil:\n",
    "    cdef int i, doc, j\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            gains[doc][0] = gains[doc][0] + scores_probs[doc][j][0] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "            gains[doc][1] = gains[doc][1] + scores_probs[doc][j][1] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "    return gains \n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank_grads(queries, preds): \n",
    "    cdef double lam = .9\n",
    "    cdef double[:] gain = np.ones_like(preds)\n",
    "    cdef double[:] hess = np.ones_like(preds) \n",
    "    \n",
    "    cdef int d, i\n",
    "    cdef double[:,:,:] score_probs\n",
    "    cdef double[:,:] accumulator = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] tmp = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] gains = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] entropies = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros((len(preds), len(q.probs), 2), dtype=np.double)\n",
    "        score_probs = doc_rank_probabilities(q.perms, score_probs, preds, accumulator, tmp)\n",
    "        gains = grad_monte_carlo_gain(q.perms, score_probs, preds, q.ndcgs, q.idealdcg, gains) \n",
    "        entropies = grad_cross_entropy(q.perms, q.probs, score_probs, entropies)\n",
    "    for i in range(len(gain)):\n",
    "        gain[i] = (lam * gains[i][1]) - ((1-lam) * entropies[i][1])\n",
    "    return gain, hess\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                            METRIC EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double energy(int[:] rank, double[:] scores) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return 0\n",
    "    cdef double factor = 4 / (m * ((m - 1)**2))\n",
    "    cdef double res = 0\n",
    "    cdef int j, k\n",
    "    for k in prange(m, schedule='static', num_threads=8):\n",
    "        for j in range(k + 1, m):\n",
    "            res += (j - k) * (scores[rank[j]] - scores[rank[k]])\n",
    "    return factor * res\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] rank_probabilities(int[:,:] sampleSet, double[:] norm_probs, double[:] scores) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef int r\n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = exp(-energy(sampleSet[r], scores))\n",
    "        norm += norm_probs[r]\n",
    "    for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = norm_probs[r] / norm\n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double cross_entropy(double[:] probs, double[:] scores_probs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(len(probs), schedule='static', num_threads=8):\n",
    "        result += probs[i] * log(scores_probs[i])\n",
    "    return -result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double monte_carlo_gain(int[:,:] perms, double[:] scores_probs, double[:] ndcgs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms), schedule='static', num_threads=8):\n",
    "        result += scores_probs[i] * ndcgs[i]\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank(queries, preds):\n",
    "    cdef double lam = .9\n",
    "    cdef double[:] score_probs\n",
    "    cdef double mc = 0\n",
    "    cdef double ce = 0\n",
    "    cdef double _mc, _ce\n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros(len(q.probs), dtype=np.double)\n",
    "        score_probs = rank_probabilities(q.perms, score_probs, preds)\n",
    "        mc += monte_carlo_gain(q.perms, score_probs, q.ndcgs) \n",
    "        ce += cross_entropy(q.probs, score_probs)\n",
    "    return (lam * mc) - ((1-lam) * ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/train.txt\n",
      "validation file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/vali.txt\n",
      "test file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/test.txt\n",
      "loading datasets... \n",
      "train dataset loading took 71.453125 s\n",
      "validation dataset loading took 22.765625 s\n",
      "test dataset loading took 22.4375 s\n",
      "converting datasets to LightGBM format... \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = retrieveFileNames()\n",
    "\n",
    "print(\"training file: \" + train_file)\n",
    "print(\"validation file: \" + valid_file)\n",
    "print(\"test file: \" + test_file)\n",
    "    \n",
    "print(\"loading datasets... \")\n",
    "import time\n",
    "start = time.process_time()\n",
    "train_dataset = loadDataset(train_file)\n",
    "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "valid_dataset = loadDataset(valid_file)\n",
    "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "test_dataset = loadDataset(test_file)\n",
    "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "\n",
    "import itertools\n",
    "print(\"converting datasets to LightGBM format... \")\n",
    "train_lgb = loadLightGBM(train_dataset)\n",
    "valid_lgb = loadLightGBM(valid_dataset)\n",
    "test_lgb = loadLightGBM(test_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating query-documents mappings...\n",
      "done\n",
      "creating sample sets...\n",
      "sample set creation took 70.5625 s\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "print(\"creating query-documents mappings...\")\n",
    "train_id = file_len(train_file)\n",
    "vali_id = file_len(valid_file)\n",
    "test_id = file_len(test_file)\n",
    "ds_to_queries = {}\n",
    "ds_to_queries[train_id] = mapQueryToDocuments(train_dataset)\n",
    "ds_to_queries[vali_id] = mapQueryToDocuments(valid_dataset)\n",
    "ds_to_queries[test_id] = mapQueryToDocuments(test_dataset)\n",
    "#queries, alllabels = mapQueryToDocuments(train_dataset)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"creating sample sets...\")\n",
    "start = time.process_time()\n",
    "for ds_id, queries in ds_to_queries.items():\n",
    "    for q in queries[0].values():\n",
    "        process_query(q, queries[1])\n",
    "#for q in queries.values():\n",
    "#    queries[q.qid] = process_query(q, alllabels)\n",
    "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lightgbm...\n",
      "MSE: 0.5592629096559084\n",
      "[1]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.5406102691116346\n",
      "[2]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.5228330623578503\n",
      "[3]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.5059312875210156\n",
      "[4]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.489904943561193\n",
      "[5]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.47475403025749\n",
      "[6]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.46047854819995854\n",
      "[7]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.4470784987803169\n",
      "[8]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.43455388418390695\n",
      "[9]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.4229047073805879\n",
      "[10]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.41213097212450084\n",
      "[11]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.4022326829433412\n",
      "[12]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.39320984514228147\n",
      "[13]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.38506246479749884\n",
      "[14]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.37779054875894785\n",
      "[15]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3713941046497817\n",
      "[16]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.36587314086879585\n",
      "[17]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.36122766659633937\n",
      "[18]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3574576917967736\n",
      "[19]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3545632272275923\n",
      "[20]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.352544284445886\n",
      "[21]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3514008758198516\n",
      "[22]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3511330145395965\n",
      "[23]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.3517407146291886\n",
      "[24]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n",
      "MSE: 0.35322399096347706\n",
      "[25]\ttrain's ndcg@10: 0.249027\tvalid's ndcg@10: 0.253556\ttest's ndcg@10: 0.252445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b6189e96b643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                             \u001b[0mvalid_names\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"valid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                             \u001b[0mevals_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbm_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                             verbose_eval = 1)\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training took \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1980\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"none\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1982\u001b[1;33m             \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1983\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-b6189e96b643>\u001b[0m in \u001b[0;36mcompute_grads\u001b[1;34m(preds, train_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mds_to_queries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mgain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_boltzrank_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_to_queries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mgain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mhess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluation(preds, train_data):\n",
    "    global ds_to_queries\n",
    "    #bz = eval_boltzrank(ds_to_queries[len(preds)][0], preds)\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE eval: \" + str(avg_mse))\n",
    "    return 'MSE', avg_mse, False\n",
    "\n",
    "def compute_grads(preds, train_data): \n",
    "    global ds_to_queries\n",
    "    global train_id\n",
    "    gain, hess = eval_boltzrank_grads(ds_to_queries[train_id][0], preds)\n",
    "    gain = np.asarray(gain)\n",
    "    hess = np.asarray(hess)\n",
    "    #print(\"min \" + str(np.min(gain)) + \" max \" + str(np.max(gain)) + \" mean \" + str(np.mean(gain)) + \" std \" + str(np.std(gain)))\n",
    "    #print(\"preds \" + str(preds))\n",
    "    #print(\"gain \" + str(gain))\n",
    "    #print(\"hess \" + str(hess))\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE grads: \" + str(avg_mse))\n",
    "    return gain, hess\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'metric': ['ndcg'],# ['None']\n",
    "    'ndcg_eval_at': 10\n",
    "}    \n",
    "\n",
    "print(\"training lightgbm...\")\n",
    "start = time.process_time()\n",
    "lgbm_info = {}\n",
    "lgbm_model = lightgbm.train(params, train_lgb, num_boost_round=100,\n",
    "                            feval = evaluation,\n",
    "                            fobj  = compute_grads,\n",
    "                            valid_sets   = [train_lgb, valid_lgb, test_lgb], \n",
    "                            valid_names  = [\"train\", \"valid\", \"test\"],\n",
    "                            evals_result = lgbm_info,\n",
    "                            verbose_eval = 1)\n",
    "print(\"training took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(lgbm_info['train'][METRIC_NAME], label='training')\n",
    "plt.plot(lgbm_info['valid'][METRIC_NAME], label='validation')\n",
    "plt.plot(lgbm_info['test'][METRIC_NAME], label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"# Trees\")\n",
    "plt.ylabel(METRIC_NAME)\n",
    "plt.title(\"Model Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(q, k):\n",
    "    indexes = set(range(0, len(q.perms)))\n",
    "    indexes.remove(k)\n",
    "    result = []\n",
    "    for i in range(len(q.perms[k])):\n",
    "        tmp = set(indexes)\n",
    "        for j in tmp:\n",
    "            if q.perms[k][i] != q.perms[j][i]:\n",
    "                indexes.remove(j)\n",
    "    for w in indexes:\n",
    "        if w < k:\n",
    "            result.append((w, k))\n",
    "        else: \n",
    "            result.append((k,w))\n",
    "    return result\n",
    "\n",
    "def checkRepetitions():\n",
    "    global queries\n",
    "    same = dict()\n",
    "    for q in queries.values():\n",
    "        for i in range(len(q.perms)):\n",
    "            r = check(q, i)\n",
    "            if len(r) != 0:\n",
    "                if not q.qid in same.keys():\n",
    "                    same[q.qid] = set()\n",
    "                for t in r:\n",
    "                    same[q.qid].add(t)\n",
    "\n",
    "    print(str(len(same.keys())) + \"/\" + str(len(queries.keys())) + \" queries have duplicate permutations\")\n",
    "    for q, s in same.items():\n",
    "        print(\"query \" + str(q) + \" has repeated permutations: \" + str(s))\n",
    "        \n",
    "#checkRepetitions()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4dcdedcd6052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for query in queries.values():\n",
    "    for prob in query.probs:\n",
    "        if not prob in freq.keys():\n",
    "            freq[prob] = 0\n",
    "        freq[prob] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for prob, f in sorted(freq.items()):\n",
    "    x.append(prob)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel(\"# rank\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"probabilities of the \" + str(totperms) + \" permutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "totperms = 0\n",
    "for query in queries.values():\n",
    "    for ndcg in query.ndcgs:\n",
    "        totperms += 1\n",
    "        if not ndcg in freq.keys():\n",
    "            freq[ndcg] = 0\n",
    "        freq[ndcg] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for ndcg, f in sorted(freq.items()):\n",
    "    x.append(ndcg)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"ndcg@10\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"ndcg@10 frequencies over \" + str(totperms) + \" permutations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
