{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# BoltzRank #\n",
    "## Luca Negrini - Mat. 956516 ##\n",
    "### From \"BoltzRank: Learning to Maximize Expected Ranking Gain\" ###\n",
    "#### Maxims M. Volkovs, Richard S. Zemel ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Initialization ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "# install lightgbm (required only on first run)\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "from sklearn.datasets import load_svmlight_file \n",
    "\n",
    "# datasets available at: \n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "DATASET_FOLDER = \"C:/opt/kiis-training/MSLR-WEB10K/Fold1/\"\n",
    "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
    "METRIC_NAME = 'ndcg@10'#'BoltzRank-NDCG@10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Data loading ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensureFile(path):\n",
    "    if not os.path.exists(path) or not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
    "    return path\n",
    "\n",
    "def retrieveFileNames():\n",
    "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
    "    train_file = ensureFile(folder + \"train.txt\")\n",
    "    valid_file = ensureFile(folder + \"vali.txt\")\n",
    "    test_file = ensureFile(folder + \"test.txt\")\n",
    "    return train_file, valid_file, test_file\n",
    "\n",
    "def loadDataset(path):\n",
    "    return load_svmlight_file(path, query_id=True)\n",
    "\n",
    "def loadLightGBM(svmlight_dataset):\n",
    "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
    "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid):\n",
    "        self.qid = qid\n",
    "        self.labels_to_docs = {}\n",
    "    def addlabel(self, label):\n",
    "        if not label in self.labels_to_docs:\n",
    "            self.labels_to_docs[label] = list()\n",
    "    def adddoc(self, label, doc):\n",
    "        self.labels_to_docs[label].append(doc)\n",
    "    def finalize(self, alllabels):\n",
    "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
    "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
    "        i = 0\n",
    "        totaldocs = 0\n",
    "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
    "        for label, docs in sorteddict:\n",
    "            self.labels[i] = label\n",
    "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
    "            for j in range(len(docs)):\n",
    "                self.docs[i][j] = docs[j]\n",
    "            i += 1\n",
    "            totaldocs += len(docs)\n",
    "        self.alldocs = np.concatenate(self.docs)\n",
    "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
    "        i = 0\n",
    "        for label, docs in sorteddict:\n",
    "            for j in range(len(docs)):\n",
    "                self.flatlabels[i] = label\n",
    "                i += 1       \n",
    "        k = min(10, len(self.alldocs))\n",
    "        self.idealdcg = dcg_k(self.alldocs, alllabels, k) \n",
    "        del self.labels_to_docs\n",
    "    def setperms(self, perms):\n",
    "        self.perms = perms\n",
    "    def setndcgs(self, ndcgs):\n",
    "        self.ndcgs = ndcgs\n",
    "    def __repr__(self):  \n",
    "        return str(self)\n",
    "    def __str__(self):\n",
    "        res = \"Query \" + str(self.qid) + \"[\"\n",
    "        res += \"\\nideal dcg: \" + str(self.idealdcg)\n",
    "        for i in range(len(self.labels)):\n",
    "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
    "        res += \"]\"\n",
    "        if hasattr(self, 'perms'):\n",
    "            for i in range(len(self.perms)):\n",
    "                res += \"\\n[\" + str(self.perms[i]) + \"] -> dcg: \" + str(self.ndcgs[i])\n",
    "        else:\n",
    "            res += \"\\nNo permutations computed yet\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "#  dataset: svmlight_dataset \n",
    "#      the datset to process\n",
    "# returned values:\n",
    "#  query_to_labels_to_documents: dict(int -> dict(float -> list(int)))\n",
    "#      a map containing, for each query in the dataset, the documents (row index in the dataset) provided \n",
    "#      in the input dataset grouped by label\n",
    "#  doc_to_query: dict(document -> query)\n",
    "#      a mapping between document (row index in the dataset) and the relative query\n",
    "def mapQueryToDocuments(dataset):\n",
    "    queries = {}\n",
    "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
    "    for i in range(0, len(dataset[2])):\n",
    "        if not dataset[2][i] in queries:\n",
    "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
    "        query = queries[dataset[2][i]]\n",
    "        query.addlabel(dataset[1][i])\n",
    "        query.adddoc(dataset[1][i], i)\n",
    "        alllabels[i] = dataset[1][i]\n",
    "        \n",
    "    for q in queries.values():\n",
    "        q.finalize(alllabels)\n",
    "    \n",
    "    return queries, alllabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Rank sample set generation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "from libc.math cimport exp\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound, cdivision\n",
    "from libc.math cimport log2\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from libc.stdio cimport printf\n",
    "from libc.stdlib cimport malloc\n",
    "from libc.stdlib cimport free\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double* E(int[:] R, double[:] S) nogil:\n",
    "    cdef int k, j, m = len(R)\n",
    "    cdef double* energy = <double *> malloc(m*sizeof(double))\n",
    "    cdef double res_w_S, factor \n",
    "    if m == 1 or m == 0:\n",
    "        for j in prange(len(R), schedule='static', num_threads=8):\n",
    "            energy[j] = 0\n",
    "    else:\n",
    "        factor = 4.0 / (m * ((m - 1)**2))\n",
    "        for j in prange(len(R), schedule='static', num_threads=8):\n",
    "            res_w_S = 0.0\n",
    "            for k in range(len(R)):\n",
    "                if k < j: \n",
    "                    res_w_S = res_w_S + (j - k) * (S[R[j]] - S[R[k]])\n",
    "                elif k > j: \n",
    "                    res_w_S = res_w_S + (k - j) * (S[R[k]] - S[R[j]])\n",
    "            energy[j] = factor * res_w_S\n",
    "    return energy\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void P(int[:,:] Rq, double[:] S, double[:,:] probs, double[:] accumulator) nogil:\n",
    "    cdef int rankid, pos, doc\n",
    "    cdef double* en\n",
    "    for rankid in prange(len(Rq), schedule='static', num_threads=8):\n",
    "        en = E(Rq[rankid], S)\n",
    "        for pos in range(len(Rq[rankid])):\n",
    "            doc = Rq[rankid][pos]\n",
    "            probs[doc][rankid] = exp(-en[pos]) # e^{-E}\n",
    "            accumulator[doc] = accumulator[doc] + probs[doc][rankid] # sum(e^{-E})\n",
    "        free(en)\n",
    "    for pos in prange(len(Rq[0]), schedule='static', num_threads=8):\n",
    "        doc = Rq[0][pos]\n",
    "        # e^{-E} / sum(e^{-E})\n",
    "        probs[doc][rankid] = probs[doc][rankid] / accumulator[doc]\n",
    "        \n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                              NDCG EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(k, schedule='static', num_threads=8):\n",
    "        result += (2**scores[rank[i]] - 1) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
    "    return result\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double ndcg_k(int[:] rank, double[:] scores, int k, double ideal) nogil:\n",
    "    if ideal == 0:\n",
    "        return 1.0\n",
    "    return dcg_k(rank, scores, k) / ideal\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                          PERMUTATIONS GENERATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
    "                                int(.30 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
    "                                int(.22 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
    "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
    "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
    "                                int(.10 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
    "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
    "                                ]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
    "    cdef bint match\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    for i in prange(len(container), schedule='static', num_threads=8):\n",
    "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
    "            continue\n",
    "        else:\n",
    "            match = True\n",
    "            for j in range(len(container[i])):\n",
    "                if container[i][j] != array[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
    "    cdef int i\n",
    "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
    "        container[pos][i] = array[i]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
    "    cdef int i = 0\n",
    "    cdef int k\n",
    "    perm = itertools.permutations(source)\n",
    "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
    "    for p in perm:\n",
    "        for k in range(len(p)):\n",
    "            result[i][k] = p[k]\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    cdef int c = 0\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = max(1, int(_min * .5))\n",
    "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
    "    cdef int k\n",
    "    cdef int d\n",
    "    for k in range(count):\n",
    "        perm = query.docs.copy()\n",
    "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
    "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
    "        for d in range(len(first)):\n",
    "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
    "        p = np.concatenate(perm)\n",
    "        if not contained(perms, p):\n",
    "            setrow(perms, start + c, p)\n",
    "            c += 1\n",
    "            if c == limit:\n",
    "                return count - c, start + c\n",
    "        else:\n",
    "            k -= 1\n",
    "    return 0, start + c\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def process_query(query, alllabels, probs, accumulator):\n",
    "    cdef int carry = 0\n",
    "    fact = factorial(len(query.alldocs))\n",
    "    cdef perms\n",
    "    cdef int last = 0\n",
    "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
    "        # evaluate all possible permutations, each one representing a different ranking\n",
    "        perms = allPerms(query.alldocs, fact)\n",
    "    else:\n",
    "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
    "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
    "        carry, last = perform_permutation(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
    "        if carry != 0:\n",
    "            if not query.alldocs in perms:\n",
    "                perms[last] = query.alldocs\n",
    "        perms = perms[perms.max(axis=1)>=0]\n",
    "    query.setperms(perms)  \n",
    "    P(perms, alllabels, probs, accumulator)\n",
    "    cdef double[:] ndcgs = np.zeros(len(perms))\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in range(len(perms)):\n",
    "        ndcgs[i] = ndcg_k(perms[i], alllabels, k, query.idealdcg)\n",
    "    query.setndcgs(ndcgs)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/train.txt\n",
      "validation file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/vali.txt\n",
      "test file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/test.txt\n",
      "loading datasets... \n",
      "train dataset loading took 49.578125 s\n",
      "validation dataset loading took 15.390625 s\n",
      "test dataset loading took 15.53125 s\n",
      "converting datasets to LightGBM format... \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = retrieveFileNames()\n",
    "\n",
    "print(\"training file: \" + train_file)\n",
    "print(\"validation file: \" + valid_file)\n",
    "print(\"test file: \" + test_file)\n",
    "    \n",
    "print(\"loading datasets... \")\n",
    "import time\n",
    "start = time.process_time()\n",
    "train_dataset = loadDataset(train_file)\n",
    "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "valid_dataset = loadDataset(valid_file)\n",
    "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "test_dataset = loadDataset(test_file)\n",
    "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "\n",
    "import itertools\n",
    "print(\"converting datasets to LightGBM format... \")\n",
    "train_lgb = loadLightGBM(train_dataset)\n",
    "valid_lgb = loadLightGBM(valid_dataset)\n",
    "test_lgb = loadLightGBM(test_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating query-documents mappings...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "print(\"creating query-documents mappings...\")\n",
    "train_id = file_len(train_file)\n",
    "vali_id = file_len(valid_file)\n",
    "test_id = file_len(test_file)\n",
    "ds_to_queries = {}\n",
    "ds_to_queries[train_id] = mapQueryToDocuments(train_dataset)\n",
    "ds_to_queries[vali_id] = mapQueryToDocuments(valid_dataset)\n",
    "ds_to_queries[test_id] = mapQueryToDocuments(test_dataset)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating sample sets...\n",
      "sample set creation took 77.0625 s\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"creating sample sets...\")\n",
    "start = time.process_time()\n",
    "\n",
    "probs_with_labels = {}\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "for ds_id, queries in ds_to_queries.items():\n",
    "    probs_with_labels[ds_id] = np.zeros((len(queries[1]), RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS))\n",
    "    accumulator = np.zeros(len(queries[1]))\n",
    "    for q in queries[0].values():\n",
    "        process_query(q, queries[1], probs_with_labels[ds_id], accumulator)    \n",
    "    del accumulator\n",
    "    \n",
    "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "#### BoltzRank logic ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "from libc.math cimport exp\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound, cdivision\n",
    "from libc.math cimport log\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from libc.stdio cimport printf\n",
    "from libc.stdlib cimport malloc\n",
    "from libc.stdlib cimport free\n",
    "\n",
    "# Energy function.\n",
    "# params: \n",
    "#  R      the ranking to evaluate. Element i contains the id of the document in position i\n",
    "#  S      the scoring vector. Element i contains the score of the document with id i\n",
    "# returns:\n",
    "#  a c matrix of size (m,2). Row i contains (energy, energy') w.r.t. document i\n",
    "#  NB: call free on the returned matrix.\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double** E(int[:] R, double[:] S) nogil:\n",
    "    cdef int k, j, m = len(R)\n",
    "    cdef double** energy = <double **> malloc(m*sizeof(double*)) # freed at 78\n",
    "    cdef double res, res_w_S, factor\n",
    "    if m == 1 or m == 0:\n",
    "        for j in prange(len(R), schedule='static', num_threads=8):\n",
    "            energy[j] = <double *> malloc(2*sizeof(double)) # freed at 77\n",
    "            energy[j][0] = 0\n",
    "            energy[j][1] = 0\n",
    "    else:\n",
    "        factor = 4.0 / (m * ((m - 1)**2))\n",
    "        for j in prange(len(R), schedule='static', num_threads=8):\n",
    "            energy[j] = <double *> malloc(2*sizeof(double)) # freed at 77\n",
    "            res = 0.0\n",
    "            res_w_S = 0.0\n",
    "            for k in range(len(R)):\n",
    "                if k < j: \n",
    "                    res = res + (j - k)\n",
    "                    res_w_S = res_w_S + (j - k) * (S[R[j]] - S[R[k]])\n",
    "                elif k > j: \n",
    "                    res = res + (k - j)\n",
    "                    res_w_S = res_w_S + (k - j) * (S[R[k]] - S[R[j]])\n",
    "            energy[j][0] = factor * res_w_S\n",
    "            energy[j][1] = factor * res\n",
    "    return energy\n",
    "\n",
    "# Probability function.\n",
    "# params: \n",
    "#  Rq    the rank sample set. Element i contains the i-th ranking, that is, an array of document ids\n",
    "#  S     the scoring vector. Element i contains the score of the document with id i\n",
    "#  probs a c matrix of size (len(S),len(Rq),3). Each row i contains the result of the computation w.r.t. document i. Each row contains a \n",
    "#        matrix (len(Rq),3) containing, for each ranking, a tuple (probability, probability', probability'')\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void P(int[:,:] Rq, double[:] S, double*** probs, double** accumulator) nogil:\n",
    "    cdef int rankid, pos, doc\n",
    "    cdef double** en\n",
    "    \n",
    "    for pos in range(len(Rq[0])):\n",
    "        doc = Rq[0][pos]\n",
    "        probs[doc] = <double **> malloc(len(Rq)*sizeof(double*)) # freed at 139\n",
    "        accumulator[doc] = <double *> malloc(3*sizeof(double*)) # freed at 92\n",
    "        accumulator[doc][0] = 0\n",
    "        accumulator[doc][1] = 0\n",
    "        accumulator[doc][2] = 0\n",
    "        for rankid in prange(len(Rq), schedule='static', num_threads=8):\n",
    "            probs[doc][rankid] = <double *> malloc(3*sizeof(double)) # freed at 138\n",
    "    \n",
    "    for rankid in prange(len(Rq), schedule='static', num_threads=8):\n",
    "        en = E(Rq[rankid], S)\n",
    "        for pos in range(len(Rq[rankid])):\n",
    "            doc = Rq[rankid][pos]\n",
    "            probs[doc][rankid][0] = exp(-en[pos][0]) # e^{-E}\n",
    "            probs[doc][rankid][1] = 0\n",
    "            probs[doc][rankid][2] = en[pos][1] # E'\n",
    "            accumulator[doc][0] = accumulator[doc][0] + probs[doc][rankid][0] # sum(e^{-E})\n",
    "            accumulator[doc][1] = accumulator[doc][1] + (-en[pos][1] * probs[doc][rankid][0]) # sum(-E' * e^{-E})\n",
    "            accumulator[doc][2] = accumulator[doc][2] + ((en[pos][1]**2) * probs[doc][rankid][0]) # sum(E'^2 * e^{-E})\n",
    "            free(en[pos]) # allocated at 28\n",
    "        free(en) # allocated at 24\n",
    "\n",
    "    for pos in prange(len(Rq[0]), schedule='static', num_threads=8):\n",
    "        doc = Rq[0][pos]\n",
    "        # e^{-E} / sum(e^{-E})\n",
    "        probs[doc][rankid][0] = probs[doc][rankid][0] / accumulator[doc][0]\n",
    "        \n",
    "        # -P * (E' + (sum(-E' * e^{-E}) / sum(e^{-E})))\n",
    "        probs[doc][rankid][1] = -probs[doc][rankid][0] * (probs[doc][rankid][2] + (accumulator[doc][1] / accumulator[doc][0]))\n",
    "                \n",
    "        # -P' * (E' + (sum(-E' * e^{-E}) / sum(e^{-E}))) - P * (1 + (sum(E'^2 * e^{-E}) / sum(-E' * e^{-E})) - (sum(-E' * e^{-E})^2 / sum(e^{-E})^2))\n",
    "        probs[doc][rankid][2] = (-probs[doc][rankid][1] * (probs[doc][rankid][2] + (accumulator[doc][1] / accumulator[doc][0]))) \n",
    "        probs[doc][rankid][2] = probs[doc][rankid][2] - (probs[doc][rankid][0] * (1 + ((accumulator[doc][2] / accumulator[doc][0]) - (accumulator[doc][1]**2 / accumulator[doc][0]**2))))\n",
    "        \n",
    "        free(accumulator[doc]) # allocated at 63\n",
    "        \n",
    "        if isnan(probs[doc][rankid][0]):\n",
    "            printf(\"prob of document %d in rank %d is nan\\n\", doc, rankid)\n",
    "            with gil:\n",
    "                raise TypeError(\"foo\")\n",
    "        if isnan(probs[doc][rankid][1]):\n",
    "            printf(\"prob' of document %d in rank %d is nan\\n\", doc, rankid)\n",
    "            with gil:\n",
    "                raise TypeError(\"foo\")\n",
    "        if isnan(probs[doc][rankid][2]):\n",
    "            printf(\"prob'' of document %d in rank %d is nan\\n\", doc, rankid)\n",
    "            with gil:\n",
    "                raise TypeError(\"foo\")\n",
    "        \n",
    "# Gain function.\n",
    "# params: \n",
    "#  Rq                the rank sample set. Element i contains the i-th ranking, that is, an array of document ids\n",
    "#  probs             a c matrix of size (len(S),len(Rq),3). Each row i contains the result of the computation w.r.t. document i. Each row contains a \n",
    "#                    matrix (len(Rq),3) containing, for each ranking, a tuple (probability, probability', probability'')\n",
    "#  probs_with_labels a c matrix of size (len(S),len(Rq)). Each row i contains the result of the computation w.r.t. document i. Each row contains an \n",
    "#                    array (len(Rq)) containing, for each ranking, its probability w.r.t. the target labeling\n",
    "#  gains             a c matrix of size (len(S),3) containing, for each document, the evaluated gain with its two derivatives. \n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void monte_carlo_gain(int[:,:] Rq, double*** probs, double[:] ndcgs, double** gains) nogil:\n",
    "    cdef int i, doc, j\n",
    "    for i in prange(len(Rq[0]), schedule='static', num_threads=8):\n",
    "        doc = Rq[0][i]\n",
    "        gains[doc] = <double*> malloc(3*sizeof(double)) # freed at 163\n",
    "        gains[doc][0] = 0\n",
    "        gains[doc][1] = 0\n",
    "        gains[doc][2] = 0\n",
    "        for j in range(len(Rq)):\n",
    "            gains[doc][0] = gains[doc][0] + probs[doc][j][0] * ndcgs[j]\n",
    "            gains[doc][1] = gains[doc][1] + probs[doc][j][1] * ndcgs[j]\n",
    "            gains[doc][2] = gains[doc][2] + probs[doc][j][2] * ndcgs[j]\n",
    "        \n",
    "# Cross entropy function.\n",
    "# params: \n",
    "#  Rq                the rank sample set. Element i contains the i-th ranking, that is, an array of document ids\n",
    "#  probs             a c matrix of size (len(S),len(Rq),3). Each row i contains the result of the computation w.r.t. document i. Each row contains a \n",
    "#                    matrix (len(Rq),3) containing, for each ranking, a tuple (probability, probability', probability'')\n",
    "#  probs_with_labels a c matrix of size (len(S),len(Rq)). Each row i contains the result of the computation w.r.t. document i. Each row contains an \n",
    "#                    array (len(Rq)) containing, for each ranking, its probability w.r.t. the target labeling\n",
    "#  entropies         a c matrix of size (len(S),3) containing, for each document, the evaluated cross entropy with its two derivatives. \n",
    "#                    NB the entropy's sign has not yet being flipped \n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "@cdivision(True)\n",
    "cdef void cross_entropy(int[:,:] Rq, double*** probs, double[:,:] probs_with_labels, double** entropies) nogil:\n",
    "    cdef int i, doc, j\n",
    "    for i in prange(len(Rq[0]), schedule='static', num_threads=8):\n",
    "        doc = Rq[0][i]\n",
    "        entropies[doc] = <double*> malloc(3*sizeof(double)) # freed at 164\n",
    "        entropies[doc][0] = 0\n",
    "        entropies[doc][1] = 0\n",
    "        entropies[doc][2] = 0\n",
    "        for j in range(len(Rq)):\n",
    "            # P(L)log(P(S))\n",
    "            entropies[doc][0] = entropies[doc][0] + probs_with_labels[doc][j] * log(probs[doc][j][0])\n",
    "            # P(L)(P'(S)/P(S))\n",
    "            entropies[doc][1] = entropies[doc][1] + (probs_with_labels[doc][j] * probs[doc][j][1] / probs[doc][j][0])\n",
    "            # P(L)(P(S)P''(S)-P'(S)^2)/P(S)^2\n",
    "            entropies[doc][2] = entropies[doc][2] + (probs_with_labels[doc][j] * ((probs[doc][j][0] * probs[doc][j][2] - probs[doc][j][1]**2) / probs[doc][j][0]**2))\n",
    "            \n",
    "            if isnan(entropies[doc][0]):\n",
    "                printf(\"entropies of document %d is nan after rank %d (probs %f)\\n\", doc, j, probs[doc][j][0])\n",
    "                with gil:\n",
    "                    raise TypeError(\"foo\")\n",
    "            if isnan(entropies[doc][1]):\n",
    "                printf(\"entropies' of document %d is nan after rank %d (probs %f)\\n\", doc, j, probs[doc][j][0])\n",
    "                with gil:\n",
    "                    raise TypeError(\"foo\")\n",
    "            if isnan(entropies[doc][2]):\n",
    "                printf(\"entropies'' of document %d is nan after rank %d (probs %f)\\n\", doc, j, probs[doc][j][0])\n",
    "                printf(\"PL %f P %f P' %f P'' %f P*P''-P'^2 %f P^2%f\\n\", probs_with_labels[doc][j], probs[doc][j][0], probs[doc][j][1], probs[doc][j][2], probs[doc][j][0] * probs[doc][j][2] - probs[doc][j][1]**2, probs[doc][j][0]**2)\n",
    "                with gil:\n",
    "                    raise TypeError(\"foo\")\n",
    "                    \n",
    "            free(probs[doc][j]) # allocated at 65\n",
    "\n",
    "        free(probs[doc]) # allocated at 62\n",
    "    \n",
    "from libc.math cimport isnan\n",
    "\n",
    "# Boltzrank grads and hess evaluation function.\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank_grads(queries, S, probs_with_labels): \n",
    "    cdef double lam = .9\n",
    "    cdef double[:] gain = np.ones_like(S)\n",
    "    cdef double[:] hess = np.ones_like(S) \n",
    "    \n",
    "    cdef int i\n",
    "    cdef double*** probs = <double***> malloc(len(S)*sizeof(double**)) # freed at 167\n",
    "    cdef double** accumulator = <double**> malloc(len(S)*sizeof(double*)) # freed at 168\n",
    "    cdef double** gains = <double**> malloc(len(S)*sizeof(double*)) # freed at 165\n",
    "    cdef double** entropies = <double**> malloc(len(S)*sizeof(double*)) # freed at 166\n",
    "    for q in queries.values():\n",
    "        P(q.perms, S, probs, accumulator)\n",
    "        monte_carlo_gain(q.perms, probs, q.ndcgs, gains) \n",
    "        cross_entropy(q.perms, probs, probs_with_labels, entropies)\n",
    "    for i in range(len(gain)):\n",
    "        gain[i] = (lam * gains[i][1]) - ((1-lam) * -entropies[i][1])\n",
    "        hess[i] = (lam * gains[i][2]) - ((1-lam) * -entropies[i][2])\n",
    "        free(gains[i]) # allocated at 108\n",
    "        free(entropies[i]) # allocated at 130\n",
    "    free(gains) # allocated at 154\n",
    "    free(entropies) # allocated at 155\n",
    "    free(probs) # allocated at 152\n",
    "    free(accumulator) # allocated at 153\n",
    "    return gain, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lightgbm...\n",
      "PREDS: min 0.0 max 0.0 mean 0.0 std 0.0\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 1.5907002066515987e-09 std 0.00019984390730105168\n",
      "HESS: min -1.0 max 597.7076790079848 mean 0.3710510601877022 std 1.0375043269400472\n",
      "[1]\ttrain's ndcg@10: 0.210195\tvalid's ndcg@10: 0.214026\ttest's ndcg@10: 0.209482\n",
      "PREDS: min -0.13116130285646085 max 0.3169541137652109 mean 2.4719944968765843e-05 std 0.0028496801451223202\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.647447482905131e-09 std 0.00019795444865257784\n",
      "HESS: min -1.0 max 597.7077578355861 mean 0.37105134608945184 std 1.0375047219339404\n",
      "[2]\ttrain's ndcg@10: 0.21011\tvalid's ndcg@10: 0.213886\ttest's ndcg@10: 0.20861\n",
      "PREDS: min -0.24613183242561704 max 0.3169541343790111 mean 1.2658953150250979e-06 std 0.003261645899727188\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.503589562747818e-09 std 0.00019789064192951274\n",
      "HESS: min -1.0 max 597.7078196826878 mean 0.37105138416701894 std 1.0375049522177446\n",
      "[3]\ttrain's ndcg@10: 0.21011\tvalid's ndcg@10: 0.213886\ttest's ndcg@10: 0.20861\n",
      "PREDS: min -0.33214042316287934 max 0.3169541551945632 mean -1.6147277206853083e-05 std 0.0038630245963198633\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.342882547174825e-09 std 0.0001978161687621513\n",
      "HESS: min -1.0 max 597.7078812911843 mean 0.37105142272366887 std 1.0375051981525647\n",
      "[4]\ttrain's ndcg@10: 0.210144\tvalid's ndcg@10: 0.213886\ttest's ndcg@10: 0.208517\n",
      "PREDS: min -0.5924949008043491 max 0.3169541761448405 mean -0.0006316922209053827 std 0.017947679095522914\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 9.03056588626723e-09 std 0.00019777312466351445\n",
      "HESS: min -1.0 max 597.707942740458 mean 0.37105150402156256 std 1.0375064517321093\n",
      "[5]\ttrain's ndcg@10: 0.210144\tvalid's ndcg@10: 0.21387\ttest's ndcg@10: 0.208636\n",
      "PREDS: min -0.6589726660899545 max 0.31695419714044637 mean -0.0007208989173727121 std 0.0202866566891496\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.997236720863646e-09 std 0.00019770759573299837\n",
      "HESS: min -1.0 max 597.7080041529808 mean 0.37105156044223353 std 1.0375070056871305\n",
      "[6]\ttrain's ndcg@10: 0.210144\tvalid's ndcg@10: 0.21387\ttest's ndcg@10: 0.208636\n",
      "PREDS: min -0.703650645563668 max 0.3169542181957024 mean -0.0007807306711904526 std 0.021878195598713637\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.926239446042132e-09 std 0.0001976442658271906\n",
      "HESS: min -1.0 max 597.7080654975044 mean 0.37105160871234355 std 1.0375074768377979\n",
      "[7]\ttrain's ndcg@10: 0.210144\tvalid's ndcg@10: 0.21387\ttest's ndcg@10: 0.208636\n",
      "PREDS: min -0.7395324480239358 max 0.3169542392983339 mean -0.0008287067972127499 std 0.023165101725227125\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.844289466625478e-09 std 0.0001975836411438891\n",
      "HESS: min -1.0 max 597.7081267876416 mean 0.3710516531098156 std 1.037507912318727\n",
      "[8]\ttrain's ndcg@10: 0.210144\tvalid's ndcg@10: 0.21387\ttest's ndcg@10: 0.208636\n",
      "PREDS: min -0.7702076973159001 max 0.31695426043899966 mean -0.0008696706065773036 std 0.024270370667741206\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.758239110358275e-09 std 0.00019752558482485948\n",
      "HESS: min -1.0 max 597.7081880341154 mean 0.371051694876538 std 1.0375083251260768\n",
      "[9]\ttrain's ndcg@10: 0.210174\tvalid's ndcg@10: 0.213843\ttest's ndcg@10: 0.208517\n",
      "PREDS: min -1.002079431563995 max 0.3169542816107417 mean -0.0012841045659240753 std 0.03549505218273123\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 9.26257996182037e-09 std 0.00019748725590630454\n",
      "HESS: min -1.0 max 597.7082492449756 mean 0.3710519040874812 std 1.037511509381044\n",
      "[10]\ttrain's ndcg@10: 0.210722\tvalid's ndcg@10: 0.214397\ttest's ndcg@10: 0.209949\n",
      "PREDS: min -1.1083373599257806 max 0.316954302746654 mean -0.0014079346852180887 std 0.03759301744308606\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 9.057508772080608e-09 std 0.00019742510437992654\n",
      "HESS: min -1.0 max 597.7083105074026 mean 0.371051790509391 std 1.037511626677387\n",
      "[11]\ttrain's ndcg@10: 0.210795\tvalid's ndcg@10: 0.214397\ttest's ndcg@10: 0.209927\n",
      "PREDS: min -1.1450525600687738 max 0.3169543238965682 mean -0.0014511501484521982 std 0.03838086117277518\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 8.904291045037025e-09 std 0.00019736951588946106\n",
      "HESS: min -1.0 max 597.7083717536892 mean 0.3710518116871015 std 1.0375118813855673\n",
      "[12]\ttrain's ndcg@10: 0.210795\tvalid's ndcg@10: 0.214397\ttest's ndcg@10: 0.209927\n",
      "PREDS: min -1.3150246031673232 max 0.3169543450639096 mean -0.0014771668637760117 std 0.03894247740329253\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.8470773106426e-09 std 0.0001956258970221908\n",
      "HESS: min -1.0 max 597.7084329793939 mean 0.37105253907990166 std 1.037515010008041\n",
      "[13]\ttrain's ndcg@10: 0.210803\tvalid's ndcg@10: 0.214397\ttest's ndcg@10: 0.209959\n",
      "PREDS: min -1.3672255672235354 max 0.3169543703413522 mean -0.0017185264657960297 std 0.04353086970081273\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.402237442638175e-09 std 0.0001954584344588551\n",
      "HESS: min -1.0 max 597.7084893066802 mean 0.37105263210816886 std 1.0375159638417726\n",
      "[14]\ttrain's ndcg@10: 0.210803\tvalid's ndcg@10: 0.214397\ttest's ndcg@10: 0.209959\n",
      "PREDS: min -1.9052525540242553 max 0.3169543954424948 mean -0.0019263298931537147 std 0.048217935950005765\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.531792352475679e-09 std 0.0001939539250736796\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.3710537427782526 std 1.0375219269740457\n",
      "[15]\ttrain's ndcg@10: 0.210698\tvalid's ndcg@10: 0.214286\ttest's ndcg@10: 0.209592\n",
      "PREDS: min -1.9993760959679325 max 0.32596759617515425 mean -0.0020612331919070614 std 0.050777308643505444\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.606367597653277e-09 std 0.0001939577702280649\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.3710538998379323 std 1.0375235567988417\n",
      "[16]\ttrain's ndcg@10: 0.210724\tvalid's ndcg@10: 0.214339\ttest's ndcg@10: 0.209603\n",
      "PREDS: min -2.086279139980147 max 0.33908879708627143 mean -0.0021839851650642013 std 0.05327561280393467\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.73468292669635e-09 std 0.00019396379953435944\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.37105411839568514 std 1.0375253109691545\n",
      "[17]\ttrain's ndcg@10: 0.210699\tvalid's ndcg@10: 0.214273\ttest's ndcg@10: 0.209597\n",
      "PREDS: min -2.799610154784827 max 0.35169409120265677 mean -0.0032630007985310935 std 0.07674227245779705\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.056051319171749e-09 std 0.00019394786661420483\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.37105556276475143 std 1.0375451878410888\n",
      "[18]\ttrain's ndcg@10: 0.210836\tvalid's ndcg@10: 0.214339\ttest's ndcg@10: 0.209582\n",
      "PREDS: min -3.62514267489695 max 0.36377677851789986 mean -0.003971621829143863 std 0.08922266950514564\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.5932682619341774e-09 std 0.00019393021258025663\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.3710574927261149 std 1.0375720211293404\n",
      "[19]\ttrain's ndcg@10: 0.210797\tvalid's ndcg@10: 0.214339\ttest's ndcg@10: 0.20959\n",
      "PREDS: min -3.735753016751043 max 0.3754342593847028 mean -0.0041265455701746485 std 0.09283783585985568\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.356979425548931e-09 std 0.00019392965409867333\n",
      "HESS: min -1.0 max 597.7085458456319 mean 0.3710581735473619 std 1.0375791269377423\n",
      "[20]\ttrain's ndcg@10: 0.211222\tvalid's ndcg@10: 0.214112\ttest's ndcg@10: 0.209387\n",
      "PREDS: min -3.896737477513824 max 0.38669862535987276 mean -0.004335170861000309 std 0.09767452345049586\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.3498935901332323e-09 std 0.00019392677550778348\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710587345999564 std 1.0375868533373949\n",
      "[21]\ttrain's ndcg@10: 0.211226\tvalid's ndcg@10: 0.214135\ttest's ndcg@10: 0.209456\n",
      "PREDS: min -4.029378728259196 max 0.39759668016091476 mean -0.004605282521647093 std 0.10272204086830834\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.7253595132441574e-09 std 0.0001939075540957527\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.37105920248516605 std 1.0375890342236929\n",
      "[22]\ttrain's ndcg@10: 0.211252\tvalid's ndcg@10: 0.214135\ttest's ndcg@10: 0.209456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS: min -4.245018205849272 max 0.4081575931303562 mean -0.004888985034506068 std 0.10930888707914603\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.6717490809967914e-09 std 0.00019390256789553718\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710610363959866 std 1.0376021204761907\n",
      "[23]\ttrain's ndcg@10: 0.211244\tvalid's ndcg@10: 0.214136\ttest's ndcg@10: 0.209496\n",
      "PREDS: min -4.499847582736522 max 0.41840052598823874 mean -0.005269683877220754 std 0.11754296604473283\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.387777224432922e-09 std 0.00019388488498676024\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710625813770398 std 1.0376241435029232\n",
      "[24]\ttrain's ndcg@10: 0.210958\tvalid's ndcg@10: 0.213708\ttest's ndcg@10: 0.209002\n",
      "PREDS: min -4.625270491402515 max 0.42834575883921133 mean -0.005432237376533625 std 0.12135891871994563\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.217978180477547e-09 std 0.00019387639274328586\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710642849281085 std 1.0376381006682514\n",
      "[25]\ttrain's ndcg@10: 0.210955\tvalid's ndcg@10: 0.213722\ttest's ndcg@10: 0.209062\n",
      "PREDS: min -4.809789565372694 max 0.43801996526521436 mean -0.005655291579945558 std 0.12709072417964393\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.981863427062042e-09 std 0.0001938624621805281\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.37106739107325476 std 1.03766254286767\n",
      "[26]\ttrain's ndcg@10: 0.210955\tvalid's ndcg@10: 0.213722\ttest's ndcg@10: 0.209062\n",
      "PREDS: min -5.200051407081426 max 0.44743742475454157 mean -0.006214640139314571 std 0.14002270421416524\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.5354833330762534e-09 std 0.00019382983314936345\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710776396282671 std 1.0377417737267\n",
      "[27]\ttrain's ndcg@10: 0.210951\tvalid's ndcg@10: 0.213737\ttest's ndcg@10: 0.209039\n",
      "PREDS: min -5.250077976896019 max 0.45660468160062556 mean -0.006287700021105355 std 0.14167738962330945\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.4939873430063686e-09 std 0.00019382881459765767\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710780274047259 std 1.037748420957012\n",
      "[28]\ttrain's ndcg@10: 0.210969\tvalid's ndcg@10: 0.213737\ttest's ndcg@10: 0.209065\n",
      "PREDS: min -5.480673562977875 max 0.465552389725106 mean -0.006724468109701925 std 0.15047283923682048\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.0029403523719285e-09 std 0.00019380255531478195\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710858042923728 std 1.0377995207677242\n",
      "[29]\ttrain's ndcg@10: 0.210961\tvalid's ndcg@10: 0.213737\ttest's ndcg@10: 0.209065\n",
      "PREDS: min -5.7136489193520354 max 0.4742852713039998 mean -0.007052778212508899 std 0.1580710634787955\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.9223459717857994e-09 std 0.00019379006322856938\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710876600941876 std 1.0378327497575859\n",
      "[30]\ttrain's ndcg@10: 0.210976\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209065\n",
      "PREDS: min -6.036028226114685 max 0.4828154001851416 mean -0.00750123522356464 std 0.1686925407983438\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 2.817461159870279e-09 std 0.00019377084322429796\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3710907217865042 std 1.0378913371880139\n",
      "[31]\ttrain's ndcg@10: 0.210991\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209072\n",
      "PREDS: min -6.543039853737256 max 0.49115058672350204 mean -0.008343896854456407 std 0.18689586372275638\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.550955997622994e-09 std 0.0001936990466177736\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.371102703112203 std 1.0380812082258974\n",
      "[32]\ttrain's ndcg@10: 0.210993\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209072\n",
      "PREDS: min -6.668659849851252 max 0.4992948455493752 mean -0.008493357145569312 std 0.1898501704668184\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.5392258000021597e-09 std 0.00019369203948768276\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3711046214501569 std 1.0381261807313082\n",
      "[33]\ttrain's ndcg@10: 0.210993\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209072\n",
      "PREDS: min -6.719550746761169 max 0.5072759133129346 mean -0.008570356375136198 std 0.1916344732657267\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4932967538220043e-09 std 0.00019369014292092238\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.37110553622252485 std 1.0381475371633417\n",
      "[34]\ttrain's ndcg@10: 0.210993\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209072\n",
      "PREDS: min -6.756699678220204 max 0.5150999296694663 mean -0.008623153153377581 std 0.1928851597662896\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4683925687342097e-09 std 0.00019368944330852827\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3711061604865048 std 1.038162920441905\n",
      "[35]\ttrain's ndcg@10: 0.210993\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.209072\n",
      "PREDS: min -6.798769196787546 max 0.522774651132893 mean -0.008682668682523948 std 0.1942977556056896\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4444913829094886e-09 std 0.00019368835805918407\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.37110688017254995 std 1.0381809739733896\n",
      "[36]\ttrain's ndcg@10: 0.211049\tvalid's ndcg@10: 0.213739\ttest's ndcg@10: 0.20912\n",
      "PREDS: min -7.547854470531245 max 0.53030668374776 mean -0.009608063052447704 std 0.2133693201846881\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.604003180527309e-09 std 0.00019363347668555685\n",
      "HESS: min -1.0 max 597.7085957866774 mean 0.3711230926493121 std 1.0386706988055587\n",
      "[37]\ttrain's ndcg@10: 0.209858\tvalid's ndcg@10: 0.213236\ttest's ndcg@10: 0.208999\n",
      "PREDS: min -7.562424287001156 max 0.5376908307230263 mean -0.009603458481455806 std 0.21337646326845605\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.488265269979734e-09 std 0.00019335503689866168\n",
      "HESS: min -1.0 max 597.708666353578 mean 0.3711231801227174 std 1.0386710796724041\n",
      "[38]\ttrain's ndcg@10: 0.209879\tvalid's ndcg@10: 0.21325\ttest's ndcg@10: 0.208986\n",
      "PREDS: min -7.575151105137326 max 0.5449463820618518 mean -0.009600521672607804 std 0.21338271417212104\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.003107585587501e-09 std 0.00019317701638535135\n",
      "HESS: min -1.0 max 597.7087369510679 mean 0.3711232470601226 std 1.0386714246869182\n",
      "[39]\ttrain's ndcg@10: 0.209879\tvalid's ndcg@10: 0.21325\ttest's ndcg@10: 0.208986\n",
      "PREDS: min -7.586580759704509 max 0.5520785666191566 mean -0.009598157376976954 std 0.213388482860564\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.3923104005063074e-09 std 0.0001930355033975471\n",
      "HESS: min -1.0 max 597.708807579249 mean 0.3711233052254606 std 1.0386717497207747\n",
      "[40]\ttrain's ndcg@10: 0.209879\tvalid's ndcg@10: 0.21325\ttest's ndcg@10: 0.208986\n",
      "PREDS: min -7.597030971160155 max 0.5590924596285641 mean -0.009596121728375236 std 0.21339390508654793\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.7102612463191415e-09 std 0.0001929155411590257\n",
      "HESS: min -1.0 max 597.7088782357062 mean 0.3711233577421721 std 1.0386720605239086\n",
      "[41]\ttrain's ndcg@10: 0.213885\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.606441159634534 max 0.5659927628057284 mean -0.009596007708488759 std 0.2133972415633488\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.651879701786147e-09 std 0.00019289446660911326\n",
      "HESS: min -1.0 max 597.7089252144586 mean 0.37112338804211775 std 1.0386722984154302\n",
      "[42]\ttrain's ndcg@10: 0.213885\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.615304436377282 max 0.5727838136276675 mean -0.009595887060146096 std 0.21340043495228236\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.596470175935721e-09 std 0.00019287479704286527\n",
      "HESS: min -1.0 max 597.708972228282 mean 0.3711234169627682 std 1.0386725301356659\n",
      "[43]\ttrain's ndcg@10: 0.21389\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.6824209597073265 max 0.5794698454226236 mean -0.009667958648448239 std 0.21489293099755785\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.5891709259879605e-09 std 0.00019287091368282004\n",
      "HESS: min -1.0 max 597.708972228282 mean 0.37112872706860695 std 1.0387709732906911\n",
      "[44]\ttrain's ndcg@10: 0.21389\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS: min -7.690816773513292 max 0.5860547158240653 mean -0.009667837175891534 std 0.214895995389082\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.5367577480538565e-09 std 0.00019285241717162665\n",
      "HESS: min -1.0 max 597.709019279347 mean 0.3711287547940943 std 1.0387711995501394\n",
      "[45]\ttrain's ndcg@10: 0.21389\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.69880696746467 max 0.5925420641809134 mean -0.009667711498724852 std 0.21489895267164658\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.486610430391236e-09 std 0.00019283495634255315\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711287814917911 std 1.0387714212041483\n",
      "[46]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.6266630374968445 max 0.5989353342570823 mean -0.009615765467260716 std 0.213911247572566\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.456647670988641e-09 std 0.00019283817228011872\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711288052844756 std 1.0387724408468058\n",
      "[47]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.470936065752018 max 0.6052378595510974 mean -0.009503507913881668 std 0.21181285432698516\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.415576455893036e-09 std 0.00019284350502856183\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711288217411357 std 1.038773452595429\n",
      "[48]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.471214037130417 max 0.6114527341067816 mean -0.009504602370349275 std 0.2118265793432163\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 5.39746725720438e-09 std 0.00019284468449030036\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112883271737435 std 1.0387737741857543\n",
      "[49]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.471977826128419 max 1.0724253542866014 mean -0.0094882546759865 std 0.2119017947120911\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.158939588606913e-09 std 0.0001929398516941612\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112916666369233 std 1.0387782945062742\n",
      "[50]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.472252991171284 max 1.0811803895627445 mean -0.009489200873097087 std 0.21191639794031733\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.14276580654241e-09 std 0.00019294142174265273\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711291821944143 std 1.0387786695522585\n",
      "[51]\ttrain's ndcg@10: 0.213903\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.4725274077442245 max 1.089532556046721 mean -0.00949016498671227 std 0.2119309856975644\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.127616010784614e-09 std 0.00019294290577656265\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112919741633993 std 1.0387790404599169\n",
      "[52]\ttrain's ndcg@10: 0.213897\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215183\n",
      "PREDS: min -7.472801081313624 max 1.0975313755074856 mean -0.00949114468447947 std 0.21194556533649309\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.1133627602904296e-09 std 0.0001929443140269415\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112921236532637 std 1.038779407658752\n",
      "[53]\ttrain's ndcg@10: 0.213897\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.473074017425297 max 1.1052168024965285 mean -0.009492138081127172 std 0.211960142764618\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.099903114716254e-09 std 0.00019294565475368205\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112922707030194 std 1.038779771495943\n",
      "[54]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.473346221899364 max 1.112621871238473 mean -0.009493143617518672 std 0.21197472287627359\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 4.087152013473131e-09 std 0.00019294693478707993\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112924155523963 std 1.0387801322599362\n",
      "[55]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.474095311549366 max 1.6264819595551092 mean -0.00947480479520988 std 0.2120977440063333\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4614861198956788e-09 std 0.00019303592930178717\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112985705465845 std 1.0387881072822955\n",
      "[56]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.474839350676726 max 1.630683269570949 mean -0.009475514987291513 std 0.21211521188320825\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4601166218197296e-09 std 0.00019303650081579468\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112988421764415 std 1.038788945494409\n",
      "[57]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.475578438016925 max 1.6348161812943307 mean -0.00947622164729272 std 0.21213255955075622\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4588240995197934e-09 std 0.00019303705873053233\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112991117141947 std 1.038789778295018\n",
      "[58]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.476312669157986 max 1.6388837070111169 mean -0.009476924791959175 std 0.21214978959126785\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4576047901774627e-09 std 0.000193037603565616\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112993792194654 std 1.0387906058120042\n",
      "[59]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.477042136567507 max 1.642888678690572 mean -0.009477624439983774 std 0.2121669044914209\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4564551881248926e-09 std 0.00019303813580844576\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711299644749098 std 1.0387914281680757\n",
      "[60]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.47776692847812 max 1.6468336865201243 mean -0.00947832061261177 std 0.21218390660432646\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.45537205593436e-09 std 0.00019303865590552028\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37112999083559056 std 1.0387922454784415\n",
      "[61]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.478487131438077 max 1.6507211746074406 mean -0.009479013333622091 std 0.21220079822470483\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.45435236614887e-09 std 0.00019303916427716643\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711300170090702 std 1.038793057855032\n",
      "[62]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.479202830571431 max 1.6545533973889797 mean -0.00947970263094591 std 0.2122175815854189\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4533933069692543e-09 std 0.00019303966131129798\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711300430001705 std 1.0387938654059243\n",
      "[63]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.479914105819257 max 1.6583325073464743 mean -0.009480388529739156 std 0.2122342587899108\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4524922314210456e-09 std 0.00019304014737730712\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113006881346855 std 1.0387946682329436\n",
      "[64]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.480621037042659 max 1.662060474631074 mean -0.009481071060615172 std 0.21225083191009825\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.451646682038284e-09 std 0.0001930406228138827\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113009445332074 std 1.0387954664354397\n",
      "[65]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.48132370002814 max 1.6657391853525296 mean -0.009481750252121354 std 0.2122673029158222\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.450854338660541e-09 std 0.00019304108794436937\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113011992388817 std 1.038796260107849\n",
      "[66]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS: min -7.482022169970783 max 1.6693703842494882 mean -0.009482426136436626 std 0.21228367374104215\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4501130341918555e-09 std 0.00019304154306816945\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113014522915017 std 1.03879704934222\n",
      "[67]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.482716519126669 max 1.6729557448311556 mean -0.009483098744572958 std 0.2122999462447817\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4494207181109293e-09 std 0.0001930419884716189\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711301703729345 std 1.038797834226992\n",
      "[68]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.48340681811263 max 1.676496816502744 mean -0.009483768109455675 std 0.212316122229332\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4487754702898772e-09 std 0.000193042424420253\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113019535887654 std 1.0387986148473434\n",
      "[69]\ttrain's ndcg@10: 0.213892\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.484093133482975 max 1.6799950907227346 mean -0.009484434261171936 std 0.21233220339875616\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.44817546835709e-09 std 0.00019304285116898977\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711302201904425 std 1.0387993912838287\n",
      "[70]\ttrain's ndcg@10: 0.213896\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.484775532811579 max 1.6834519471826255 mean -0.009485097234362284 std 0.2123481914617829\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4476190033492942e-09 std 0.00019304326895396175\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113024487099416 std 1.0388001636166395\n",
      "[71]\ttrain's ndcg@10: 0.213896\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.48545408071918 max 1.6868687178511113 mean -0.009485757061769974 std 0.21236408805527188\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4471044497068683e-09 std 0.00019304367800245537\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711302694037682 std 1.0388009319226064\n",
      "[72]\ttrain's ndcg@10: 0.213896\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.486128839378724 max 1.6902466493169417 mean -0.009486413775960613 std 0.2123798947476922\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.446630273983097e-09 std 0.0001930440785274383\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711302937918305 std 1.0388016962750064\n",
      "[73]\ttrain's ndcg@10: 0.213896\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.486799870911049 max 1.693586934257677 mean -0.009487067411080527 std 0.21239561309897206\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.446195020056703e-09 std 0.00019304447073218128\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113031803819285 std 1.038802456746594\n",
      "[74]\ttrain's ndcg@10: 0.213896\tvalid's ndcg@10: 0.216456\ttest's ndcg@10: 0.215188\n",
      "PREDS: min -7.487467234070927 max 1.6968907017705923 mean -0.009487717999392643 std 0.21241124458396315\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.445797307145983e-09 std 0.00019304485480919697\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711303421456995 std 1.038803213405951\n",
      "[75]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.4881309867334105 max 1.7001590219322884 mean -0.009488365573958919 std 0.2124267906489233\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4454358251400776e-09 std 0.00019304523094085117\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113036611710426 std 1.0388039663201267\n",
      "[76]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.488791184857846 max 1.7033929211637937 mean -0.009489010166998286 std 0.2124422526912204\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4451093261294275e-09 std 0.0001930455993017656\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113038995506326 std 1.0388047155538185\n",
      "[77]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.489447883292961 max 1.7065933604026458 mean -0.009489651811461339 std 0.21245763207271953\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.444816627498251e-09 std 0.00019304596005572132\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711304136621216 std 1.0388054611697826\n",
      "[78]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.490101136930156 max 1.7097612751524627 mean -0.009490290541117512 std 0.2124729301537609\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4445565977882502e-09 std 0.00019304631336145207\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113043724080835 std 1.0388062032307614\n",
      "[79]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.490750996150134 max 1.7128975535985358 mean -0.00949092638606783 std 0.21248814818623848\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.44432815879479e-09 std 0.0001930466593699167\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113046069347005 std 1.0388069417942984\n",
      "[80]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.491874512043297 max 1.7160030505615411 mean -0.009491767952134808 std 0.21251091225437363\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.443424397192972e-09 std 0.00019304701567287008\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711305316357857 std 1.0388076844700846\n",
      "[81]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.492508375827949 max 1.719078561819741 mean -0.009492387515591649 std 0.2125257622973724\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4432528656490993e-09 std 0.00019304734825148248\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113055456597943 std 1.0388084059306864\n",
      "[82]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.493139135215467 max 1.7221248742015363 mean -0.009493004448960092 std 0.21254053910763254\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.443110022593645e-09 std 0.0001930476739350826\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113057738077093 std 1.0388091241982904\n",
      "[83]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.493766833891686 max 1.7251427192756579 mean -0.009493618779513336 std 0.21255524376432294\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.442994972500873e-09 std 0.0001930479928481099\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711306000821481 std 1.0388098393222764\n",
      "[84]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.494856087777012 max 1.7281328150835351 mean -0.009494435343998337 std 0.21257731740432065\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4422235786422855e-09 std 0.00019304832186719362\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.371130669006109 std 1.0388105621968209\n",
      "[85]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.495469277002253 max 1.731095839595845 mean -0.009495034896609465 std 0.21259168824637326\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.442158619839378e-09 std 0.00019304862827054065\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711306912336463 std 1.038811261792768\n",
      "[86]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.496079651233835 max 1.7340324451280837 mean -0.009495632062640577 std 0.21260599270593883\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4421190210286614e-09 std 0.00019304892824111345\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.3711307133567432 std 1.038811958502347\n",
      "[87]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.49713757598988 max 1.7369432684449893 mean -0.009496424967357097 std 0.21262743945995627\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4414405091366204e-09 std 0.0001930492382340449\n",
      "HESS: min -1.0 max 597.7090663616768 mean 0.37113078046505654 std 1.038812663640357\n",
      "[88]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS: min -7.497721151826873 max 1.7398289020791797 mean -0.009496345811703225 std 0.21262819091767052\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.439082166814242e-09 std 0.0001930482013243016\n",
      "HESS: min -1.0 max 597.709113395563 mean 0.3711307867509223 std 1.0388128016915863\n",
      "[89]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.498318155022186 max 1.7426899358644694 mean -0.009496930121362792 std 0.2126421837231902\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.439112351760297e-09 std 0.0001930484830339309\n",
      "HESS: min -1.0 max 597.709113395563 mean 0.3711308084088782 std 1.0388134837372927\n",
      "[90]\ttrain's ndcg@10: 0.213875\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215201\n",
      "PREDS: min -7.4989125571404776 max 1.7455269275970817 mean -0.00949751221463924 std 0.2126561152635032\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.439165064689008e-09 std 0.00019304875870792743\n",
      "HESS: min -1.0 max 597.709113395563 mean 0.37113082997087904 std 1.0388141631283163\n",
      "[91]\ttrain's ndcg@10: 0.213872\tvalid's ndcg@10: 0.216442\ttest's ndcg@10: 0.215315\n",
      "PREDS: min -7.499941674441608 max 1.7483404175712078 mean -0.009498284107444688 std 0.21267698165051702\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4385943342742712e-09 std 0.0001930490444061288\n",
      "HESS: min -1.0 max 597.709113395563 mean 0.3711308953821136 std 1.0388148516249185\n",
      "[92]\ttrain's ndcg@10: 0.214273\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215431\n",
      "PREDS: min -7.500523702712797 max 1.7511309164544744 mean -0.009498853590281667 std 0.2126906285004394\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4386876675175975e-09 std 0.00019304930885336\n",
      "HESS: min -1.0 max 597.709113395563 mean 0.3711309165401573 std 1.0388155177634686\n",
      "[93]\ttrain's ndcg@10: 0.214273\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215431\n",
      "PREDS: min -7.501106985335065 max 1.7538989260563906 mean -0.009498778008424708 std 0.21269136113023762\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4364458486323263e-09 std 0.0001930482424085556\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.37113092269281317 std 1.0388156541900846\n",
      "[94]\ttrain's ndcg@10: 0.214273\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215431\n",
      "PREDS: min -7.501686599754476 max 1.756644929901486 mean -0.009499346109198495 std 0.21270494758814767\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4365807442497783e-09 std 0.0001930484951947079\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.3711309437368025 std 1.0388163175641576\n",
      "[95]\ttrain's ndcg@10: 0.21427\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215454\n",
      "PREDS: min -7.502689110726896 max 1.759369396491978 mean -0.009500098569151476 std 0.2127252780743427\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.436106679976627e-09 std 0.00019304875797899242\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.37113100758175843 std 1.038816990708233\n",
      "[96]\ttrain's ndcg@10: 0.21427\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215454\n",
      "PREDS: min -7.503257229280285 max 1.7620727700641727 mean -0.009500654937237958 std 0.21273860010971035\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4362777975935705e-09 std 0.00019304900014143942\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.3711310282510338 std 1.0388176417718646\n",
      "[97]\ttrain's ndcg@10: 0.21427\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215454\n",
      "PREDS: min -7.503823100600148 max 1.7647554861901396 mean -0.009501209387102284 std 0.212751869255993\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.436467442845064e-09 std 0.0001930492368296044\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.3711310488377497 std 1.0388182905554972\n",
      "[98]\ttrain's ndcg@10: 0.21427\tvalid's ndcg@10: 0.21684\ttest's ndcg@10: 0.215454\n",
      "PREDS: min -7.5043867501559545 max 1.7674179574951756 mean -0.009501761935070966 std 0.2127650861396416\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4366751177963656e-09 std 0.0001930494681134734\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.37113106934304446 std 1.038818937087459\n",
      "[99]\ttrain's ndcg@10: 0.214458\tvalid's ndcg@10: 0.217159\ttest's ndcg@10: 0.215842\n",
      "PREDS: min -7.505364579929994 max 1.7700605933098412 mean -0.009502496345691955 std 0.21278491938924424\n",
      "GAIN: min -0.05098687660112451 max 0.05074442516811658 mean 3.4362881258791037e-09 std 0.0001930497091747759\n",
      "HESS: min -1.0 max 597.7091604311347 mean 0.37113113173603474 std 1.0388195960054825\n",
      "[100]\ttrain's ndcg@10: 0.214458\tvalid's ndcg@10: 0.217159\ttest's ndcg@10: 0.215842\n",
      "training took 3989.1875 s\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Error')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluation(preds, train_data):\n",
    "    global ds_to_queries\n",
    "    #bz = eval_boltzrank(ds_to_queries[len(preds)][0], preds)\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE eval: \" + str(avg_mse))\n",
    "    return 'MSE', avg_mse, False\n",
    "\n",
    "def compute_grads(preds, train_data): \n",
    "    global ds_to_queries\n",
    "    global train_id\n",
    "    global probs_with_labels\n",
    "    gain, hess = eval_boltzrank_grads(ds_to_queries[train_id][0], preds, probs_with_labels[train_id])\n",
    "    gain = np.asarray(gain)\n",
    "    hess = np.asarray(hess)\n",
    "    print(\"PREDS: min \" + str(np.min(preds)) + \" max \" + str(np.max(preds)) + \" mean \" + str(np.mean(preds)) + \" std \" + str(np.std(preds)))\n",
    "    print(\"GAIN: min \" + str(np.min(gain)) + \" max \" + str(np.max(gain)) + \" mean \" + str(np.mean(gain)) + \" std \" + str(np.std(gain)))\n",
    "    print(\"HESS: min \" + str(np.min(hess)) + \" max \" + str(np.max(hess)) + \" mean \" + str(np.mean(hess)) + \" std \" + str(np.std(hess)))\n",
    "    return gain, hess\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'metric': ['ndcg'],# ['None']\n",
    "    'ndcg_eval_at': 10\n",
    "}    \n",
    "\n",
    "print(\"training lightgbm...\")\n",
    "start = time.process_time()\n",
    "lgbm_info = {}\n",
    "lgbm_model = lightgbm.train(params, train_lgb, num_boost_round=100,\n",
    "                            #feval = evaluation,\n",
    "                            fobj  = compute_grads,\n",
    "                            valid_sets   = [train_lgb, valid_lgb, test_lgb], \n",
    "                            valid_names  = [\"train\", \"valid\", \"test\"],\n",
    "                            evals_result = lgbm_info,\n",
    "                            verbose_eval = 1)\n",
    "print(\"training took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(lgbm_info['train'][METRIC_NAME], label='training')\n",
    "plt.plot(lgbm_info['valid'][METRIC_NAME], label='validation')\n",
    "plt.plot(lgbm_info['test'][METRIC_NAME], label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"# Trees\")\n",
    "plt.ylabel(METRIC_NAME)\n",
    "plt.title(\"Model Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(q, k):\n",
    "    indexes = set(range(0, len(q.perms)))\n",
    "    indexes.remove(k)\n",
    "    result = []\n",
    "    for i in range(len(q.perms[k])):\n",
    "        tmp = set(indexes)\n",
    "        for j in tmp:\n",
    "            if q.perms[k][i] != q.perms[j][i]:\n",
    "                indexes.remove(j)\n",
    "    for w in indexes:\n",
    "        if w < k:\n",
    "            result.append((w, k))\n",
    "        else: \n",
    "            result.append((k,w))\n",
    "    return result\n",
    "\n",
    "def checkRepetitions():\n",
    "    global queries\n",
    "    same = dict()\n",
    "    for q in queries.values():\n",
    "        for i in range(len(q.perms)):\n",
    "            r = check(q, i)\n",
    "            if len(r) != 0:\n",
    "                if not q.qid in same.keys():\n",
    "                    same[q.qid] = set()\n",
    "                for t in r:\n",
    "                    same[q.qid].add(t)\n",
    "\n",
    "    print(str(len(same.keys())) + \"/\" + str(len(queries.keys())) + \" queries have duplicate permutations\")\n",
    "    for q, s in same.items():\n",
    "        print(\"query \" + str(q) + \" has repeated permutations: \" + str(s))\n",
    "        \n",
    "#checkRepetitions()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for query in queries.values():\n",
    "    for prob in query.probs:\n",
    "        if not prob in freq.keys():\n",
    "            freq[prob] = 0\n",
    "        freq[prob] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for prob, f in sorted(freq.items()):\n",
    "    x.append(prob)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel(\"# rank\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"probabilities of the \" + str(totperms) + \" permutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "totperms = 0\n",
    "for query in queries.values():\n",
    "    for ndcg in query.ndcgs:\n",
    "        totperms += 1\n",
    "        if not ndcg in freq.keys():\n",
    "            freq[ndcg] = 0\n",
    "        freq[ndcg] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for ndcg, f in sorted(freq.items()):\n",
    "    x.append(ndcg)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"ndcg@10\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"ndcg@10 frequencies over \" + str(totperms) + \" permutations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
