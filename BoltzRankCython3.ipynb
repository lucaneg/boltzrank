{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# BoltzRank #\n",
    "## Luca Negrini - Mat. 956516 ##\n",
    "### From \"BoltzRank: Learning to Maximize Expected Ranking Gain\" ###\n",
    "#### Maxims M. Volkovs, Richard S. Zemel ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Initialization ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "%load_ext cython\n",
    "\n",
    "# install lightgbm (required only on first run)\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# see http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "from sklearn.datasets import load_svmlight_file \n",
    "\n",
    "# datasets available at: \n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "DATASET_FOLDER = \"C:/opt/kiis-training/MSLR-WEB10K/Fold1/\"\n",
    "PERM_FOLDER = DATASET_FOLDER + \"perms/\"\n",
    "METRIC_NAME = 'ndcg@10'#'BoltzRank-NDCG@10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Data loading ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensureFile(path):\n",
    "    if not os.path.exists(path) or not os.path.isfile(path):\n",
    "        raise FileNotFoundError(\"'\" + path + \"': no such file\")        \n",
    "    return path\n",
    "\n",
    "def retrieveFileNames():\n",
    "    folder = DATASET_FOLDER + '/' if DATASET_FOLDER[-1:] != '/' else DATASET_FOLDER\n",
    "    train_file = ensureFile(folder + \"train.txt\")\n",
    "    valid_file = ensureFile(folder + \"vali.txt\")\n",
    "    test_file = ensureFile(folder + \"test.txt\")\n",
    "    return train_file, valid_file, test_file\n",
    "\n",
    "def loadDataset(path):\n",
    "    return load_svmlight_file(path, query_id=True)\n",
    "\n",
    "def loadLightGBM(svmlight_dataset):\n",
    "    query_lens = [sum(1 for _ in group) for key, group in itertools.groupby(svmlight_dataset[2])]\n",
    "    return lightgbm.Dataset(data=svmlight_dataset[0], label=svmlight_dataset[1], group=query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, qid):\n",
    "        self.qid = qid\n",
    "        self.labels_to_docs = {}\n",
    "    def addlabel(self, label):\n",
    "        if not label in self.labels_to_docs:\n",
    "            self.labels_to_docs[label] = list()\n",
    "    def adddoc(self, label, doc):\n",
    "        self.labels_to_docs[label].append(doc)\n",
    "    def finalize(self, alllabels):\n",
    "        self.labels = np.zeros(len(self.labels_to_docs.keys()), dtype=int)\n",
    "        self.docs = np.empty(len(self.labels_to_docs.keys()), dtype=object)\n",
    "        i = 0\n",
    "        totaldocs = 0\n",
    "        sorteddict = sorted(self.labels_to_docs.items(), reverse = True)\n",
    "        for label, docs in sorteddict:\n",
    "            self.labels[i] = label\n",
    "            self.docs[i] = np.zeros(len(docs), dtype=int)\n",
    "            for j in range(len(docs)):\n",
    "                self.docs[i][j] = docs[j]\n",
    "            i += 1\n",
    "            totaldocs += len(docs)\n",
    "        self.alldocs = np.concatenate(self.docs)\n",
    "        self.flatlabels = np.zeros(totaldocs, dtype=np.double)\n",
    "        i = 0\n",
    "        for label, docs in sorteddict:\n",
    "            for j in range(len(docs)):\n",
    "                self.flatlabels[i] = label\n",
    "                i += 1       \n",
    "        k = min(10, len(self.alldocs))\n",
    "        self.idealdcg = dcg_k(self.alldocs, alllabels, k) \n",
    "        del self.labels_to_docs\n",
    "    def setperms(self, perms):\n",
    "        self.perms = perms\n",
    "    def setprobs(self, probs):\n",
    "        self.probs = probs\n",
    "    def setndcgs(self, ndcgs):\n",
    "        self.ndcgs = ndcgs\n",
    "    def __repr__(self):  \n",
    "        return str(self)\n",
    "    def __str__(self):\n",
    "        res = \"Query \" + str(self.qid) + \"[\"\n",
    "        res += \"\\nideal dcg: \" + str(self.idealdcg)\n",
    "        for i in range(len(self.labels)):\n",
    "            res += \"\\n\" + str(self.labels[i]) + \" -> \" + str(self.docs[i])\n",
    "        res += \"]\"\n",
    "        if hasattr(self, 'perms'):\n",
    "            for i in range(len(self.perms)):\n",
    "                res += \"\\n[\" + str(self.perms[i]) + \"] -> p: \" + str(self.probs[i]) + \", dcg: \" + str(self.ndcgs[i])\n",
    "        else:\n",
    "            res += \"\\nNo permutations computed yet\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "#  dataset: svmlight_dataset \n",
    "#      the datset to process\n",
    "# returned values:\n",
    "#  query_to_labels_to_documents: dict(int -> dict(float -> list(int)))\n",
    "#      a map containing, for each query in the dataset, the documents (row index in the dataset) provided \n",
    "#      in the input dataset grouped by label\n",
    "#  doc_to_query: dict(document -> query)\n",
    "#      a mapping between document (row index in the dataset) and the relative query\n",
    "def mapQueryToDocuments(dataset):\n",
    "    queries = {}\n",
    "    alllabels = np.negative(np.ones(len(dataset[2]), dtype=np.double))\n",
    "    for i in range(0, len(dataset[2])):\n",
    "        if not dataset[2][i] in queries:\n",
    "            queries[dataset[2][i]] = Query(dataset[2][i])\n",
    "        query = queries[dataset[2][i]]\n",
    "        query.addlabel(dataset[1][i])\n",
    "        query.adddoc(dataset[1][i], i)\n",
    "        alllabels[i] = dataset[1][i]\n",
    "        \n",
    "    for q in queries.values():\n",
    "        q.finalize(alllabels)\n",
    "    \n",
    "    return queries, alllabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### BoltzRank logic in Cython ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "from libc.math cimport exp\n",
    "from cython.parallel import prange\n",
    "from cython import boundscheck, wraparound\n",
    "from libc.math cimport log, log2\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from libc.stdio cimport printf\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                              NDCG EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double dcg_k(int[:] rank, double[:] scores, int k) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(k, schedule='static', num_threads=8):\n",
    "        result += (2**scores[rank[i]] - 1) / (log2(i + 2)) # should be i+1, but with numbering starting from 1 instead of 0\n",
    "    return result\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double ndcg_k(int[:] rank, double[:] scores, int k, double ideal) nogil:\n",
    "    if ideal == 0:\n",
    "        return 1.0\n",
    "    return dcg_k(rank, scores, k) / ideal\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                          PERMUTATIONS GENERATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS = 100\n",
    "RANK_SAMPLE_SET_DISTRIBUTIONS = [\n",
    "                                int(.30 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->0\n",
    "                                int(.22 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->1\n",
    "                                int(.18 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->2\n",
    "                                int(.12 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 4->3\n",
    "                                int(.10 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->0\n",
    "                                int(.06 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->1\n",
    "                                int(.02 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 3->2\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->0\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS), # 2->1\n",
    "                                int(.0 * RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS)  # 1->0\n",
    "                                ]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef bint contained(int[:,:] container, int[:] array) nogil:\n",
    "    cdef bint match\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    for i in prange(len(container), schedule='static', num_threads=8):\n",
    "        if container[i][0] == -1 or len(container[i]) != len(array):\n",
    "            continue\n",
    "        else:\n",
    "            match = True\n",
    "            for j in range(len(container[i])):\n",
    "                if container[i][j] != array[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef void setrow(int[:,:] container, int pos, int[:] array) nogil:\n",
    "    cdef int i\n",
    "    for i in prange(len(container[pos]), schedule='static', num_threads=8):\n",
    "        container[pos][i] = array[i]\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef int[:,:] allPerms(int[:] source, long long fact):\n",
    "    cdef int i = 0\n",
    "    cdef int k\n",
    "    perm = itertools.permutations(source)\n",
    "    cdef int[:,:] result = np.zeros((fact, len(source)), dtype=int)\n",
    "    for p in perm:\n",
    "        for k in range(len(p)):\n",
    "            result[i][k] = p[k]\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "#source: label -> docid*, i: int, j: int, count: int, perms_with_prob: tuple<int> -> float\n",
    "#return: number of not computed permutations\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def perform_permutation(query, int i, int j, int count, int[:,:] perms, int start):\n",
    "    if not i in query.labels or not j in query.labels:\n",
    "        # no swapping possible\n",
    "        return count, start\n",
    "    # find the indexes of the desired labels\n",
    "    i = [k for k in range(len(query.labels)) if query.labels[k] == i][0]\n",
    "    j = [k for k in range(len(query.labels)) if query.labels[k] == j][0]\n",
    "    cdef int c = 0\n",
    "    cdef int _min = min(len(query.docs[i]), len(query.docs[j]))\n",
    "    cdef int amount = max(1, int(_min * .5))\n",
    "    limit = factorial(_min) / (factorial(amount) * factorial(_min - amount))\n",
    "    cdef int k\n",
    "    cdef int d\n",
    "    for k in range(count):\n",
    "        perm = query.docs.copy()\n",
    "        first = random.sample(range(len(query.docs[i])), k=amount)\n",
    "        second = random.sample(range(len(query.docs[j])), k=amount)\n",
    "        for d in range(len(first)):\n",
    "            perm[i][first[d]], perm[j][second[d]] = query.docs[j][second[d]], query.docs[i][first[d]]\n",
    "        p = np.concatenate(perm)\n",
    "        if not contained(perms, p):\n",
    "            setrow(perms, start + c, p)\n",
    "            c += 1\n",
    "            if c == limit:\n",
    "                return count - c, start + c\n",
    "        else:\n",
    "            k -= 1\n",
    "    return 0, start + c\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def process_query(query, alllabels):\n",
    "    cdef int carry = 0\n",
    "    fact = factorial(len(query.alldocs))\n",
    "    cdef perms\n",
    "    cdef int last = 0\n",
    "    if fact <= RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS:\n",
    "        # evaluate all possible permutations, each one representing a different ranking\n",
    "        perms = allPerms(query.alldocs, fact)\n",
    "    else:\n",
    "        perms = np.negative(np.ones((RANK_SAMPLE_SET_MAX_QUERY_PERMUTATIONS, len(query.alldocs)), dtype=int))\n",
    "        # switch the labels of the documents, then sort the documents by label to obtain a ranking\n",
    "        carry, last = perform_permutation(query, 4, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[0], perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[1] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[2] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 4, 3, RANK_SAMPLE_SET_DISTRIBUTIONS[3] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[4] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[5] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 3, 2, RANK_SAMPLE_SET_DISTRIBUTIONS[6] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[7] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 2, 1, RANK_SAMPLE_SET_DISTRIBUTIONS[8] + carry, perms, last)\n",
    "        carry, last = perform_permutation(query, 1, 0, RANK_SAMPLE_SET_DISTRIBUTIONS[9] + carry, perms, last)\n",
    "        if carry != 0:\n",
    "            if not query.alldocs in perms:\n",
    "                perms[last] = query.alldocs\n",
    "        perms = perms[perms.max(axis=1)>=0]\n",
    "    cdef double[:] probs = np.zeros(len(perms))\n",
    "    cdef double[:] ndcgs = np.zeros(len(perms))\n",
    "    query.setperms(perms)\n",
    "    query.setprobs(rank_probabilities(perms, probs, alllabels))\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in range(len(perms)):\n",
    "        ndcgs[i] = ndcg_k(perms[i], alllabels, k, query.idealdcg)\n",
    "    query.setndcgs(ndcgs)\n",
    "    return query\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                           GRADIENTS EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] doc_energy(int[:] rank, double[:] scores, double[:,:] result) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return result\n",
    "    cdef double factor = 4.0 / (m * ((m - 1)**2))\n",
    "    cdef double res\n",
    "    cdef double res_w_scores\n",
    "    cdef int k, pos\n",
    "    for pos in prange(len(rank), schedule='static', num_threads=8):\n",
    "        res = 0.0\n",
    "        res_w_scores = 0.0\n",
    "        for k in range(len(rank)):\n",
    "            if k < pos: \n",
    "                res = res + (pos - k)\n",
    "                res_w_scores = res_w_scores + (pos - k) * (scores[rank[pos]] - scores[rank[k]])\n",
    "            #elif k > pos: \n",
    "            #    res = res + (k - pos)\n",
    "            #    res_w_scores = res_w_scores + (k - pos) * (scores[rank[k]] - scores[rank[pos]])\n",
    "        result[rank[pos]][0] = factor * res_w_scores\n",
    "        result[rank[pos]][1] = factor * res\n",
    "        #printf(\"result of %i: m %i, factor %f, res %f, res_scores %f, result[0] %f, result[1] %f\\n\", pos, m, factor, res_w_scores, res, result[rank[pos]][0], result[rank[pos]][0])\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:,:] doc_rank_probabilities(int[:,:] sampleSet, double[:,:,:] norm_probs, double[:] scores, double[:,:] accumulator, double[:,:] tmp) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef double grad_norm = 0\n",
    "    cdef double _energy = 0\n",
    "    cdef double grad_energy = 0\n",
    "    cdef int r\n",
    "    cdef int d\n",
    "    cdef int pos\n",
    "    cdef int doc\n",
    "    \n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        doc_energy(sampleSet[r], scores, tmp)\n",
    "        for d in range(len(sampleSet[r])):\n",
    "            doc = sampleSet[r][d]\n",
    "            norm_probs[doc][r][0] = exp(-tmp[doc][0]) # e^{-E}\n",
    "            norm_probs[doc][r][1] = -tmp[doc][1] * norm_probs[doc][r][0] # -E' * e^{-E}\n",
    "            accumulator[doc][0] = accumulator[doc][0] + norm_probs[doc][r][0] # sum(e^{-E})\n",
    "            accumulator[doc][1] = accumulator[doc][1] + norm_probs[doc][r][1] # sum(-E' * e^{-E})\n",
    "        \n",
    "        #_energy, grad_energy = doc_energy(sampleSet[r], scores, pos) # E, E'\n",
    "        #norm_probs[r][0] = exp(-_energy) # e^{-E}\n",
    "        #norm_probs[r][1] = -grad_energy * norm_probs[r][0] # -E' * e^{-E}\n",
    "        #norm += norm_probs[r][0] # sum(e^{-E})\n",
    "        #grad_norm += norm_probs[r][1] # sum(-E' * e^{-E})\n",
    "\n",
    "    for pos in prange(len(sampleSet[0]), schedule='static', num_threads=8):\n",
    "        doc = sampleSet[0][pos]\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "        norm_probs[doc][r][1] = ((norm_probs[doc][r][1] * accumulator[doc][0]) - (norm_probs[doc][r][0] * accumulator[doc][1])) / (accumulator[doc][0]**2)\n",
    "        norm_probs[doc][r][0] = norm_probs[doc][r][0] / accumulator[doc][0] # e^{-E} / sum(e^{-E})\n",
    "        \n",
    "    #for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "    #    norm_probs[r][0] = norm_probs[r][0] / norm # e^{-E} / sum(e^{-E})\n",
    "        # -E' * e^{-E} * sum(e^{-E}) - e^{-E} * sum(-E' * e^{-E})\n",
    "        # _______________________________________________________\n",
    "        # (sum(e^{-E}))^2\n",
    "    #    norm_probs[r][1] = ((norm_probs[r][1] * norm) - (norm_probs[r][0] * grad_norm)) / (norm**2) \n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_cross_entropy(int[:,:] perms, double[:] probs, double[:,:,:] scores_probs, double[:,:] entropies) nogil:\n",
    "    cdef int i, doc, j\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            entropies[doc][0] = entropies[doc][0] + -probs[j] * log(scores_probs[doc][j][0])\n",
    "            entropies[doc][1] = entropies[doc][1] + -1 * (probs[j] / scores_probs[doc][j][0]) * scores_probs[doc][j][1]\n",
    "    return entropies\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double[:,:] grad_monte_carlo_gain(int[:,:] perms, double[:,:,:] scores_probs, double[:] scores, double[:] ndcgs, double ideal, double[:,:] gains) nogil:\n",
    "    cdef int i, doc, j\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms[0]), schedule='static', num_threads=8):\n",
    "        doc = perms[0][i]\n",
    "        for j in range(len(perms)):\n",
    "            gains[doc][0] = gains[doc][0] + scores_probs[doc][j][0] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "            gains[doc][1] = gains[doc][1] + scores_probs[doc][j][1] * ndcgs[j]#ndcg_k(perms[j], scores, k, ideal)#ndcgs[j]\n",
    "    return gains \n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank_grads(queries, preds): \n",
    "    cdef double lam = .9\n",
    "    cdef double[:] gain = np.ones_like(preds)\n",
    "    cdef double[:] hess = np.ones_like(preds) \n",
    "    \n",
    "    cdef int d, i\n",
    "    cdef double[:,:,:] score_probs\n",
    "    cdef double[:,:] accumulator = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] tmp = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] gains = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    cdef double[:,:] entropies = np.zeros((len(preds), 2), dtype=np.double) \n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros((len(preds), len(q.probs), 2), dtype=np.double)\n",
    "        score_probs = doc_rank_probabilities(q.perms, score_probs, preds, accumulator, tmp)\n",
    "        gains = grad_monte_carlo_gain(q.perms, score_probs, preds, q.ndcgs, q.idealdcg, gains) \n",
    "        entropies = grad_cross_entropy(q.perms, q.probs, score_probs, entropies)\n",
    "    for i in range(len(gain)):\n",
    "        gain[i] = (lam * gains[i][1]) - ((1-lam) * entropies[i][1])\n",
    "    return gain, hess\n",
    "\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "#                                            METRIC EVALUATION\n",
    "###############################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double energy(int[:] rank, double[:] scores) nogil:\n",
    "    cdef int m = len(rank)\n",
    "    if m == 1 or m == 0:\n",
    "        return 0\n",
    "    cdef double factor = 4 / (m * ((m - 1)**2))\n",
    "    cdef double res = 0\n",
    "    cdef int j, k\n",
    "    for k in prange(m, schedule='static', num_threads=8):\n",
    "        for j in range(k + 1, m):\n",
    "            res += (j - k) * (scores[rank[j]] - scores[rank[k]])\n",
    "    return factor * res\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] rank_probabilities(int[:,:] sampleSet, double[:] norm_probs, double[:] scores) nogil:\n",
    "    cdef double norm = 0\n",
    "    cdef int r\n",
    "    for r in prange(len(sampleSet), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = exp(-energy(sampleSet[r], scores))\n",
    "        norm += norm_probs[r]\n",
    "    for r in prange(len(norm_probs), schedule='static', num_threads=8):\n",
    "        norm_probs[r] = norm_probs[r] / norm\n",
    "    return norm_probs\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double cross_entropy(double[:] probs, double[:] scores_probs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    for i in prange(len(probs), schedule='static', num_threads=8):\n",
    "        result += probs[i] * log(scores_probs[i])\n",
    "    return -result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cdef double monte_carlo_gain(int[:,:] perms, double[:] scores_probs, double[:] ndcgs) nogil:\n",
    "    cdef double result = 0\n",
    "    cdef int i\n",
    "    cdef int k = min(10, len(perms[0]))\n",
    "    for i in prange(len(perms), schedule='static', num_threads=8):\n",
    "        result += scores_probs[i] * ndcgs[i]\n",
    "    return result\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def eval_boltzrank(queries, preds):\n",
    "    cdef double lam = .9\n",
    "    cdef double[:] score_probs\n",
    "    cdef double mc = 0\n",
    "    cdef double ce = 0\n",
    "    cdef double _mc, _ce\n",
    "    for q in queries.values():\n",
    "        score_probs = np.zeros(len(q.probs), dtype=np.double)\n",
    "        score_probs = rank_probabilities(q.perms, score_probs, preds)\n",
    "        mc += monte_carlo_gain(q.perms, score_probs, q.ndcgs) \n",
    "        ce += cross_entropy(q.probs, score_probs)\n",
    "    return (lam * mc) - ((1-lam) * ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/train.txt\n",
      "validation file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/vali.txt\n",
      "test file: C:/opt/kiis-training/MSLR-WEB10K/Fold1/test.txt\n",
      "loading datasets... \n",
      "train dataset loading took 59.9375 s\n",
      "validation dataset loading took 19.9375 s\n",
      "test dataset loading took 18.328125 s\n",
      "converting datasets to LightGBM format... \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_file, valid_file, test_file = retrieveFileNames()\n",
    "\n",
    "print(\"training file: \" + train_file)\n",
    "print(\"validation file: \" + valid_file)\n",
    "print(\"test file: \" + test_file)\n",
    "    \n",
    "print(\"loading datasets... \")\n",
    "import time\n",
    "start = time.process_time()\n",
    "train_dataset = loadDataset(train_file)\n",
    "print(\"train dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "valid_dataset = loadDataset(valid_file)\n",
    "print(\"validation dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "start = time.process_time()\n",
    "test_dataset = loadDataset(test_file)\n",
    "print(\"test dataset loading took \" + str(time.process_time() - start) + \" s\")\n",
    "\n",
    "import itertools\n",
    "print(\"converting datasets to LightGBM format... \")\n",
    "train_lgb = loadLightGBM(train_dataset)\n",
    "valid_lgb = loadLightGBM(valid_dataset)\n",
    "test_lgb = loadLightGBM(test_dataset)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating query-documents mappings...\n",
      "done\n",
      "creating sample sets...\n",
      "sample set creation took 52.140625 s\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "print(\"creating query-documents mappings...\")\n",
    "train_id = file_len(train_file)\n",
    "vali_id = file_len(valid_file)\n",
    "test_id = file_len(test_file)\n",
    "ds_to_queries = {}\n",
    "ds_to_queries[train_id] = mapQueryToDocuments(train_dataset)\n",
    "ds_to_queries[vali_id] = mapQueryToDocuments(valid_dataset)\n",
    "ds_to_queries[test_id] = mapQueryToDocuments(test_dataset)\n",
    "#queries, alllabels = mapQueryToDocuments(train_dataset)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"creating sample sets...\")\n",
    "start = time.process_time()\n",
    "for ds_id, queries in ds_to_queries.items():\n",
    "    for q in queries[0].values():\n",
    "        process_query(q, queries[1])\n",
    "#for q in queries.values():\n",
    "#    queries[q.qid] = process_query(q, alllabels)\n",
    "print(\"sample set creation took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lightgbm...\n",
      "MSE grads: 0.5592629096559084\n",
      "MSE eval: 0.5498625922419258\n",
      "MSE eval: 0.5645173609984029\n",
      "MSE eval: 0.5674167956854171\n",
      "[1]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.549863\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.564517\ttest's ndcg@10: 0.235728\ttest's MSE: 0.567417\n",
      "MSE grads: 0.5498625922419258\n",
      "MSE eval: 0.5406808407882051\n",
      "MSE eval: 0.5550321003943863\n",
      "MSE eval: 0.558095407095237\n",
      "[2]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.540681\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.555032\ttest's ndcg@10: 0.235728\ttest's MSE: 0.558095\n",
      "MSE grads: 0.5406808407882051\n",
      "MSE eval: 0.5317176528568477\n",
      "MSE eval: 0.5457709174603376\n",
      "MSE eval: 0.5489918727943256\n",
      "[3]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.531718\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.545771\ttest's ndcg@10: 0.235728\ttest's MSE: 0.548992\n",
      "MSE grads: 0.5317176528568477\n",
      "MSE eval: 0.5229730260835924\n",
      "MSE eval: 0.5367338082274403\n",
      "MSE eval: 0.5401061903345741\n",
      "[4]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.522973\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.536734\ttest's ndcg@10: 0.235728\ttest's MSE: 0.540106\n",
      "MSE grads: 0.5229730260835924\n",
      "MSE eval: 0.5144469581787409\n",
      "MSE eval: 0.5279207688218234\n",
      "MSE eval: 0.5314383573455352\n",
      "[5]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.514447\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.527921\ttest's ndcg@10: 0.235728\ttest's MSE: 0.531438\n",
      "MSE grads: 0.5144469581787409\n",
      "MSE eval: 0.506139446924618\n",
      "MSE eval: 0.5193317954611204\n",
      "MSE eval: 0.5229883715316479\n",
      "[6]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.506139\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.519332\ttest's ndcg@10: 0.235728\ttest's MSE: 0.522988\n",
      "MSE grads: 0.506139446924618\n",
      "MSE eval: 0.4980504901739498\n",
      "MSE eval: 0.5109668844527318\n",
      "MSE eval: 0.5147562306705933\n",
      "[7]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.49805\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.510967\ttest's ndcg@10: 0.235728\ttest's MSE: 0.514756\n",
      "MSE grads: 0.4980504901739498\n",
      "MSE eval: 0.4901800858496082\n",
      "MSE eval: 0.5028260321928351\n",
      "MSE eval: 0.5067419326128175\n",
      "[8]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.49018\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.502826\ttest's ndcg@10: 0.235728\ttest's MSE: 0.506742\n",
      "MSE grads: 0.4901800858496082\n",
      "MSE eval: 0.4825282319438368\n",
      "MSE eval: 0.4949092351650472\n",
      "MSE eval: 0.4989454752809039\n",
      "[9]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.482528\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.494909\ttest's ndcg@10: 0.235728\ttest's MSE: 0.498945\n",
      "MSE grads: 0.4825282319438368\n",
      "MSE eval: 0.4750949265141614\n",
      "MSE eval: 0.48721648993571504\n",
      "MSE eval: 0.4913668566651559\n",
      "[10]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.475095\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.487216\ttest's ndcg@10: 0.235728\ttest's MSE: 0.491367\n",
      "MSE grads: 0.4750949265141614\n",
      "MSE eval: 0.4678801676858284\n",
      "MSE eval: 0.47974779315639937\n",
      "MSE eval: 0.4840060748262164\n",
      "[11]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.46788\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.479748\ttest's ndcg@10: 0.235728\ttest's MSE: 0.484006\n",
      "MSE grads: 0.4678801676858284\n",
      "MSE eval: 0.4608839536490188\n",
      "MSE eval: 0.47250314155989664\n",
      "MSE eval: 0.47686312789176133\n",
      "[12]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.460884\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.472503\ttest's ndcg@10: 0.235728\ttest's MSE: 0.476863\n",
      "MSE grads: 0.4608839536490188\n",
      "MSE eval: 0.454106282656999\n",
      "MSE eval: 0.4654825319583812\n",
      "MSE eval: 0.4699380140551442\n",
      "[13]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.454106\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.465483\ttest's ndcg@10: 0.235728\ttest's MSE: 0.469938\n",
      "MSE grads: 0.454106282656999\n",
      "MSE eval: 0.4475471530262009\n",
      "MSE eval: 0.45868596124283856\n",
      "MSE eval: 0.4632307315749573\n",
      "[14]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.447547\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.458686\ttest's ndcg@10: 0.235728\ttest's MSE: 0.463231\n",
      "MSE grads: 0.4475471530262009\n",
      "MSE eval: 0.4412065631365389\n",
      "MSE eval: 0.4521134263829786\n",
      "MSE eval: 0.4567412787753847\n",
      "[15]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.441207\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.452113\ttest's ndcg@10: 0.235728\ttest's MSE: 0.456741\n",
      "MSE grads: 0.4412065631365389\n",
      "MSE eval: 0.4350845114276958\n",
      "MSE eval: 0.44576492442314647\n",
      "MSE eval: 0.45046965404256684\n",
      "[16]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.435085\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.445765\ttest's ndcg@10: 0.235728\ttest's MSE: 0.45047\n",
      "MSE grads: 0.4350845114276958\n",
      "MSE eval: 0.42918099639963037\n",
      "MSE eval: 0.4396404524822227\n",
      "MSE eval: 0.4444158558248304\n",
      "[17]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.429181\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.43964\ttest's ndcg@10: 0.235728\ttest's MSE: 0.444416\n",
      "MSE grads: 0.42918099639963037\n",
      "MSE eval: 0.4234960166122785\n",
      "MSE eval: 0.4337400077529845\n",
      "MSE eval: 0.4385798826323495\n",
      "[18]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.423496\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.43374\ttest's ndcg@10: 0.235728\ttest's MSE: 0.43858\n",
      "MSE grads: 0.4234960166122785\n",
      "MSE eval: 0.4180295706841534\n",
      "MSE eval: 0.4280635875003981\n",
      "MSE eval: 0.43296173303582614\n",
      "[19]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.41803\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.428064\ttest's ndcg@10: 0.235728\ttest's MSE: 0.432962\n",
      "MSE grads: 0.4180295706841534\n",
      "MSE eval: 0.4127816572910191\n",
      "MSE eval: 0.42261118905978\n",
      "MSE eval: 0.4275614056649428\n",
      "[20]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.412782\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.422611\ttest's ndcg@10: 0.235728\ttest's MSE: 0.427561\n",
      "MSE grads: 0.4127816572910191\n",
      "MSE eval: 0.4077522751666452\n",
      "MSE eval: 0.41738280983708725\n",
      "MSE eval: 0.42237889920903143\n",
      "[21]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.407752\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.417383\ttest's ndcg@10: 0.235728\ttest's MSE: 0.422379\n",
      "MSE grads: 0.4077522751666452\n",
      "MSE eval: 0.4029414231006316\n",
      "MSE eval: 0.4123784473064598\n",
      "MSE eval: 0.41741421241497545\n",
      "[22]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.402941\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.412378\ttest's ndcg@10: 0.235728\ttest's MSE: 0.417414\n",
      "MSE grads: 0.4029414231006316\n",
      "MSE eval: 0.3983490999383778\n",
      "MSE eval: 0.4075980990097\n",
      "MSE eval: 0.41266734408694977\n",
      "[23]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.398349\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.407598\ttest's ndcg@10: 0.235728\ttest's MSE: 0.412667\n",
      "MSE grads: 0.3983490999383778\n",
      "MSE eval: 0.39397530458045465\n",
      "MSE eval: 0.40304176255533924\n",
      "MSE eval: 0.4081382930858975\n",
      "[24]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.393975\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.403042\ttest's ndcg@10: 0.235728\ttest's MSE: 0.408138\n",
      "MSE grads: 0.39397530458045465\n",
      "MSE eval: 0.3898200359822816\n",
      "MSE eval: 0.39870943561790423\n",
      "MSE eval: 0.40382705832890875\n",
      "[25]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.38982\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.398709\ttest's ndcg@10: 0.235728\ttest's MSE: 0.403827\n",
      "MSE grads: 0.3898200359822816\n",
      "MSE eval: 0.3858832931531726\n",
      "MSE eval: 0.39460111593667724\n",
      "MSE eval: 0.3997336387884026\n",
      "[26]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.385883\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.394601\ttest's ndcg@10: 0.235728\ttest's MSE: 0.399734\n",
      "MSE grads: 0.3858832931531726\n",
      "MSE eval: 0.3821650751555616\n",
      "MSE eval: 0.3907168013145414\n",
      "MSE eval: 0.39585803349126863\n",
      "[27]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.382165\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.390717\ttest's ndcg@10: 0.235728\ttest's MSE: 0.395858\n",
      "MSE grads: 0.3821650751555616\n",
      "MSE eval: 0.3786653811052746\n",
      "MSE eval: 0.387056489617749\n",
      "MSE eval: 0.39220024151900784\n",
      "[28]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.378665\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.387056\ttest's ndcg@10: 0.235728\ttest's MSE: 0.3922\n",
      "MSE grads: 0.3786653811052746\n",
      "MSE eval: 0.3753842101710812\n",
      "MSE eval: 0.38362017877531884\n",
      "MSE eval: 0.3887602620070687\n",
      "[29]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.375384\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.38362\ttest's ndcg@10: 0.235728\ttest's MSE: 0.38876\n",
      "MSE grads: 0.3753842101710812\n",
      "MSE eval: 0.3723215615730914\n",
      "MSE eval: 0.3804078667770003\n",
      "MSE eval: 0.38553809414361206\n",
      "[30]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.372322\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.380408\ttest's ndcg@10: 0.235728\ttest's MSE: 0.385538\n",
      "MSE grads: 0.3723215615730914\n",
      "MSE eval: 0.36947743458388377\n",
      "MSE eval: 0.3774195516741085\n",
      "MSE eval: 0.3825337371701789\n",
      "[31]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.369477\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.37742\ttest's ndcg@10: 0.235728\ttest's MSE: 0.382534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 0.36947743458388377\n",
      "MSE eval: 0.36685182852708964\n",
      "MSE eval: 0.3746552315777363\n",
      "MSE eval: 0.37974719038038024\n",
      "[32]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.366852\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.374655\ttest's ndcg@10: 0.235728\ttest's MSE: 0.379747\n",
      "MSE grads: 0.36685182852708964\n",
      "MSE eval: 0.3644447427772423\n",
      "MSE eval: 0.3721149046583778\n",
      "MSE eval: 0.377178453119728\n",
      "[33]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.364445\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.372115\ttest's ndcg@10: 0.235728\ttest's MSE: 0.377178\n",
      "MSE grads: 0.3644447427772423\n",
      "MSE eval: 0.36225617676055416\n",
      "MSE eval: 0.36979856914621456\n",
      "MSE eval: 0.3748275247861709\n",
      "[34]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.362256\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.369799\ttest's ndcg@10: 0.235728\ttest's MSE: 0.374828\n",
      "MSE grads: 0.36225617676055416\n",
      "MSE eval: 0.3602861299530924\n",
      "MSE eval: 0.36770622332926284\n",
      "MSE eval: 0.3726944048284212\n",
      "[35]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.360286\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.367706\ttest's ndcg@10: 0.235728\ttest's MSE: 0.372694\n",
      "MSE grads: 0.3602861299530924\n",
      "MSE eval: 0.3585346018817391\n",
      "MSE eval: 0.36583786555374054\n",
      "MSE eval: 0.3707790927467229\n",
      "[36]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.358535\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.365838\ttest's ndcg@10: 0.235728\ttest's MSE: 0.370779\n",
      "MSE grads: 0.3585346018817391\n",
      "MSE eval: 0.357001592122921\n",
      "MSE eval: 0.36419349422265335\n",
      "MSE eval: 0.3690815880916416\n",
      "[37]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.357002\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.364193\ttest's ndcg@10: 0.235728\ttest's MSE: 0.369082\n",
      "MSE grads: 0.357001592122921\n",
      "MSE eval: 0.35568710030395595\n",
      "MSE eval: 0.3627731077968708\n",
      "MSE eval: 0.36760189046528485\n",
      "[38]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.355687\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.362773\ttest's ndcg@10: 0.235728\ttest's MSE: 0.367602\n",
      "MSE grads: 0.35568710030395595\n",
      "MSE eval: 0.3545911261017005\n",
      "MSE eval: 0.3615767047933959\n",
      "MSE eval: 0.36633999951981006\n",
      "[39]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.354591\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.361577\ttest's ndcg@10: 0.235728\ttest's MSE: 0.36634\n",
      "MSE grads: 0.3545911261017005\n",
      "MSE eval: 0.3537136692426298\n",
      "MSE eval: 0.3606042837852619\n",
      "MSE eval: 0.36529591495766073\n",
      "[40]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.353714\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.360604\ttest's ndcg@10: 0.235728\ttest's MSE: 0.365296\n",
      "MSE grads: 0.3537136692426298\n",
      "MSE eval: 0.3530547295035316\n",
      "MSE eval: 0.35985584340183807\n",
      "MSE eval: 0.36446963653189923\n",
      "[41]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.353055\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.359856\ttest's ndcg@10: 0.235728\ttest's MSE: 0.36447\n",
      "MSE grads: 0.3530547295035316\n",
      "MSE eval: 0.3526143067106111\n",
      "MSE eval: 0.35933138232777384\n",
      "MSE eval: 0.3638611640456875\n",
      "[42]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.352614\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.359331\ttest's ndcg@10: 0.235728\ttest's MSE: 0.363861\n",
      "MSE grads: 0.3526143067106111\n",
      "MSE eval: 0.35239240073995465\n",
      "MSE eval: 0.35903089930318194\n",
      "MSE eval: 0.3634704973522978\n",
      "[43]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.352392\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.359031\ttest's ndcg@10: 0.235728\ttest's MSE: 0.36347\n",
      "MSE grads: 0.35239240073995465\n",
      "MSE eval: 0.3523890115175455\n",
      "MSE eval: 0.35895439312326716\n",
      "MSE eval: 0.36329763635527357\n",
      "[44]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.352389\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.358954\ttest's ndcg@10: 0.235728\ttest's MSE: 0.363298\n",
      "MSE grads: 0.3523890115175455\n",
      "MSE eval: 0.35260413901944754\n",
      "MSE eval: 0.35910186263845423\n",
      "MSE eval: 0.3633425810085261\n",
      "[45]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.352604\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.359102\ttest's ndcg@10: 0.235728\ttest's MSE: 0.363343\n",
      "MSE grads: 0.35260413901944754\n",
      "MSE eval: 0.3530377832712911\n",
      "MSE eval: 0.35947330675347994\n",
      "MSE eval: 0.3636053313157775\n",
      "[46]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.353038\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.359473\ttest's ndcg@10: 0.235728\ttest's MSE: 0.363605\n",
      "MSE grads: 0.3530377832712911\n",
      "MSE eval: 0.3536899443499519\n",
      "MSE eval: 0.3600687244288494\n",
      "MSE eval: 0.3640858873320523\n",
      "[47]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.35369\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.360069\ttest's ndcg@10: 0.235728\ttest's MSE: 0.364086\n",
      "MSE grads: 0.3536899443499519\n",
      "MSE eval: 0.3545606223812998\n",
      "MSE eval: 0.3608881146783614\n",
      "MSE eval: 0.36478424916174057\n",
      "[48]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.354561\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.360888\ttest's ndcg@10: 0.235728\ttest's MSE: 0.364784\n",
      "MSE grads: 0.3545606223812998\n",
      "MSE eval: 0.35564981754282177\n",
      "MSE eval: 0.36193147657155145\n",
      "MSE eval: 0.3657004169606719\n",
      "[49]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.35565\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.361931\ttest's ndcg@10: 0.235728\ttest's MSE: 0.3657\n",
      "MSE grads: 0.35564981754282177\n",
      "MSE eval: 0.35695753006209274\n",
      "MSE eval: 0.3631988092318047\n",
      "MSE eval: 0.36683439093504533\n",
      "[50]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.356958\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.363199\ttest's ndcg@10: 0.235728\ttest's MSE: 0.366834\n",
      "MSE grads: 0.35695753006209274\n",
      "MSE eval: 0.3584837602186264\n",
      "MSE eval: 0.36469011183811384\n",
      "MSE eval: 0.368186171342952\n",
      "[51]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.358484\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.36469\ttest's ndcg@10: 0.235728\ttest's MSE: 0.368186\n",
      "MSE grads: 0.3584837602186264\n",
      "MSE eval: 0.3602285083428346\n",
      "MSE eval: 0.3664053836235815\n",
      "MSE eval: 0.36975575849307907\n",
      "[52]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.360229\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.366405\ttest's ndcg@10: 0.235728\ttest's MSE: 0.369756\n",
      "MSE grads: 0.3602285083428346\n",
      "MSE eval: 0.3621917748172862\n",
      "MSE eval: 0.3683446238767956\n",
      "MSE eval: 0.37154315274638283\n",
      "[53]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.362192\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.368345\ttest's ndcg@10: 0.235728\ttest's MSE: 0.371543\n",
      "MSE grads: 0.3621917748172862\n",
      "MSE eval: 0.36437356007632793\n",
      "MSE eval: 0.37050783194094183\n",
      "MSE eval: 0.3735483545154792\n",
      "[54]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.364374\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.370508\ttest's ndcg@10: 0.235728\ttest's MSE: 0.373548\n",
      "MSE grads: 0.36437356007632793\n",
      "MSE eval: 0.3667738646075577\n",
      "MSE eval: 0.3728950072152388\n",
      "MSE eval: 0.37577136426590263\n",
      "[55]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.366774\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.372895\ttest's ndcg@10: 0.235728\ttest's MSE: 0.375771\n",
      "MSE grads: 0.3667738646075577\n",
      "MSE eval: 0.3693926889505309\n",
      "MSE eval: 0.3755061491532533\n",
      "MSE eval: 0.37821218251496347\n",
      "[56]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.369393\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.375506\ttest's ndcg@10: 0.235728\ttest's MSE: 0.378212\n",
      "MSE grads: 0.3693926889505309\n",
      "MSE eval: 0.37223003369925006\n",
      "MSE eval: 0.3783412572653552\n",
      "MSE eval: 0.38087080983404775\n",
      "[57]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.37223\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.378341\ttest's ndcg@10: 0.235728\ttest's MSE: 0.380871\n",
      "MSE grads: 0.37223003369925006\n",
      "MSE eval: 0.37528589950198166\n",
      "MSE eval: 0.3814003311181872\n",
      "MSE eval: 0.38374724684835765\n",
      "[58]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.375286\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.3814\ttest's ndcg@10: 0.235728\ttest's MSE: 0.383747\n",
      "MSE grads: 0.37528589950198166\n",
      "MSE eval: 0.3785602870605403\n",
      "MSE eval: 0.3846833703339258\n",
      "MSE eval: 0.38684149423659964\n",
      "[59]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.37856\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.384683\ttest's ndcg@10: 0.235728\ttest's MSE: 0.386841\n",
      "MSE grads: 0.3785602870605403\n",
      "MSE eval: 0.38205319713419833\n",
      "MSE eval: 0.3881903745937791\n",
      "MSE eval: 0.39015355273402874\n",
      "[60]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.382053\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.38819\ttest's ndcg@10: 0.235728\ttest's MSE: 0.390154\n",
      "MSE grads: 0.38205319713419833\n",
      "MSE eval: 0.38576463053565124\n",
      "MSE eval: 0.39192134363388575\n",
      "MSE eval: 0.39368342312926063\n",
      "[61]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.385765\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.391921\ttest's ndcg@10: 0.235728\ttest's MSE: 0.393683\n",
      "MSE grads: 0.38576463053565124\n",
      "MSE eval: 0.3896945881347695\n",
      "MSE eval: 0.3958762772487744\n",
      "MSE eval: 0.39743110626734945\n",
      "[62]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.389695\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.395876\ttest's ndcg@10: 0.235728\ttest's MSE: 0.397431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 0.3896945881347695\n",
      "MSE eval: 0.3938430708601827\n",
      "MSE eval: 0.40005517529302836\n",
      "MSE eval: 0.4013966030515533\n",
      "[63]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.393843\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.400055\ttest's ndcg@10: 0.235728\ttest's MSE: 0.401397\n",
      "MSE grads: 0.3938430708601827\n",
      "MSE eval: 0.3982100796967757\n",
      "MSE eval: 0.4044580376780571\n",
      "MSE eval: 0.405579914440827\n",
      "[64]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.39821\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.404458\ttest's ndcg@10: 0.235728\ttest's MSE: 0.40558\n",
      "MSE grads: 0.3982100796967757\n",
      "MSE eval: 0.4027956156894679\n",
      "MSE eval: 0.4090848643761489\n",
      "MSE eval: 0.4099810414530694\n",
      "[65]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.402796\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.409085\ttest's ndcg@10: 0.235728\ttest's MSE: 0.409981\n",
      "MSE grads: 0.4027956156894679\n",
      "MSE eval: 0.4075996799420967\n",
      "MSE eval: 0.41393565541894994\n",
      "MSE eval: 0.41459998516489327\n",
      "[66]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.4076\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.413936\ttest's ndcg@10: 0.235728\ttest's MSE: 0.4146\n",
      "MSE grads: 0.4075996799420967\n",
      "MSE eval: 0.41262227361977577\n",
      "MSE eval: 0.4190104108997949\n",
      "MSE eval: 0.4194367467133497\n",
      "[67]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.412622\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.41901\ttest's ndcg@10: 0.235728\ttest's MSE: 0.419437\n",
      "MSE grads: 0.41262227361977577\n",
      "MSE eval: 0.41786339794804295\n",
      "MSE eval: 0.4243091309725157\n",
      "MSE eval: 0.4244913272948725\n",
      "[68]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.417863\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.424309\ttest's ndcg@10: 0.235728\ttest's MSE: 0.424491\n",
      "MSE grads: 0.41786339794804295\n",
      "MSE eval: 0.42332305421541483\n",
      "MSE eval: 0.42983181585386393\n",
      "MSE eval: 0.4297637281681112\n",
      "[69]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.423323\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.429832\ttest's ndcg@10: 0.235728\ttest's MSE: 0.429764\n",
      "MSE grads: 0.42332305421541483\n",
      "MSE eval: 0.42900124377398086\n",
      "MSE eval: 0.43557846582378634\n",
      "MSE eval: 0.4352539506543261\n",
      "[70]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.429001\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.435578\ttest's ndcg@10: 0.235728\ttest's MSE: 0.435254\n",
      "MSE grads: 0.42900124377398086\n",
      "MSE eval: 0.43489796804006614\n",
      "MSE eval: 0.441549081226271\n",
      "MSE eval: 0.4409619961382373\n",
      "[71]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.434898\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.441549\ttest's ndcg@10: 0.235728\ttest's MSE: 0.440962\n",
      "MSE grads: 0.43489796804006614\n",
      "MSE eval: 0.44101322849614594\n",
      "MSE eval: 0.44774366247069375\n",
      "MSE eval: 0.4468878660692024\n",
      "[72]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.441013\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.447744\ttest's ndcg@10: 0.235728\ttest's MSE: 0.446888\n",
      "MSE grads: 0.44101322849614594\n",
      "MSE eval: 0.4473470266891456\n",
      "MSE eval: 0.45416221003000934\n",
      "MSE eval: 0.4530315619605689\n",
      "[73]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.447347\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.454162\ttest's ndcg@10: 0.235728\ttest's MSE: 0.453032\n",
      "MSE grads: 0.4473470266891456\n",
      "MSE eval: 0.4538993642371971\n",
      "MSE eval: 0.46080472444759035\n",
      "MSE eval: 0.45939308539527796\n",
      "[74]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.453899\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.460805\ttest's ndcg@10: 0.235728\ttest's MSE: 0.459393\n",
      "MSE grads: 0.4538993642371971\n",
      "MSE eval: 0.4606702428250625\n",
      "MSE eval: 0.46767120633195314\n",
      "MSE eval: 0.4659724380218908\n",
      "[75]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.46067\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.467671\ttest's ndcg@10: 0.235728\ttest's MSE: 0.465972\n",
      "MSE grads: 0.4606702428250625\n",
      "MSE eval: 0.46765966420794053\n",
      "MSE eval: 0.4747616563610934\n",
      "MSE eval: 0.4727696215586071\n",
      "[76]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.46766\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.474762\ttest's ndcg@10: 0.235728\ttest's MSE: 0.47277\n",
      "MSE grads: 0.46765966420794053\n",
      "MSE eval: 0.47486763021302203\n",
      "MSE eval: 0.48207607528306645\n",
      "MSE eval: 0.4797846377933978\n",
      "[77]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.474868\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.482076\ttest's ndcg@10: 0.235728\ttest's MSE: 0.479785\n",
      "MSE grads: 0.47486763021302203\n",
      "MSE eval: 0.48229414274084637\n",
      "MSE eval: 0.48961446391790975\n",
      "MSE eval: 0.4870174885873331\n",
      "[78]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.482294\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.489614\ttest's ndcg@10: 0.235728\ttest's MSE: 0.487017\n",
      "MSE grads: 0.48229414274084637\n",
      "MSE eval: 0.48993920376508593\n",
      "MSE eval: 0.4973768231566957\n",
      "MSE eval: 0.4944681758725263\n",
      "[79]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.489939\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.497377\ttest's ndcg@10: 0.235728\ttest's MSE: 0.494468\n",
      "MSE grads: 0.48993920376508593\n",
      "MSE eval: 0.4978028153363895\n",
      "MSE eval: 0.5053631539653637\n",
      "MSE eval: 0.5021367016569018\n",
      "[80]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.497803\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.505363\ttest's ndcg@10: 0.235728\ttest's MSE: 0.502137\n",
      "MSE grads: 0.4978028153363895\n",
      "MSE eval: 0.5058849795825378\n",
      "MSE eval: 0.513573457384778\n",
      "MSE eval: 0.5100230680237088\n",
      "[81]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.505885\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.513573\ttest's ndcg@10: 0.235728\ttest's MSE: 0.510023\n",
      "MSE grads: 0.5058849795825378\n",
      "MSE eval: 0.5141856987097466\n",
      "MSE eval: 0.5220077345315979\n",
      "MSE eval: 0.5181272771332616\n",
      "[82]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.514186\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.522008\ttest's ndcg@10: 0.235728\ttest's MSE: 0.518127\n",
      "MSE grads: 0.5141856987097466\n",
      "MSE eval: 0.5227049750034608\n",
      "MSE eval: 0.5306659865992691\n",
      "MSE eval: 0.5264493312239655\n",
      "[83]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.522705\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.530666\ttest's ndcg@10: 0.235728\ttest's MSE: 0.526449\n",
      "MSE grads: 0.5227049750034608\n",
      "MSE eval: 0.5314428108340067\n",
      "MSE eval: 0.5395482148632657\n",
      "MSE eval: 0.5349892326167279\n",
      "[84]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.531443\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.539548\ttest's ndcg@10: 0.235728\ttest's MSE: 0.534989\n",
      "MSE grads: 0.5314428108340067\n",
      "MSE eval: 0.5403992086534015\n",
      "MSE eval: 0.5486544206775485\n",
      "MSE eval: 0.5437469837130187\n",
      "[85]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.540399\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.548654\ttest's ndcg@10: 0.235728\ttest's MSE: 0.543747\n",
      "MSE grads: 0.5403992086534015\n",
      "MSE eval: 0.5495741710002684\n",
      "MSE eval: 0.5579846054795916\n",
      "MSE eval: 0.5527225869985997\n",
      "[86]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.549574\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.557985\ttest's ndcg@10: 0.235728\ttest's MSE: 0.552723\n",
      "MSE grads: 0.5495741710002684\n",
      "MSE eval: 0.5589677005011903\n",
      "MSE eval: 0.5675387707912158\n",
      "MSE eval: 0.5619160450455871\n",
      "[87]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.558968\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.567539\ttest's ndcg@10: 0.235728\ttest's MSE: 0.561916\n",
      "MSE grads: 0.5589677005011903\n",
      "MSE eval: 0.5685797998691582\n",
      "MSE eval: 0.5773169182174216\n",
      "MSE eval: 0.571327360510849\n",
      "[88]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.56858\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.577317\ttest's ndcg@10: 0.235728\ttest's MSE: 0.571327\n",
      "MSE grads: 0.5685797998691582\n",
      "MSE eval: 0.5784104719115823\n",
      "MSE eval: 0.5873190494532938\n",
      "MSE eval: 0.5809565361432072\n",
      "[89]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.57841\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.587319\ttest's ndcg@10: 0.235728\ttest's MSE: 0.580957\n",
      "MSE grads: 0.5784104719115823\n",
      "MSE eval: 0.5884597195265939\n",
      "MSE eval: 0.59754516628112\n",
      "MSE eval: 0.5908035747805638\n",
      "[90]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.58846\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.597545\ttest's ndcg@10: 0.235728\ttest's MSE: 0.590804\n",
      "MSE grads: 0.5884597195265939\n",
      "MSE eval: 0.598727545710967\n",
      "MSE eval: 0.6079952705769885\n",
      "MSE eval: 0.6008684793571757\n",
      "[91]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.598728\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.607995\ttest's ndcg@10: 0.235728\ttest's MSE: 0.600868\n",
      "MSE grads: 0.598727545710967\n",
      "MSE eval: 0.6092139535535278\n",
      "MSE eval: 0.6186693643052896\n",
      "MSE eval: 0.6111512528976821\n",
      "[92]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.609214\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.618669\ttest's ndcg@10: 0.235728\ttest's MSE: 0.611151\n",
      "MSE grads: 0.6092139535535278\n",
      "MSE eval: 0.6199189462480036\n",
      "MSE eval: 0.6295674495302259\n",
      "MSE eval: 0.6216518985286602\n",
      "[93]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.619919\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.629567\ttest's ndcg@10: 0.235728\ttest's MSE: 0.621652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE grads: 0.6199189462480036\n",
      "MSE eval: 0.6308425270867428\n",
      "MSE eval: 0.6406895284097712\n",
      "MSE eval: 0.6323704194736022\n",
      "[94]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.630843\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.64069\ttest's ndcg@10: 0.235728\ttest's MSE: 0.63237\n",
      "MSE grads: 0.6308425270867428\n",
      "MSE eval: 0.6419846994671708\n",
      "MSE eval: 0.6520356032021243\n",
      "MSE eval: 0.6433068190581691\n",
      "[95]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.641985\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.652036\ttest's ndcg@10: 0.235728\ttest's MSE: 0.643307\n",
      "MSE grads: 0.6419846994671708\n",
      "MSE eval: 0.6533454668950667\n",
      "MSE eval: 0.6636056762684818\n",
      "MSE eval: 0.6544611007142298\n",
      "[96]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.653345\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.663606\ttest's ndcg@10: 0.235728\ttest's MSE: 0.654461\n",
      "MSE grads: 0.6533454668950667\n",
      "MSE eval: 0.6649248329819551\n",
      "MSE eval: 0.6753997500700589\n",
      "MSE eval: 0.6658332679771384\n",
      "[97]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.664925\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.6754\ttest's ndcg@10: 0.235728\ttest's MSE: 0.665833\n",
      "MSE grads: 0.6649248329819551\n",
      "MSE eval: 0.6767228014533393\n",
      "MSE eval: 0.6874178271764934\n",
      "MSE eval: 0.6774233244932966\n",
      "[98]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.676723\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.687418\ttest's ndcg@10: 0.235728\ttest's MSE: 0.677423\n",
      "MSE grads: 0.6767228014533393\n",
      "MSE eval: 0.6887393761467566\n",
      "MSE eval: 0.6996599102634583\n",
      "MSE eval: 0.6892312740185251\n",
      "[99]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.688739\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.69966\ttest's ndcg@10: 0.235728\ttest's MSE: 0.689231\n",
      "MSE grads: 0.6887393761467566\n",
      "MSE eval: 0.7009745610186305\n",
      "MSE eval: 0.712126002119367\n",
      "MSE eval: 0.7012571204250824\n",
      "[100]\ttrain's ndcg@10: 0.237615\ttrain's MSE: 0.700975\tvalid's ndcg@10: 0.240808\tvalid's MSE: 0.712126\ttest's ndcg@10: 0.235728\ttest's MSE: 0.701257\n",
      "training took 9349.546875 s\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Error')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluation(preds, train_data):\n",
    "    global ds_to_queries\n",
    "    #bz = eval_boltzrank(ds_to_queries[len(preds)][0], preds)\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE eval: \" + str(avg_mse))\n",
    "    return 'MSE', avg_mse, False\n",
    "\n",
    "def compute_grads(preds, train_data): \n",
    "    global ds_to_queries\n",
    "    global train_id\n",
    "    gain, hess = eval_boltzrank_grads(ds_to_queries[train_id][0], preds)\n",
    "    gain = np.asarray(gain)\n",
    "    hess = np.asarray(hess)\n",
    "    #print(\"min \" + str(np.min(gain)) + \" max \" + str(np.max(gain)) + \" mean \" + str(np.mean(gain)) + \" std \" + str(np.std(gain)))\n",
    "    #print(\"preds \" + str(preds))\n",
    "    #print(\"gain \" + str(gain))\n",
    "    #print(\"hess \" + str(hess))\n",
    "    labels = train_data.get_label()\n",
    "    avg_mse = 0.5 * np.mean( (labels-preds)**2 )\n",
    "    print(\"MSE grads: \" + str(avg_mse))\n",
    "    return gain, hess\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 16,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'metric': ['ndcg'],# ['None']\n",
    "    'ndcg_eval_at': 10\n",
    "}    \n",
    "\n",
    "print(\"training lightgbm...\")\n",
    "start = time.process_time()\n",
    "lgbm_info = {}\n",
    "lgbm_model = lightgbm.train(params, train_lgb, num_boost_round=100,\n",
    "                            feval = evaluation,\n",
    "                            fobj  = compute_grads,\n",
    "                            valid_sets   = [train_lgb, valid_lgb, test_lgb], \n",
    "                            valid_names  = [\"train\", \"valid\", \"test\"],\n",
    "                            evals_result = lgbm_info,\n",
    "                            verbose_eval = 1)\n",
    "print(\"training took \" + str(time.process_time() - start) + \" s\")\n",
    "print(\"done\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(lgbm_info['train'][METRIC_NAME], label='training')\n",
    "plt.plot(lgbm_info['valid'][METRIC_NAME], label='validation')\n",
    "plt.plot(lgbm_info['test'][METRIC_NAME], label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"# Trees\")\n",
    "plt.ylabel(METRIC_NAME)\n",
    "plt.title(\"Model Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(q, k):\n",
    "    indexes = set(range(0, len(q.perms)))\n",
    "    indexes.remove(k)\n",
    "    result = []\n",
    "    for i in range(len(q.perms[k])):\n",
    "        tmp = set(indexes)\n",
    "        for j in tmp:\n",
    "            if q.perms[k][i] != q.perms[j][i]:\n",
    "                indexes.remove(j)\n",
    "    for w in indexes:\n",
    "        if w < k:\n",
    "            result.append((w, k))\n",
    "        else: \n",
    "            result.append((k,w))\n",
    "    return result\n",
    "\n",
    "def checkRepetitions():\n",
    "    global queries\n",
    "    same = dict()\n",
    "    for q in queries.values():\n",
    "        for i in range(len(q.perms)):\n",
    "            r = check(q, i)\n",
    "            if len(r) != 0:\n",
    "                if not q.qid in same.keys():\n",
    "                    same[q.qid] = set()\n",
    "                for t in r:\n",
    "                    same[q.qid].add(t)\n",
    "\n",
    "    print(str(len(same.keys())) + \"/\" + str(len(queries.keys())) + \" queries have duplicate permutations\")\n",
    "    for q, s in same.items():\n",
    "        print(\"query \" + str(q) + \" has repeated permutations: \" + str(s))\n",
    "        \n",
    "#checkRepetitions()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4dcdedcd6052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for query in queries.values():\n",
    "    for prob in query.probs:\n",
    "        if not prob in freq.keys():\n",
    "            freq[prob] = 0\n",
    "        freq[prob] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for prob, f in sorted(freq.items()):\n",
    "    x.append(prob)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"probability\")\n",
    "plt.ylabel(\"# rank\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"probabilities of the \" + str(totperms) + \" permutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "totperms = 0\n",
    "for query in queries.values():\n",
    "    for ndcg in query.ndcgs:\n",
    "        totperms += 1\n",
    "        if not ndcg in freq.keys():\n",
    "            freq[ndcg] = 0\n",
    "        freq[ndcg] += 1\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for ndcg, f in sorted(freq.items()):\n",
    "    x.append(ndcg)\n",
    "    y.append(f)\n",
    "    \n",
    "plt.figure(figsize=(9,6), tight_layout=True)\n",
    "plt.plot(x, y, '.')\n",
    "plt.grid()\n",
    "plt.xlabel(\"ndcg@10\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"ndcg@10 frequencies over \" + str(totperms) + \" permutations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
